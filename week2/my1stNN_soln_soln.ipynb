{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "1 #define _CUDA_NDARRAY_C\n",
      "2 \n",
      "3 #include <Python.h>\n",
      "4 #include <structmember.h>\n",
      "5 #include \"theano_mod_helper.h\"\n",
      "6 \n",
      "7 #include <numpy/arrayobject.h>\n",
      "8 #include <iostream>\n",
      "9 \n",
      "10 #include \"cuda_ndarray.cuh\"\n",
      "11 \n",
      "12 #ifndef CNMEM_DLLEXPORT\n",
      "13 #define CNMEM_DLLEXPORT\n",
      "14 #endif\n",
      "15 \n",
      "16 #include \"cnmem.h\"\n",
      "17 #include \"cnmem.cpp\"\n",
      "18 \n",
      "19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.\n",
      "20 #define COMPUTE_GPU_MEM_USED 0\n",
      "21 \n",
      "22 //If true, we fill with NAN allocated device memory.\n",
      "23 #define ALLOC_MEMSET 0\n",
      "24 \n",
      "25 //If true, we print out when we free a device pointer, uninitialize a\n",
      "26 //CudaNdarray, or allocate a device pointer\n",
      "27 #define PRINT_FREE_MALLOC 0\n",
      "28 \n",
      "29 //If true, we do error checking at the start of functions, to make sure there\n",
      "30 //is not a pre-existing error when the function is called.\n",
      "31 //You probably need to set the environment variable\n",
      "32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC\n",
      "33 //preprocessor macro in cuda_ndarray.cuh\n",
      "34 //if you want this to work.\n",
      "35 #define PRECHECK_ERROR 0\n",
      "36 \n",
      "37 cublasHandle_t handle = NULL;\n",
      "38 int* err_var = NULL;\n",
      "39 \n",
      "40 /////////////////////////\n",
      "41 // Alloc and Free\n",
      "42 /////////////////////////\n",
      "43 \n",
      "44 static int g_gpu_context_active = 0;\n",
      "45 \n",
      "46 \n",
      "47 PyObject *\n",
      "48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);\n",
      "49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);\n",
      "50 \n",
      "51 \n",
      "52 /**\n",
      "53  *\n",
      "54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.\n",
      "55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.\n",
      "56  *\n",
      "57  */\n",
      "58 int _outstanding_mallocs[] = {0,0};\n",
      "59 \n",
      "60 #if COMPUTE_GPU_MEM_USED\n",
      "61 size_t _allocated_size = 0;\n",
      "62 size_t _max_allocated_size = 0;\n",
      "63 \n",
      "64 const int TABLE_SIZE = 10000;\n",
      "65 struct table_struct{\n",
      "66     void* ptr;\n",
      "67     size_t size;\n",
      "68 };\n",
      "69 table_struct _alloc_size_table[TABLE_SIZE];\n",
      "70 #endif\n",
      "71 \n",
      "72 void * device_malloc(size_t size)\n",
      "73 {\n",
      "74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);\n",
      "75 }\n",
      "76 \n",
      "77 static bool g_use_cnmem = false;\n",
      "78 static const int g_max_devices = 8;\n",
      "79 int initCnmem(int card_number_provided, int card_nb, size_t mem) {\n",
      "80     static bool cnmemInitialized = false;\n",
      "81     if(cnmemInitialized) {\n",
      "82         return 0;\n",
      "83     }\n",
      "84     // On stderr to be at the same place as \"Using gpu device...\"\n",
      "85     int numDevices = 0;\n",
      "86     cnmemDevice_t devices[g_max_devices];\n",
      "87     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {\n",
      "88         PyErr_Format(PyExc_RuntimeError,\n",
      "89                      \"initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\\n\",\n",
      "90                      cudaGetErrorString(cudaGetLastError()));\n",
      "91         return -1;\n",
      "92     }\n",
      "93     if(card_number_provided){\n",
      "94         numDevices = 1;\n",
      "95         int i = 0;\n",
      "96         devices[i].device = card_nb;\n",
      "97         devices[i].size = mem;\n",
      "98         ///@TODO: thejaswi: add support for multiple streams\n",
      "99         devices[i].numStreams = 0;\n",
      "100         devices[i].streams = NULL;\n",
      "101         devices[i].streamSizes = NULL;\n",
      "102     }else{\n",
      "103         for(int i=0;i<numDevices;++i) {\n",
      "104             devices[i].device = i;\n",
      "105             devices[i].size = mem;\n",
      "106             ///@TODO: thejaswi: add support for multiple streams\n",
      "107             devices[i].numStreams = 0;\n",
      "108             devices[i].streams = NULL;\n",
      "109         }\n",
      "110     }\n",
      "111 \n",
      "112     ///@TODO: thejaswi: passing custom cnmem flags?\n",
      "113     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);\n",
      "114     if(status != CNMEM_STATUS_SUCCESS) {\n",
      "115         PyErr_Format(PyExc_RuntimeError,\n",
      "116                      \"initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\\n\",\n",
      "117                      cnmemGetErrorString(status), numDevices);\n",
      "118         return -1;\n",
      "119     }\n",
      "120     cnmemInitialized = true;\n",
      "121     return 0;\n",
      "122 }\n",
      "123 \n",
      "124 void * device_malloc(size_t size, int verbose)\n",
      "125 {\n",
      "126     #if PRECHECK_ERROR\n",
      "127         cudaThreadSynchronize();\n",
      "128         cudaError_t prevError = cudaGetLastError();\n",
      "129         if (cudaSuccess != prevError)\n",
      "130         {\n",
      "131             fprintf(stderr,\n",
      "132                     \"Error existed before calling device_malloc. %s\\n\",\n",
      "133                     cudaGetErrorString(prevError)\n",
      "134                     );\n",
      "135         }\n",
      "136     #endif\n",
      "137     void * rval=NULL;\n",
      "138     ///@TODO: thejaswi: support for multiple-streams?\n",
      "139     if(g_use_cnmem) {\n",
      "140         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;\n",
      "141         status = cnmemMalloc(&rval, size, NULL);\n",
      "142         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "143             PyErr_Format(PyExc_MemoryError,\n",
      "144                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "145                          (unsigned long long)size, cnmemGetErrorString(status));\n",
      "146             return NULL;\n",
      "147         }\n",
      "148     }\n",
      "149     else {\n",
      "150         cudaError_t err = cudaMalloc(&rval, size);\n",
      "151         if (cudaSuccess != err)\n",
      "152         {\n",
      "153             // Clear the error flag, cudaMalloc doesn't do it.\n",
      "154             // Currently this returns the same thing as err, but if in future\n",
      "155             // it returns something else I still don't see why we should ignore\n",
      "156             // it.  All we want to do here is reset the flag.\n",
      "157             cudaGetLastError();\n",
      "158             if (verbose)\n",
      "159             {\n",
      "160                 size_t free = 0, total = 0;\n",
      "161                 cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "162                 if (err2 != cudaSuccess){\n",
      "163                     cudaGetLastError();\n",
      "164                     fprintf(stderr,\n",
      "165                             \"Error when trying to find the memory information\"\n",
      "166                             \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "167                 }\n",
      "168                 #if COMPUTE_GPU_MEM_USED\n",
      "169                     fprintf(stderr,\n",
      "170                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "171                             \" new total bytes allocated: %llu.\"\n",
      "172                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "173                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)_allocated_size,\n",
      "174                             (unsigned long long)free, (unsigned long long)total);\n",
      "175                 #else\n",
      "176                     fprintf(stderr,\n",
      "177                             \"Error allocating %llu bytes of device memory (%s).\"\n",
      "178                             \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "179                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "180                 #endif\n",
      "181             }\n",
      "182             PyErr_Format(PyExc_MemoryError,\n",
      "183                          \"Error allocating %llu bytes of device memory (%s).\",\n",
      "184                          (unsigned long long)size, cudaGetErrorString(err));\n",
      "185             return NULL;\n",
      "186         }\n",
      "187     }\n",
      "188     if (rval != NULL){\n",
      "189         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?\n",
      "190         // Could this be what happen if size is 0?\n",
      "191         _outstanding_mallocs[0] += 1;\n",
      "192 \n",
      "193 #if COMPUTE_GPU_MEM_USED\n",
      "194         _allocated_size += size;\n",
      "195         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);\n",
      "196         int i = 0;\n",
      "197         for(;i<TABLE_SIZE;i++){\n",
      "198             if(NULL==_alloc_size_table[i].ptr){\n",
      "199                 _alloc_size_table[i].ptr=rval;\n",
      "200                 _alloc_size_table[i].size=size;\n",
      "201                 break;\n",
      "202             }\n",
      "203         }\n",
      "204         if (i == TABLE_SIZE){\n",
      "205             fprintf(stderr,\n",
      "206                     \"When tracking GPU malloc, our table size wasn't big enough.\"\n",
      "207                     \" So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu\");\n",
      "208         }\n",
      "209 #endif\n",
      "210     }\n",
      "211     //fprintf(stderr,\n",
      "212     //\"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\\n\",\n",
      "213     //(long)size, cudaGetErrorString(err),_allocated_size,rval);\n",
      "214 \n",
      "215     if(ALLOC_MEMSET){\n",
      "216         //We init them to nan to make sure we catch more debug case.\n",
      "217         cudaMemset(rval, 0xFF, size);\n",
      "218         //printf(\"MEMSET\\n\");\n",
      "219     }\n",
      "220     #if PRINT_FREE_MALLOC\n",
      "221         fprintf(stderr, \"device malloc %p of size %d\\n\", rval, size);\n",
      "222     #endif\n",
      "223     return rval;\n",
      "224 }\n",
      "225 \n",
      "226 int device_free(void *ptr)\n",
      "227 {\n",
      "228     #if PRECHECK_ERROR\n",
      "229         cudaThreadSynchronize();\n",
      "230         cudaError_t prevError = cudaGetLastError();\n",
      "231         if (cudaSuccess != prevError)\n",
      "232         {\n",
      "233             fprintf(stderr,\n",
      "234                     \"Error existed before calling device_free. %s\\n\",\n",
      "235                     cudaGetErrorString(prevError)\n",
      "236                     );\n",
      "237         }\n",
      "238     #endif\n",
      "239     #if PRINT_FREE_MALLOC\n",
      "240         size_t free = 0, total = 0;\n",
      "241         cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "242         if (err2 != cudaSuccess){\n",
      "243             cudaGetLastError();\n",
      "244             fprintf(stderr,\n",
      "245                     \"Error when tring to find the memory information\"\n",
      "246                     \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "247         }\n",
      "248         #if COMPUTE_GPU_MEM_USED\n",
      "249         {\n",
      "250             int i = 0;\n",
      "251             for(;i<TABLE_SIZE;i++)\n",
      "252                 if(_alloc_size_table[i].ptr==ptr){\n",
      "253                     break;\n",
      "254                 }\n",
      "255             assert(i<TABLE_SIZE);\n",
      "256             fprintf(stderr, \"device_free %p of size %d.\"\n",
      "257                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "258                     ptr, _alloc_size_table[i].size, free, total);\n",
      "259         }\n",
      "260         #else\n",
      "261             fprintf(stderr, \"device_free %p.\"\n",
      "262                     \" Driver report %d bytes free and %d bytes total \\n\",\n",
      "263                     ptr, free, total);\n",
      "264         #endif\n",
      "265     #endif\n",
      "266 \n",
      "267     // if there is no gpu context, the call to cudaFree will fail; skip it entirely\n",
      "268     if(!g_gpu_context_active) {\n",
      "269         return 0;\n",
      "270     }\n",
      "271 \n",
      "272     ///@TODO: thejaswi: multi-stream support\n",
      "273     if(g_use_cnmem) {\n",
      "274         cnmemStatus_t status = cnmemFree(ptr, NULL);\n",
      "275         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "276             fprintf(stderr, \"device_free: cnmemFree call failed! Reason=%s\\n\",\n",
      "277                     cnmemGetErrorString(status));\n",
      "278         }\n",
      "279     }\n",
      "280     else {\n",
      "281         // We need sync as the Theano's GC could remove intermediate variable that\n",
      "282         // are still needed as the gpu kernel are running or in the queue.\n",
      "283         CNDA_BEGIN_ALLOW_THREADS\n",
      "284         cudaThreadSynchronize();\n",
      "285         CNDA_END_ALLOW_THREADS\n",
      "286 \n",
      "287         cudaError_t err =  cudaFree(ptr);\n",
      "288         if (cudaSuccess != err)\n",
      "289         {\n",
      "290             // Clear the error flag, cudaFree doesn't do it.\n",
      "291             // Currently this returns the same thing as err, but if in future\n",
      "292             // it returns something else I still don't see why we should ignore\n",
      "293             // it.  All we want to do here is reset the flag.\n",
      "294             cudaGetLastError();\n",
      "295             size_t free = 0, total = 0;\n",
      "296             cudaError_t err2 = cudaMemGetInfo(&free, &total);\n",
      "297             if (err2 != cudaSuccess){\n",
      "298                 cudaGetLastError();\n",
      "299                 fprintf(stderr,\n",
      "300                         \"Error when tring to find the memory information\"\n",
      "301                         \" on the GPU: %s\\n\", cudaGetErrorString(err2));\n",
      "302             }\n",
      "303             #if COMPUTE_GPU_MEM_USED\n",
      "304             {\n",
      "305                 int i = 0;\n",
      "306                 for(;i<TABLE_SIZE;i++)\n",
      "307                     if(_alloc_size_table[i].ptr==ptr){\n",
      "308                         break;\n",
      "309                     }\n",
      "310                 assert(i<TABLE_SIZE);\n",
      "311                 fprintf(stderr,\n",
      "312                         \"Error freeing device pointer %p (%s) of size %llu. %llu byte already allocated.\"\n",
      "313                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "314                         ptr, cudaGetErrorString(err),\n",
      "315                         (unsigned long long)_alloc_size_table[i].size, (unsigned long long)_allocated_size, (unsigned long long)free, (unsigned long long)total);\n",
      "316             }\n",
      "317             #else\n",
      "318                 fprintf(stderr,\n",
      "319                         \"Error freeing device pointer %p (%s).\"\n",
      "320                         \" Driver report %llu bytes free and %llu bytes total \\n\",\n",
      "321                         ptr,\n",
      "322                         cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);\n",
      "323             #endif\n",
      "324             if (NULL != PyErr_Occurred()){\n",
      "325                 fprintf(stderr,\n",
      "326                         \"device_free: cudaFree() returned an error, but there is already an\"\n",
      "327                         \" Python error set. This happen during the clean up when there is a\"\n",
      "328                         \" first error and the CUDA driver is in a so bad state that it don't\"\n",
      "329                         \" work anymore. We keep the previous error set to help debugging it.\");\n",
      "330                 return -1;\n",
      "331             }\n",
      "332             PyErr_Format(PyExc_MemoryError,\n",
      "333                     \"error freeing device pointer %p (%s)\",\n",
      "334                     ptr,\n",
      "335                     cudaGetErrorString(err));\n",
      "336             return -1;\n",
      "337         }\n",
      "338     }\n",
      "339     _outstanding_mallocs[0] -= (ptr != NULL);\n",
      "340     #if COMPUTE_GPU_MEM_USED\n",
      "341         int i=0;\n",
      "342         size_t total_freed = 0;\n",
      "343         for(;i<TABLE_SIZE;i++)\n",
      "344             if(_alloc_size_table[i].ptr==ptr){\n",
      "345                 _allocated_size -= _alloc_size_table[i].size;\n",
      "346                 total_freed += _alloc_size_table[i].size;\n",
      "347                 _alloc_size_table[i].ptr=0;\n",
      "348                 _alloc_size_table[i].size=0;\n",
      "349 \n",
      "350                 break;\n",
      "351             }\n",
      "352         //if(i==TABLE_SIZE)\n",
      "353         //    printf(\"Unallocated unknow size!\\n\");\n",
      "354         //fprintf(stderr, \"freed %li bytes of device memory (%s). %d already allocated, ptr=%p\\n\", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);\n",
      "355     #endif\n",
      "356     return 0;\n",
      "357 }\n",
      "358 \n",
      "359 static PyObject *\n",
      "360 outstanding_mallocs(PyObject* self, PyObject * args)\n",
      "361 {\n",
      "362     return PyInt_FromLong(_outstanding_mallocs[0]);\n",
      "363 }\n",
      "364 \n",
      "365 \n",
      "366 static void *work_mem = NULL;\n",
      "367 static size_t work_size = 0;\n",
      "368 \n",
      "369 /*\n",
      "370  * Returns a chunk of memory for temporary work inside of an op. You can only\n",
      "371  * request a single chunk of memory at a time since it is reused.\n",
      "372  */\n",
      "373 void *get_work_mem(size_t sz) {\n",
      "374     if (sz <= work_size)\n",
      "375         return work_mem;\n",
      "376     device_free(work_mem);\n",
      "377     work_mem = device_malloc(sz);\n",
      "378     work_size = sz;\n",
      "379     if (work_mem == NULL)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380         work_size = 0;\n",
      "381     return work_mem;\n",
      "382 }\n",
      "383 \n",
      "384 /////////////////////////\n",
      "385 // Static helper methods\n",
      "386 /////////////////////////\n",
      "387 \n",
      "388 static void\n",
      "389 CudaNdarray_null_init(CudaNdarray*self)\n",
      "390 {\n",
      "391     self->base = NULL;\n",
      "392     self->nd = -1;\n",
      "393     self->host_structure = NULL;\n",
      "394     self->data_allocated = 0;\n",
      "395     self->dev_structure_fresh = 1;\n",
      "396     self->dev_structure = NULL;\n",
      "397     self->devdata = NULL;\n",
      "398 }\n",
      "399 \n",
      "400 static int\n",
      "401 CudaNdarray_uninit(CudaNdarray*self)\n",
      "402 {\n",
      "403     #if PRINT_FREE_MALLOC\n",
      "404         fprintf(stderr, \"CudaNdarray_uninit %p\\n\", self);\n",
      "405     #endif\n",
      "406     int rval = 0;\n",
      "407     if (self->data_allocated) {\n",
      "408         assert(self->devdata);\n",
      "409         if (device_free(self->devdata))\n",
      "410         {\n",
      "411             fprintf(stderr,\n",
      "412                     \"CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\\n\",\n",
      "413                     self, self->devdata);\n",
      "414             rval = -1;\n",
      "415         }\n",
      "416         self->devdata = NULL;\n",
      "417         self->data_allocated = 0;\n",
      "418     }\n",
      "419     if (self->dev_structure)\n",
      "420     {\n",
      "421         if (device_free(self->dev_structure))\n",
      "422         {\n",
      "423             fprintf(stderr,\n",
      "424                     \"CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\\n\",\n",
      "425                     self->dev_structure, self);\n",
      "426             rval = -1;\n",
      "427         }\n",
      "428         self->dev_structure = NULL;\n",
      "429     }\n",
      "430     if (self->host_structure)\n",
      "431     {\n",
      "432         free(self->host_structure);\n",
      "433         self->host_structure = NULL;\n",
      "434     }\n",
      "435     self->nd = -1;\n",
      "436     Py_XDECREF(self->base);\n",
      "437     self->base = NULL;\n",
      "438     return rval;\n",
      "439 }\n",
      "440 \n",
      "441 \n",
      "442 //make the rightmost coords change fastest\n",
      "443 //TODO: why does a downward for-loop not work????\n",
      "444 //TODO: use the log2_dims and driver code to remove / and %\n",
      "445 //TODO: skip the last division (when d == 0)\n",
      "446 #define decl_k_elemwise_unary_rowmajor(name, F) \\\n",
      "447 __global__ void name (unsigned int numEls,  \\\n",
      "448         unsigned int nd, \\\n",
      "449         const int * dim,  \\\n",
      "450         const float * a_data, const int * a_str, \\\n",
      "451         float * z_data, const int * z_str) \\\n",
      "452 { \\\n",
      "453     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \\\n",
      "454     const unsigned int numThreads = blockDim.x * gridDim.x; \\\n",
      "455  \\\n",
      "456     for (unsigned int i = idx; i < numEls; i += numThreads) \\\n",
      "457     { \\\n",
      "458         unsigned int ii = i; \\\n",
      "459         const float * a_i = a_data; \\\n",
      "460         float * z_i = z_data; \\\n",
      "461         for (unsigned int _d = 0; _d < nd; ++_d) \\\n",
      "462         { \\\n",
      "463             unsigned int d = nd - _d-1;  \\\n",
      "464             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \\\n",
      "465             ii = ii / dim[d]; \\\n",
      "466             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \\\n",
      "467             z_i += i_d * z_str[d]; \\\n",
      "468         } \\\n",
      "469         z_i[0] = F(a_i[0]); \\\n",
      "470     } \\\n",
      "471 }\n",
      "472 \n",
      "473 template<typename T> __device__ T unary_copy(T a) { return a; }\n",
      "474 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)\n",
      "475 \n",
      "476 template<typename T> __device__ T unary_exp(T a) { return exp(a); }\n",
      "477 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)\n",
      "478 \n",
      "479 /////////////////////////////\n",
      "480 // Satisfying reqs to be Type\n",
      "481 /////////////////////////////\n",
      "482 \n",
      "483 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead\n",
      "484 static void\n",
      "485 CudaNdarray_dealloc(CudaNdarray* self)\n",
      "486 {\n",
      "487     if (0) std::cerr << \"CudaNdarray dealloc \" << self << \" \" << self->devdata << '\\n';\n",
      "488     if(Py_REFCNT(self) > 1)\n",
      "489       printf(\"WARNING:CudaNdarray_dealloc called when there is still active reference to it.\\n\");\n",
      "490     CudaNdarray_uninit(self);\n",
      "491     Py_TYPE(self)->tp_free((PyObject*)self);\n",
      "492     --_outstanding_mallocs[1];\n",
      "493     if (0)\n",
      "494     {\n",
      "495         fprintf(stderr, \"device_malloc_counts: (device) %i (obj) %i\\n\",\n",
      "496                 _outstanding_mallocs[0],\n",
      "497                 _outstanding_mallocs[1]);\n",
      "498     }\n",
      "499 }\n",
      "500 \n",
      "501 static PyObject *\n",
      "502 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)\n",
      "503 {\n",
      "504     CudaNdarray *self;\n",
      "505 \n",
      "506     self = (CudaNdarray *)type->tp_alloc(type, 0);\n",
      "507     if (self != NULL)\n",
      "508     {\n",
      "509         CudaNdarray_null_init(self);\n",
      "510         ++_outstanding_mallocs[1];\n",
      "511     }\n",
      "512     return (PyObject *)self;\n",
      "513 }\n",
      "514 static int\n",
      "515 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)\n",
      "516 {\n",
      "517     PyObject *arr=NULL;\n",
      "518 \n",
      "519     if (! PyArg_ParseTuple(args, \"O\", &arr))\n",
      "520         return -1;\n",
      "521     if (! PyArray_Check(arr))\n",
      "522     {\n",
      "523         PyErr_SetString(PyExc_TypeError, \"PyArray arg required\");\n",
      "524         return -1;\n",
      "525     }\n",
      "526     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);\n",
      "527     return rval;\n",
      "528 }\n",
      "529 static PyMemberDef CudaNdarray_members[] =\n",
      "530 {\n",
      "531     /*\n",
      "532     {\"first\", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,\n",
      "533      \"first name\"},\n",
      "534     {\"last\", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,\n",
      "535      \"last name\"},\n",
      "536     {\"number\", T_INT, offsetof(CudaNdarray, number), 0,\n",
      "537      \"noddy number\"},\n",
      "538      */\n",
      "539     {NULL}  /* Sentinel */\n",
      "540 };\n",
      "541 \n",
      "542 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)\n",
      "543 {\n",
      "544     PyObject * dtype = NULL;\n",
      "545     if (args && !PyArg_ParseTuple(args, \"|O\", &dtype))\n",
      "546         return NULL;\n",
      "547     if (dtype) {\n",
      "548         PyArray_Descr* dtype2;\n",
      "549         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.\n",
      "550         if(!PyArray_DescrConverter(dtype, &dtype2))\n",
      "551         {\n",
      "552             PyObject * str = PyObject_Repr(dtype);\n",
      "553             PyErr_Format(PyExc_TypeError,\n",
      "554                          \"CudaNdarray dtype parameter not understood: %s\",\n",
      "555                          PyString_AsString(str)\n",
      "556                          );\n",
      "557             Py_CLEAR(str);\n",
      "558             return NULL;\n",
      "559         }\n",
      "560         int typeNum = dtype2->type_num;\n",
      "561         Py_DECREF(dtype2);\n",
      "562         if (typeNum != NPY_FLOAT32)\n",
      "563         {\n",
      "564             PyObject * str = PyObject_Repr(dtype);\n",
      "565             PyErr_Format(PyExc_TypeError,\n",
      "566                          \"CudaNdarray support only support float32 dtype, provided: %d\",\n",
      "567                          typeNum\n",
      "568                          );\n",
      "569             Py_CLEAR(str);\n",
      "570             return NULL;\n",
      "571         }\n",
      "572     }\n",
      "573 \n",
      "574     int verbose = 0;\n",
      "575     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){\n",
      "576         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "577         assert (npydims);\n",
      "578         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "579         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);\n",
      "580         free(npydims);\n",
      "581         if (!rval){\n",
      "582             return NULL;\n",
      "583         }\n",
      "584         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));\n",
      "585         return rval;\n",
      "586     }\n",
      "587     if ((self->nd < 0) || (self->devdata == 0))\n",
      "588     {\n",
      "589         PyErr_SetString(PyExc_ValueError, \"can't copy from un-initialized CudaNdarray\");\n",
      "590         return NULL;\n",
      "591     }\n",
      "592     CudaNdarray * contiguous_self = NULL;\n",
      "593     if (CudaNdarray_is_c_contiguous(self))\n",
      "594     {\n",
      "595         contiguous_self = self;\n",
      "596         Py_INCREF(contiguous_self);\n",
      "597         if (verbose) std::cerr << \"CreateArrayObj already contiguous\" << contiguous_self << '\\n';\n",
      "598     }\n",
      "599     else\n",
      "600     {\n",
      "601         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);\n",
      "602         if (verbose) std::cerr << \"CreateArrayObj created contiguous\" << contiguous_self << '\\n';\n",
      "603     }\n",
      "604     if (!contiguous_self)\n",
      "605     {\n",
      "606         return NULL;\n",
      "607     }\n",
      "608 \n",
      "609     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));\n",
      "610     assert (npydims);\n",
      "611     for (int i = 0; i < self->nd; ++i)\n",
      "612         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);\n",
      "613     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,\n",
      "614                                                                npydims,\n",
      "615                                                                REAL_TYPENUM);\n",
      "616     free(npydims);\n",
      "617     if (!rval)\n",
      "618     {\n",
      "619         Py_DECREF(contiguous_self);\n",
      "620         return NULL;\n",
      "621     }\n",
      "622 \n",
      "623     assert (PyArray_ITEMSIZE(rval) == sizeof(real));\n",
      "624 \n",
      "625     npy_intp rval_size = PyArray_SIZE(rval);\n",
      "626     void *rval_data = PyArray_DATA(rval);\n",
      "627     cudaError_t err;\n",
      "628     CNDA_BEGIN_ALLOW_THREADS;\n",
      "629 \n",
      "630     err = cudaMemcpy(rval_data, contiguous_self->devdata,\n",
      "631                      rval_size * sizeof(real),\n",
      "632                      cudaMemcpyDeviceToHost\n",
      "633                      );\n",
      "634     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "635     CNDA_END_ALLOW_THREADS;\n",
      "636 \n",
      "637     if (cudaSuccess != err)\n",
      "638     {\n",
      "639         PyErr_Format(PyExc_RuntimeError, \"error (%s)copying data to host\",\n",
      "640                      cudaGetErrorString(err));\n",
      "641         Py_DECREF(rval);\n",
      "642         rval = NULL;\n",
      "643     }\n",
      "644 \n",
      "645     Py_DECREF(contiguous_self);\n",
      "646     return (PyObject *)rval;\n",
      "647 }\n",
      "648 \n",
      "649 // TODO-- we have two functions here, ZEROS and Zeros.\n",
      "650 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)\n",
      "651 // but this naming is very weird, makes it look like a macro\n",
      "652 // we should figure out the correct convention and change to that\n",
      "653 PyObject* CudaNdarray_ZEROS(int n, int * dims)\n",
      "654 {\n",
      "655 \n",
      "656     size_t total_elements = 1;\n",
      "657 \n",
      "658     for(size_t i=0;i<n;i++){\n",
      "659         // Detect overflow on unsigned integer\n",
      "660         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {\n",
      "661             PyErr_Format(PyExc_RuntimeError,\n",
      "662                          \"Can't store in size_t for the bytes requested %llu * %llu\",\n",
      "663                          (unsigned long long)total_elements,\n",
      "664                          (unsigned long long)dims[i]);\n",
      "665             return NULL;\n",
      "666         }\n",
      "667         total_elements*=dims[i];\n",
      "668     }\n",
      "669 \n",
      "670     // total_elements now contains the size of the array, in reals\n",
      "671     if (total_elements > (SIZE_MAX / sizeof(real))){\n",
      "672         PyErr_Format(PyExc_RuntimeError,\n",
      "673                      \"Can't store in size_t for the bytes requested %llu * 4\",\n",
      "674                      (unsigned long long)total_elements);\n",
      "675         return NULL;\n",
      "676     }\n",
      "677     size_t total_size = total_elements * sizeof(real);\n",
      "678 \n",
      "679     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();\n",
      "680     if (!rval)\n",
      "681     {\n",
      "682         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: call to New failed\");\n",
      "683         return NULL;\n",
      "684     }\n",
      "685 \n",
      "686     if (CudaNdarray_alloc_contiguous(rval, n, dims))\n",
      "687     {\n",
      "688         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: allocation failed.\");\n",
      "689         Py_DECREF(rval);\n",
      "690         return NULL;\n",
      "691     }\n",
      "692 \n",
      "693     // Fill with zeros\n",
      "694     //fprintf(stdout, \"Sizeof: %d\\n\", total_size);\n",
      "695     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))\n",
      "696     {\n",
      "697         PyErr_Format(PyExc_MemoryError,\n",
      "698                      \"CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.\",\n",
      "699                      (unsigned long long)total_size);\n",
      "700         Py_DECREF(rval);\n",
      "701         return NULL;\n",
      "702     }\n",
      "703 \n",
      "704     if (cnda_copy_structure_to_device(rval))\n",
      "705     {\n",
      "706         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_ZEROS: syncing structure to device failed\");\n",
      "707         Py_DECREF(rval);\n",
      "708         return NULL;\n",
      "709     }\n",
      "710     return (PyObject*) rval;\n",
      "711 }\n",
      "712 \n",
      "713 // declared as a static method (hence 1st parameter is not used)\n",
      "714 // Based on _Copy and _dimshuffle\n",
      "715 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)\n",
      "716 {\n",
      "717     if(!shape)\n",
      "718     {\n",
      "719         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_Zeros: function takes at least 1 argument (0 given)\");\n",
      "720         return NULL;\n",
      "721     }\n",
      "722     if(!PySequence_Check(shape))\n",
      "723     {\n",
      "724         PyErr_SetString(PyExc_TypeError, \"shape argument must be a sequence\");\n",
      "725         return NULL;\n",
      "726     }\n",
      "727 \n",
      "728     int shplen = PySequence_Length(shape);\n",
      "729 \n",
      "730     if (shplen == 0)\n",
      "731     {\n",
      "732         return CudaNdarray_ZEROS(0, NULL);\n",
      "733     }\n",
      "734 \n",
      "735     int* newdims = (int *)malloc(sizeof(int) * shplen);\n",
      "736 \n",
      "737     if (!newdims)\n",
      "738     {\n",
      "739         PyErr_SetString(PyExc_MemoryError,\n",
      "740             \"CudaNdarray_Zeros: Failed to allocate temporary space\");\n",
      "741         return NULL;\n",
      "742     }\n",
      "743 \n",
      "744     // start from the end to compute strides\n",
      "745     for (int i = shplen-1; i >= 0; --i)\n",
      "746     {\n",
      "747         PyObject* shp_el_obj = PySequence_GetItem(shape, i);\n",
      "748         if(shp_el_obj == NULL)\n",
      "749         {\n",
      "750             // shouldn't happen since we checked length before...\n",
      "751             PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_Zeros: Index out of bound in sequence\");\n",
      "752             free(newdims);\n",
      "753             return NULL;\n",
      "754         }\n",
      "755 \n",
      "756         int shp_el = PyInt_AsLong(shp_el_obj);\n",
      "757         Py_DECREF(shp_el_obj);\n",
      "758 \n",
      "759         if (shp_el < 0)\n",
      "760         {\n",
      "761             PyErr_SetString(PyExc_ValueError, \"CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension\");\n",
      "762             free(newdims);\n",
      "763             return NULL;\n",
      "764         }\n",
      "765 \n",
      "766         newdims[i] = shp_el;\n",
      "767     }\n",
      "768 \n",
      "769     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);\n",
      "770 \n",
      "771     free(newdims);\n",
      "772 \n",
      "773     return (PyObject*)rval;\n",
      "774 }\n",
      "775 \n",
      "776 \n",
      "777 \n",
      "778 \n",
      "779 \n",
      "780 PyObject * CudaNdarray_Copy(const CudaNdarray * self)\n",
      "781 {\n",
      "782     PyObject * rval = CudaNdarray_New();\n",
      "783     if ((!rval) || (-1 == self->nd))\n",
      "784     {\n",
      "785         return rval;\n",
      "786     }\n",
      "787     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "788     {\n",
      "789         Py_DECREF(rval);\n",
      "790         return NULL;\n",
      "791     }\n",
      "792     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))\n",
      "793     {\n",
      "794         Py_DECREF(rval);\n",
      "795         return NULL;\n",
      "796     }\n",
      "797     return rval;\n",
      "798 }\n",
      "799 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)\n",
      "800 {\n",
      "801     assert(PyDict_Check(memo));\n",
      "802     PyObject * selfkey = PyInt_FromLong((long)self);\n",
      "803     assert(selfkey);\n",
      "804     if (PyDict_Contains(memo, selfkey))\n",
      "805     {\n",
      "806         PyObject * rval = PyDict_GetItem(memo, selfkey);\n",
      "807         Py_DECREF(selfkey);\n",
      "808         Py_XINCREF(rval);\n",
      "809         return rval;\n",
      "810     }\n",
      "811     else\n",
      "812     {\n",
      "813         PyObject * rval = CudaNdarray_Copy(self);\n",
      "814         if (0) std::cerr << \"DeepCopy created \" << rval << \" devdata \" << ((CudaNdarray*)rval)->devdata << \"\\n\";\n",
      "815         if (NULL == rval)\n",
      "816         {\n",
      "817             Py_DECREF(selfkey);\n",
      "818             return NULL;\n",
      "819         }\n",
      "820         if (PyDict_SetItem(memo, selfkey, rval))\n",
      "821         {\n",
      "822             Py_DECREF(rval);\n",
      "823             Py_DECREF(selfkey);\n",
      "824             return NULL;\n",
      "825         }\n",
      "826         Py_DECREF(selfkey);\n",
      "827         return rval;\n",
      "828     }\n",
      "829 }\n",
      "830 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)\n",
      "831 {\n",
      "832     if (!PySequence_Check(py_reduce_mask))\n",
      "833     {\n",
      "834         PyErr_SetString(PyExc_TypeError, \"reduce_mask must be sequence of ints\");\n",
      "835         return NULL;\n",
      "836     }\n",
      "837     int len = PySequence_Length(py_reduce_mask);\n",
      "838     if (len != self->nd)\n",
      "839     {\n",
      "840         PyErr_SetString(PyExc_TypeError, \"length of reduce_mask must match self->nd\");\n",
      "841         return NULL;\n",
      "842     }\n",
      "843     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();\n",
      "844     if (!self_sum)\n",
      "845     {\n",
      "846         return NULL;\n",
      "847     }\n",
      "848     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,\n",
      "849     //      and use it if it is big enough.\n",
      "850     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));\n",
      "851     int * sum_dims = dimshuffle_pattern + len;\n",
      "852     int n_remaining_dims = 0;\n",
      "853     if (!dimshuffle_pattern)\n",
      "854     {\n",
      "855         Py_DECREF(self_sum);\n",
      "856         PyErr_SetString(PyExc_MemoryError, \"failed to alloc internal storage\");\n",
      "857         return NULL;\n",
      "858     }\n",
      "859     for (int i = 0; i < len; ++i)\n",
      "860     {\n",
      "861         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);\n",
      "862         int o_i_int = PyInt_AsLong(o_i);\n",
      "863         Py_XDECREF(o_i);\n",
      "864         if (PyErr_Occurred())\n",
      "865         {\n",
      "866             Py_DECREF(self_sum);\n",
      "867             free(dimshuffle_pattern);\n",
      "868             return NULL;\n",
      "869         }\n",
      "870         if (o_i_int) // this is a dimension over which we are reducing\n",
      "871         {\n",
      "872             sum_dims[i] = 1;\n",
      "873         }\n",
      "874         else\n",
      "875         {\n",
      "876             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "877             dimshuffle_pattern[n_remaining_dims++] = i;\n",
      "878         }\n",
      "879     }\n",
      "880     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)\n",
      "881             || CudaNdarray_reduce_sum(self_sum, self)\n",
      "882             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))\n",
      "883     {\n",
      "884         Py_DECREF(self_sum);\n",
      "885         free(dimshuffle_pattern);\n",
      "886         return NULL;\n",
      "887     }\n",
      "888     free(dimshuffle_pattern);\n",
      "889     return (PyObject*)self_sum;\n",
      "890 }\n",
      "891 \n",
      "892 // Reshape self to the new shape gived by the tuple shape.\n",
      "893 //\n",
      "894 // If self is c contiguous, it return a view. Otherwise it always do a copy.\n",
      "895 // TODO: make it return a view when the strides allow it even if it is not\n",
      "896 //       c contiguous\n",
      "897 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)\n",
      "898 {\n",
      "899     if(!CudaNdarray_is_c_contiguous(self))\n",
      "900     {\n",
      "901         // allocate new space\n",
      "902         //TODO: test to see if we can re-use old one and take a new param to\n",
      "903         //  use this\n",
      "904         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);\n",
      "905         if (!rval)\n",
      "906         {\n",
      "907             return NULL;\n",
      "908         }\n",
      "909 \n",
      "910         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);\n",
      "911         Py_XDECREF(rval);\n",
      "912         return (PyObject*)ret;\n",
      "913     }\n",
      "914 \n",
      "915     // check shape tuple\n",
      "916     unsigned int rval_nd;\n",
      "917     unsigned int * rval_dims;\n",
      "918     size_t rval_size = 1;\n",
      "919 \n",
      "920     if (PyTuple_Check(shape)){\n",
      "921         // copy shape to integer array\n",
      "922         rval_nd = PyTuple_Size(shape);\n",
      "923     }else if (PyInt_Check(shape)){\n",
      "924         rval_nd = 1;\n",
      "925     }else{\n",
      "926         PyErr_SetString(PyExc_TypeError, \"shape must be tuple of integers or an integer\");\n",
      "927         return NULL;\n",
      "928     }\n",
      "929     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));\n",
      "930 \n",
      "931     if(PyTuple_Check(shape)){\n",
      "932         for (int i = 0; i < rval_nd; ++i)\n",
      "933         {\n",
      "934             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference\n",
      "935             if (PyErr_Occurred()) //error in AsLong\n",
      "936             {\n",
      "937                 free(rval_dims);\n",
      "938                 return NULL;\n",
      "939             }\n",
      "940             if(rval_dims[i]<0){\n",
      "941                 PyErr_Format(PyExc_ValueError, \"Reshape has invalid dimension %i (must be >=0)\",rval_dims[i]);\n",
      "942                 free(rval_dims);\n",
      "943                 return NULL;\n",
      "944             }\n",
      "945             rval_size = rval_size * rval_dims[i];\n",
      "946         }\n",
      "947     }else{\n",
      "948         rval_size = PyInt_AsLong(shape);\n",
      "949         rval_dims[0] = rval_size;\n",
      "950     }\n",
      "951     // calculate new size, assert same as old size\n",
      "952     if (rval_size != CudaNdarray_SIZE(self))\n",
      "953     {\n",
      "954         PyErr_Format(PyExc_ValueError, \"size must remain unchanged, changed from %lld to %lld\", CudaNdarray_SIZE(self), rval_size);\n",
      "955         free(rval_dims);\n",
      "956         return NULL;\n",
      "957     }\n",
      "958     if (rval_size==0)\n",
      "959     {\n",
      "960         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);\n",
      "961         free(rval_dims);\n",
      "962         return rval;\n",
      "963     }\n",
      "964 \n",
      "965     //return a view, not a copy\n",
      "966     //we can do this as we checked self is c_contiguous\n",
      "967     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);\n",
      "968 \n",
      "969     if (!rval || 0 != rval->data_allocated\n",
      "970         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "971     {\n",
      "972         Py_XDECREF(rval);\n",
      "973         free(rval_dims);\n",
      "974         return NULL;\n",
      "975     }\n",
      "976     //set dim and stride\n",
      "977     int size = 1;\n",
      "978     for (int i = rval_nd-1; i >= 0; --i)\n",
      "979     {\n",
      "980         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);\n",
      "981         CudaNdarray_set_dim(rval, i, rval_dims[i]);\n",
      "982         size = size * rval_dims[i];\n",
      "983     }\n",
      "984     free(rval_dims);\n",
      "985     return (PyObject*)rval;\n",
      "986 }\n",
      "987 \n",
      "988 PyObject * CudaNdarray_View(const CudaNdarray * self)\n",
      "989 {\n",
      "990     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);\n",
      "991     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "992     {\n",
      "993         Py_XDECREF(rval);\n",
      "994         rval = NULL;\n",
      "995     }\n",
      "996     else\n",
      "997     {\n",
      "998         for (int i = 0; i < self->nd; ++i)\n",
      "999         {\n",
      "1000             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);\n",
      "1001             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "1002         }\n",
      "1003     }\n",
      "1004     return (PyObject*)rval;\n",
      "1005 }\n",
      "1006 \n",
      "1007 /*\n",
      "1008  * d0,... are the output dims\n",
      "1009  * indices are a list of index to operate on\n",
      "1010  *         They are int32 viewed as float32.\n",
      "1011  * a is the output\n",
      "1012  * b is the input\n",
      "1013  * dB0, the source leading dimensions size\n",
      "1014  */\n",
      "1015 template <int operator_num>\n",
      "1016 __global__ void k_take_3(const int d0, const int d1, const int d2,\n",
      "1017                          const npy_int64* indices,\n",
      "1018                          float* a,\n",
      "1019                          const int sA0, const int sA1, const int sA2,\n",
      "1020                          const float* b, const int dB0,\n",
      "1021                          const int sB0, const int sB1, const int sB2,\n",
      "1022                          int* err){\n",
      "1023     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1024         npy_int64 idx = indices[i0];\n",
      "1025         if (idx<0)\n",
      "1026             idx += dB0; // To allow negative indexing.\n",
      "1027         if ((idx < 0) || (idx >= dB0)){\n",
      "1028             // Any value other the 0 probably work. But to be more safe, I want\n",
      "1029             // to change all bits to prevent problem with concurrent write that\n",
      "1030             // could cross cache line. But this should not happen with the\n",
      "1031             // current code and driver.\n",
      "1032             *err = 0xFFFF;\n",
      "1033             continue;\n",
      "1034         }\n",
      "1035         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){\n",
      "1036             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){\n",
      "1037                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;\n",
      "1038                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;\n",
      "1039                 a[a_idx] = b[b_idx];\n",
      "1040             }\n",
      "1041         }\n",
      "1042     }\n",
      "1043 }\n",
      "1044 \n",
      "1045 // We try to be similar to the PyArray_TakeFrom function\n",
      "1046 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html\n",
      "1047 //TODO: support other clip mode then raise(clip, wrap)\n",
      "1048 //self is the input that we copy data from.\n",
      "1049 //The indices that we receive MUST be an CudaNdarray(float32)\n",
      "1050 //    that is in fact a view to int64 indices\n",
      "1051 PyObject*\n",
      "1052 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){\n",
      "1053     int verbose = 0;\n",
      "1054     PyObject * indices_obj = NULL;\n",
      "1055     //int axis; Default None, that mean the flattened array.\n",
      "1056     PyObject * axis_obj = Py_None;\n",
      "1057     PyObject * out_obj = Py_None;\n",
      "1058     PyObject * clipmode_obj = NULL;\n",
      "1059     int max_threads = 1; // max threads per blocks\n",
      "1060 \n",
      "1061     if (! PyArg_ParseTuple(args, \"O|OOOi\", &indices_obj, &axis_obj,\n",
      "1062                            &out_obj, &clipmode_obj, &max_threads))\n",
      "1063         return NULL;\n",
      "1064 \n",
      "1065     //Check argument indices\n",
      "1066     //TODO: if not a numpy.ndarray, convert to numpy.ndarray\n",
      "1067     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?\n",
      "1068     //TODO: Support ndarray of other dtype then int32\n",
      "1069     //TODO: support list of indices that are not c_contiguous\n",
      "1070     CudaNdarray * indices = NULL;\n",
      "1071     if (CudaNdarray_Check(indices_obj)) {\n",
      "1072         if (verbose) printf(\"cudandarray indices\\n\");\n",
      "1073         indices = (CudaNdarray*) indices_obj;\n",
      "1074         Py_INCREF(indices);\n",
      "1075     } else if (PyArray_Check(indices_obj)) {\n",
      "1076         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1077         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {\n",
      "1078             PyErr_SetString(PyExc_TypeError,\n",
      "1079                             \"CudaNdarray_TakeFrom: need a ndarray for indices\"\n",
      "1080                             \" with dtype int64\");\n",
      "1081             return NULL;\n",
      "1082         }\n",
      "1083         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {\n",
      "1084             PyErr_SetString(PyExc_TypeError,\n",
      "1085                             \"CudaNdarray_TakeFrom: need a CudaNdarray of\"\n",
      "1086                             \" indices with only 1 dimensions\");\n",
      "1087             return NULL;\n",
      "1088         }\n",
      "1089         // We need indices_obj to be contiguous, in order to take a view\n",
      "1090         // with a different dtype.\n",
      "1091         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {\n",
      "1092             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);\n",
      "1093             if (!indices_obj_contig)\n",
      "1094                 return NULL;\n",
      "1095             indices_obj = indices_obj_contig;\n",
      "1096         } else {\n",
      "1097             // Keep the refcount consistent\n",
      "1098             Py_INCREF(indices_obj);\n",
      "1099         }\n",
      "1100         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);\n",
      "1101         PyObject * indices_float32 = NULL;\n",
      "1102         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,\n",
      "1103                                                   float32_descr, NULL);\n",
      "1104         if (verbose) printf(\"ndarray indices\\n\");\n",
      "1105         if (!indices_float32) {\n",
      "1106             Py_DECREF(indices_obj);\n",
      "1107             return NULL;\n",
      "1108         }\n",
      "1109 \n",
      "1110         indices = (CudaNdarray*) CudaNdarray_New();\n",
      "1111         if (verbose) printf(\"\\nndarray after new\\n\");\n",
      "1112         if (! indices){\n",
      "1113             Py_DECREF(indices_obj);\n",
      "1114             Py_DECREF(indices_float32);\n",
      "1115             return NULL;\n",
      "1116         }\n",
      "1117         if (CudaNdarray_CopyFromArray(indices,\n",
      "1118                                       (PyArrayObject *)indices_float32)){\n",
      "1119             Py_DECREF(indices_obj);\n",
      "1120             Py_DECREF(indices_float32);\n",
      "1121             return NULL;\n",
      "1122         }\n",
      "1123         Py_DECREF(indices_obj);\n",
      "1124         Py_DECREF(indices_float32);\n",
      "1125     } else {\n",
      "1126         PyObject* py_s = PyObject_Str(indices_obj);\n",
      "1127         const char* s = PyString_AsString(py_s);\n",
      "1128         Py_DECREF(py_s);\n",
      "1129         PyErr_Format(PyExc_TypeError,\n",
      "1130                      \"CudaNdarray_TakeFrom: need an ndarray of int64 or a\"\n",
      "1131                      \" CudaNdarray(float32) that is a view from int64 data\"\n",
      "1132                      \" for indices. Got %s\", s);\n",
      "1133         return NULL;\n",
      "1134     }\n",
      "1135 \n",
      "1136     if (verbose) {\n",
      "1137         printf(\"indices used on the gpu\\n\");\n",
      "1138         fprint_CudaNdarray(stdout, indices);\n",
      "1139         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);\n",
      "1140         PyObject_Print(used_indices, stdout, 0);\n",
      "1141         Py_DECREF(used_indices);\n",
      "1142     }\n",
      "1143     if (verbose) printf(\"after print of object\\n\");\n",
      "1144     if(!CudaNdarray_is_c_contiguous(indices) != 0) {\n",
      "1145         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1146                         \"CudaNdarray_TakeFrom: The indices must be contiguous in memory.\");\n",
      "1147         Py_DECREF(indices);\n",
      "1148         return NULL;\n",
      "1149     }\n",
      "1150     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes\n",
      "1151 \n",
      "1152     //Check argument axis\n",
      "1153     //TODO: implement the default and other axis\n",
      "1154     long axis = PyInt_AsLong(axis_obj);\n",
      "1155 \n",
      "1156     if (axis != 0) {\n",
      "1157         PyErr_Format(PyExc_NotImplementedError,\n",
      "1158                      \"CudaNdarray_TakeFrom: only axis=0 is currently supported.\"\n",
      "1159                      \" Got %ld.\", axis);\n",
      "1160         Py_DECREF(indices);\n",
      "1161         return NULL;\n",
      "1162     }\n",
      "1163 \n",
      "1164     //Check argument out_obj\n",
      "1165     CudaNdarray * out = NULL;\n",
      "1166     if (out_obj && CudaNdarray_Check(out_obj))\n",
      "1167         out = (CudaNdarray*) out_obj;\n",
      "1168     if (out && (out->nd != self->nd ||\n",
      "1169                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))\n",
      "1170         out = NULL;\n",
      "1171     int * dims = (int *)malloc(sizeof(int) * self->nd);\n",
      "1172     dims[0] = nb_indices;\n",
      "1173 \n",
      "1174     for (int i=1 ; i<self->nd ; i++) {\n",
      "1175         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "1176         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {\n",
      "1177             out = NULL;\n",
      "1178         }\n",
      "1179     }\n",
      "1180     if (!out) {\n",
      "1181         out = (CudaNdarray*)CudaNdarray_New();\n",
      "1182         if (!out){\n",
      "1183             Py_DECREF(indices);\n",
      "1184             free(dims);\n",
      "1185             return NULL;\n",
      "1186         }\n",
      "1187         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {\n",
      "1188             Py_DECREF(out);\n",
      "1189             Py_DECREF(indices);\n",
      "1190             free(dims);\n",
      "1191             return NULL;\n",
      "1192         }\n",
      "1193     }else {\n",
      "1194         Py_INCREF(out);\n",
      "1195     }\n",
      "1196 \n",
      "1197     //Check argument clipmode\n",
      "1198     if (clipmode_obj) {\n",
      "1199         char * clipmode = PyString_AsString(clipmode_obj);\n",
      "1200         if (! clipmode){\n",
      "1201             Py_DECREF(indices);\n",
      "1202             Py_DECREF(out);\n",
      "1203             free(dims);\n",
      "1204             return NULL;\n",
      "1205         }\n",
      "1206         if (strcmp(clipmode, \"raise\") != 0) {\n",
      "1207             PyErr_Format(PyExc_NotImplementedError,\n",
      "1208                          \"CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'\",\n",
      "1209                          clipmode);\n",
      "1210             Py_DECREF(indices);\n",
      "1211             Py_DECREF(out);\n",
      "1212             free(dims);\n",
      "1213             return NULL;\n",
      "1214         }\n",
      "1215     }\n",
      "1216     void (*k3)(const int, const int, const int,\n",
      "1217                const npy_int64*,\n",
      "1218                float*, const int, const int, const int,\n",
      "1219                const float*, const int,\n",
      "1220                const int, const int, const int,\n",
      "1221                int*);\n",
      "1222     k3 = k_take_3<CPY>;\n",
      "1223 \n",
      "1224     // Create the memory place that will store the error information.\n",
      "1225     if(init_err_var() != 0) return NULL;\n",
      "1226 \n",
      "1227     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);\n",
      "1228     if(CudaNdarray_HOST_DIMS(out)[0] == 0){\n",
      "1229         // We take 0 elements, so no need for the rest of the code.\n",
      "1230         // This speed up that case AND fix crash otherwise.\n",
      "1231         free(dims);\n",
      "1232         Py_DECREF(indices);\n",
      "1233         return (PyObject *)out;\n",
      "1234     }\n",
      "1235 \n",
      "1236     switch (self->nd) {\n",
      "1237         case 1:\n",
      "1238             {\n",
      "1239                 dim3 n_threads(1, 1, 1);\n",
      "1240                 if (verbose)\n",
      "1241                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1242                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1243                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1244                            cudaGetLastError(), self->nd,\n",
      "1245                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1246                 k3<<<n_blocks, n_threads>>>(\n",
      "1247                         dims[0],\n",
      "1248                         1,\n",
      "1249                         1,\n",
      "1250                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1251                         CudaNdarray_DEV_DATA(out),\n",
      "1252                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1253                         1,\n",
      "1254                         1,\n",
      "1255                         CudaNdarray_DEV_DATA(self),\n",
      "1256                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1257                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1258                         1,\n",
      "1259                         1,\n",
      "1260                         err_var);\n",
      "1261             }\n",
      "1262             break;\n",
      "1263         case 2:\n",
      "1264             {\n",
      "1265                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);\n",
      "1266 \n",
      "1267                 if (verbose)\n",
      "1268                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1269                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1270                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1271                            cudaGetLastError(), self->nd,\n",
      "1272                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1273 \n",
      "1274                 k3<<<n_blocks, n_threads>>>(\n",
      "1275                         dims[0], //dimensions\n",
      "1276                         dims[1],\n",
      "1277                         1,\n",
      "1278                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1279                         CudaNdarray_DEV_DATA(out),\n",
      "1280                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1281                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1282                         1,\n",
      "1283                         CudaNdarray_DEV_DATA(self),\n",
      "1284                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1285                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1286                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1287                         1,\n",
      "1288                         err_var);\n",
      "1289             }\n",
      "1290             break;\n",
      "1291         case 3:\n",
      "1292             {\n",
      "1293                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);\n",
      "1294                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);\n",
      "1295                 dim3 n_threads(tx, ty, 1);\n",
      "1296                 if (verbose)\n",
      "1297                     printf(\"cudaGetLastError=%d, nd=%d\"\n",
      "1298                            \" kernel config: (n_blocks.x=%d, n_blocks.y=%d,\"\n",
      "1299                            \" n_threads.x=%i, n_threads.y=%i)\\n\",\n",
      "1300                            cudaGetLastError(), self->nd,\n",
      "1301                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);\n",
      "1302                 k3<<<n_blocks, n_threads>>>(\n",
      "1303                         dims[0], //dimensions\n",
      "1304                         dims[1],\n",
      "1305                         dims[2],\n",
      "1306                         (npy_int64*) CudaNdarray_DEV_DATA(indices),\n",
      "1307                         CudaNdarray_DEV_DATA(out),\n",
      "1308                         CudaNdarray_HOST_STRIDES(out)[0], //strides\n",
      "1309                         CudaNdarray_HOST_STRIDES(out)[1],\n",
      "1310                         CudaNdarray_HOST_STRIDES(out)[2],\n",
      "1311                         CudaNdarray_DEV_DATA(self),\n",
      "1312                         CudaNdarray_HOST_DIMS(self)[0], //For indices check\n",
      "1313                         CudaNdarray_HOST_STRIDES(self)[0], //strides\n",
      "1314                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1315                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1316                         err_var);\n",
      "1317             }\n",
      "1318             break;\n",
      "1319     default:\n",
      "1320         PyErr_SetString(PyExc_NotImplementedError,\n",
      "1321                         \"CudaNdarray_TakeFrom: only input with 1, 2 or 3\"\n",
      "1322                         \" dimensions are currently supported\");\n",
      "1323 \n",
      "1324     }\n",
      "1325     free(dims);\n",
      "1326     CNDA_THREAD_SYNC;\n",
      "1327     cudaError_t err = cudaGetLastError();\n",
      "1328     if (cudaSuccess != err) {\n",
      "1329         PyErr_Format(PyExc_RuntimeError,\n",
      "1330                      \"Cuda error: %s: %s.\\n\",\n",
      "1331                      \"CudaNdarray_TakeFrom\",\n",
      "1332                      cudaGetErrorString(err));\n",
      "1333         Py_DECREF(indices);\n",
      "1334         Py_DECREF(out);\n",
      "1335         return NULL;\n",
      "1336     }\n",
      "1337 \n",
      "1338     int index_err = check_err_var();\n",
      "1339     Py_DECREF(indices);\n",
      "1340     if (index_err != 0) {\n",
      "1341         Py_DECREF(out);\n",
      "1342         return NULL;\n",
      "1343     }\n",
      "1344 \n",
      "1345     if (verbose) printf(\"TAKE SUCCEDED\\n\");\n",
      "1346     return (PyObject *)out;\n",
      "1347 }\n",
      "1348 \n",
      "1349 \n",
      "1350 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)\n",
      "1351 {\n",
      "1352     int pos, stride;\n",
      "1353     if (! PyArg_ParseTuple(args, \"ii\", &pos, &stride))\n",
      "1354         return NULL;\n",
      "1355     if ((pos < 0) || (pos >= self->nd))\n",
      "1356     {\n",
      "1357         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1358         return NULL;\n",
      "1359     }\n",
      "1360     CudaNdarray_set_stride(self, pos, stride);\n",
      "1361     if (cnda_copy_structure_to_device(self))\n",
      "1362     {\n",
      "1363         return NULL;\n",
      "1364     }\n",
      "1365     Py_INCREF(Py_None);\n",
      "1366     return Py_None;\n",
      "1367 }\n",
      "1368 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)\n",
      "1369 {\n",
      "1370     int pos, dim;\n",
      "1371     if (! PyArg_ParseTuple(args, \"ii\", &pos, &dim))\n",
      "1372         return NULL;\n",
      "1373     if ((pos < 0) || (pos >= self->nd))\n",
      "1374     {\n",
      "1375         PyErr_Format(PyExc_ValueError, \"position argument out of legal range [0, %i)\", self->nd);\n",
      "1376         return NULL;\n",
      "1377     }\n",
      "1378     CudaNdarray_set_dim(self, pos, dim);\n",
      "1379     if (cnda_copy_structure_to_device(self))\n",
      "1380     {\n",
      "1381         return NULL;\n",
      "1382     }\n",
      "1383     Py_INCREF(Py_None);\n",
      "1384     return Py_None;\n",
      "1385 }\n",
      "1386 \n",
      "1387 static PyObject *\n",
      "1388 CudaNdarray_exp(CudaNdarray* self)\n",
      "1389 {\n",
      "1390     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1391     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1392     {\n",
      "1393         Py_XDECREF(rval);\n",
      "1394         return NULL;\n",
      "1395     }\n",
      "1396     unsigned int size = 1;\n",
      "1397     for (int i = 0; i < self->nd; i++)\n",
      "1398     {\n",
      "1399         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1400     }\n",
      "1401     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1402     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1403     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),\n",
      "1404             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),\n",
      "1405             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));\n",
      "1406 \n",
      "1407     //TODO: don't do this right away, do it when we need the result\n",
      "1408     CNDA_THREAD_SYNC;\n",
      "1409     cudaError_t err = cudaGetLastError();\n",
      "1410     if( cudaSuccess != err)\n",
      "1411     {\n",
      "1412         Py_DECREF(rval);\n",
      "1413         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kExp\", cudaGetErrorString(err));\n",
      "1414         return NULL;\n",
      "1415     }\n",
      "1416 \n",
      "1417     return (PyObject*)rval;\n",
      "1418 }\n",
      "1419 \n",
      "1420 static PyMethodDef CudaNdarray_methods[] =\n",
      "1421 {\n",
      "1422     {\"__array__\",\n",
      "1423         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,\n",
      "1424         \"Copy from the device to a numpy ndarray\"},\n",
      "1425     {\"__copy__\",\n",
      "1426         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1427         \"Create a shallow copy of this object. used by module copy\"},\n",
      "1428     {\"__deepcopy__\",\n",
      "1429         (PyCFunction)CudaNdarray_DeepCopy, METH_O,\n",
      "1430         \"Create a copy of this object\"},\n",
      "1431     {\"zeros\",\n",
      "1432         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,\n",
      "1433         \"Create a new CudaNdarray with specified shape, filled with zeros.\"},\n",
      "1434     {\"copy\",\n",
      "1435         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,\n",
      "1436         \"Create a copy of this object\"},\n",
      "1437     {\"is_c_contiguous\",\n",
      "1438         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,\n",
      "1439         \"Return True is the object is c contiguous. False otherwise.\"},\n",
      "1440     {\"reduce_sum\",\n",
      "1441         (PyCFunction)CudaNdarray_ReduceSum, METH_O,\n",
      "1442         \"Reduce over the given dimensions by summation\"},\n",
      "1443     {\"exp\",\n",
      "1444         (PyCFunction)CudaNdarray_exp, METH_NOARGS,\n",
      "1445         \"Return the exponential of all elements\"},\n",
      "1446     {\"reshape\",\n",
      "1447         (PyCFunction)CudaNdarray_Reshape, METH_O,\n",
      "1448         \"Return a reshaped view (or copy) of this ndarray\\n\\\n",
      "1449             The required argument is a tuple of integers specifying the shape of the new ndarray.\"},\n",
      "1450     {\"view\",\n",
      "1451         (PyCFunction)CudaNdarray_View, METH_NOARGS,\n",
      "1452         \"Return an alias of this ndarray\"},\n",
      "1453     {\"_set_stride\",\n",
      "1454         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,\n",
      "1455         \"For integer arguments (i, s), set the 'i'th stride to 's'\"},\n",
      "1456     {\"take\",\n",
      "1457         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,\n",
      "1458         \"Equivalent of numpy.take\"},\n",
      "1459     {\"_set_shape_i\",\n",
      "1460         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,\n",
      "1461         \"For integer arguments (i, s), set the 'i'th shape to 's'\"},\n",
      "1462     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "1463 };\n",
      "1464 \n",
      "1465 \n",
      "1466 ////////////////////\n",
      "1467 // Number protocol\n",
      "1468 ////////////////////\n",
      "1469 \n",
      "1470 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {\n",
      "1471     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "1472     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "1473 \n",
      "1474     for (unsigned int i = idx; i < numEls; i += numThreads) {\n",
      "1475         dest[i] = a[i] + b[i];\n",
      "1476     }\n",
      "1477 }\n",
      "1478 \n",
      "1479 // Will be called by __add__ in Python\n",
      "1480 static PyObject *\n",
      "1481 CudaNdarray_add(PyObject* py_self, PyObject * py_other)\n",
      "1482 {\n",
      "1483     if (! CudaNdarray_Check(py_self)) {\n",
      "1484         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on left\");\n",
      "1485         return NULL;\n",
      "1486     }\n",
      "1487     if (! CudaNdarray_Check(py_other)) {\n",
      "1488         PyErr_SetString(PyExc_TypeError, \"need a CudaNdarray on right\");\n",
      "1489         return NULL;\n",
      "1490     }\n",
      "1491     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1492     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1493     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){\n",
      "1494         PyErr_SetString(PyExc_TypeError, \"We have implementet only the c_contiguous version for now.\");\n",
      "1495         return NULL;\n",
      "1496     }\n",
      "1497 \n",
      "1498     //standard elemwise size checks\n",
      "1499     if (self->nd != other->nd)\n",
      "1500     {\n",
      "1501         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_add: need same number of dims\");\n",
      "1502         return NULL;\n",
      "1503     }\n",
      "1504     //standard elemwise dim checks\n",
      "1505     unsigned int size = 1;\n",
      "1506     for (int i = 0; i< self->nd; ++i)\n",
      "1507     {\n",
      "1508         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "1509         {\n",
      "1510             PyErr_SetString(PyExc_TypeError, \"need same dimensions\");\n",
      "1511             return NULL;\n",
      "1512         }\n",
      "1513         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1514     }\n",
      "1515     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();\n",
      "1516     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))\n",
      "1517     {\n",
      "1518         Py_XDECREF(rval);\n",
      "1519         return NULL;\n",
      "1520     }\n",
      "1521 \n",
      "1522     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){\n",
      "1523       return (PyObject *) rval;\n",
      "1524     }\n",
      "1525 \n",
      "1526     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "1527     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "1528     kAdd_contiguous<<<n_blocks,threads_per_block>>>(\n",
      "1529             self->devdata, other->devdata, rval->devdata, size);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1530     CNDA_THREAD_SYNC;\n",
      "1531     cudaError_t err = cudaGetLastError();\n",
      "1532     if( cudaSuccess != err)\n",
      "1533     {\n",
      "1534         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kAdd\", cudaGetErrorString(err));\n",
      "1535         Py_DECREF(rval);\n",
      "1536         return NULL;\n",
      "1537     }\n",
      "1538     return (PyObject *) rval;\n",
      "1539 }\n",
      "1540 \n",
      "1541 template <int operator_num>\n",
      "1542 __global__ void k_ielem_3(const int d0, const int d1, const int d2,\n",
      "1543         float* a, const int sA0, const int sA1, const int sA2,\n",
      "1544         const float* b, const int sB0, const int sB1, const int sB2){\n",
      "1545     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1546         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1547             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1548                 switch (operator_num)\n",
      "1549                 {\n",
      "1550                   case IADD:\n",
      "1551                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1552                     break;\n",
      "1553                   case IDIV:\n",
      "1554                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1555                     break;\n",
      "1556                   case CPY:\n",
      "1557                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];\n",
      "1558                     break;\n",
      "1559                 }\n",
      "1560             }\n",
      "1561         }\n",
      "1562     }\n",
      "1563 }\n",
      "1564 \n",
      "1565 template <int operator_num>\n",
      "1566 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,\n",
      "1567                          float* a, const int sA0, const int sA1,\n",
      "1568                          const int sA2, const int sA3,\n",
      "1569                          const float* b, const int sB0, const int sB1,\n",
      "1570                          const int sB2, const int sB3){\n",
      "1571     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1572         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1573             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){\n",
      "1574                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){\n",
      "1575                     switch (operator_num) {\n",
      "1576                         case IADD:\n",
      "1577                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1578                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1579                             break;\n",
      "1580                         case IDIV:\n",
      "1581                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1582                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1583                             break;\n",
      "1584                         case CPY:\n",
      "1585                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]\n",
      "1586                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];\n",
      "1587                             break;\n",
      "1588                     }\n",
      "1589                 }\n",
      "1590             }\n",
      "1591         }\n",
      "1592     }\n",
      "1593 }\n",
      "1594 \n",
      "1595 template <int operator_num>\n",
      "1596 __global__ void k_ielem_6(const int d0, const int d1,\n",
      "1597                           const int d2, const int d3,\n",
      "1598                           const int d4, const int d5,\n",
      "1599                           float* a, const int sA0, const int sA1,\n",
      "1600                           const int sA2, const int sA3,\n",
      "1601                           const int sA4, const int sA5,\n",
      "1602                           const float* b, const int sB0, const int sB1,\n",
      "1603                           const int sB2, const int sB3,\n",
      "1604                           const int sB4, const int sB5\n",
      "1605                           ){\n",
      "1606     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){\n",
      "1607         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){\n",
      "1608             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){\n",
      "1609                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){\n",
      "1610                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){\n",
      "1611                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){\n",
      "1612                             switch (operator_num) {\n",
      "1613                             case IADD:\n",
      "1614                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1615                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1616                                 break;\n",
      "1617                             case IDIV:\n",
      "1618                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1619                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1620                                 break;\n",
      "1621                             case CPY:\n",
      "1622                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]\n",
      "1623                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];\n",
      "1624                                 break;\n",
      "1625                             }\n",
      "1626                         }\n",
      "1627                     }\n",
      "1628                 }\n",
      "1629             }\n",
      "1630         }\n",
      "1631     }\n",
      "1632 }\n",
      "1633 \n",
      "1634 /*\n",
      "1635 CudaNdarray_inplace_elemwise\n",
      "1636 Compute elemwise, working inplace on A.\n",
      "1637 Currently implemented A / B, A + B and A = B\n",
      "1638 (the last is not tested and not used!)\n",
      "1639 \n",
      "1640 py_self - the CudaNdarray that we'll modify (A)\n",
      "1641 py_other - the other argument (B)\n",
      "1642 fct_nb - which operation to perform (operator_t)\n",
      "1643 \n",
      "1644 Returns 0 on success.\n",
      "1645 Returns -1 on failure, and sets Python exception.\n",
      "1646 \n",
      "1647 */\n",
      "1648 int\n",
      "1649 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)\n",
      "1650 {\n",
      "1651     int verbose = 0;\n",
      "1652     void (*k3)(const int, const int, const int,\n",
      "1653                     float*, const int, const int, const int,\n",
      "1654                     const float*, const int, const int, const int);\n",
      "1655     void (*k4)(const int, const int, const int, const int,\n",
      "1656                     float*, const int, const int,\n",
      "1657                     const int, const int,\n",
      "1658                     const float*, const int, const int,\n",
      "1659                     const int, const int);\n",
      "1660     void (*k6)(const int, const int,\n",
      "1661                const int, const int,\n",
      "1662                const int, const int,\n",
      "1663                float*, const int, const int,\n",
      "1664                const int, const int,\n",
      "1665                const int, const int,\n",
      "1666                const float*, const int, const int,\n",
      "1667                const int, const int,\n",
      "1668                const int, const int);\n",
      "1669     switch (fct_nb)\n",
      "1670     {\n",
      "1671         case IADD:\n",
      "1672             k3 = k_ielem_3<IADD>;\n",
      "1673             k4 = k_ielem_4<IADD>;\n",
      "1674             k6 = k_ielem_6<IADD>;\n",
      "1675             break;\n",
      "1676         case IDIV:\n",
      "1677             k3 = k_ielem_3<IDIV>;\n",
      "1678             k4 = k_ielem_4<IDIV>;\n",
      "1679             k6 = k_ielem_6<IDIV>;\n",
      "1680             break;\n",
      "1681         case CPY:\n",
      "1682             k3 = k_ielem_3<CPY>;\n",
      "1683             k4 = k_ielem_4<CPY>;\n",
      "1684             k6 = k_ielem_6<CPY>;\n",
      "1685             break;\n",
      "1686         default:\n",
      "1687             assert (0);\n",
      "1688             PyErr_Format(\n",
      "1689                 PyExc_TypeError,\n",
      "1690                 \"CudaNdarray_inplace_elemwise invalid fct_nb (%i).\",\n",
      "1691                 (int)fct_nb);\n",
      "1692             return -1;\n",
      "1693     }\n",
      "1694     if (!CudaNdarray_Check(py_self)) {\n",
      "1695         PyErr_SetString(\n",
      "1696             PyExc_TypeError,\n",
      "1697             \"CudaNdarray_inplace_elemwise need a CudaNdarray on left\");\n",
      "1698         return -1;\n",
      "1699     }\n",
      "1700     CudaNdarray * new_other = NULL;\n",
      "1701     if (!CudaNdarray_Check(py_other)) {\n",
      "1702         new_other = (CudaNdarray*) CudaNdarray_New();\n",
      "1703         if(!new_other)\n",
      "1704         {\n",
      "1705             return -1;\n",
      "1706         }\n",
      "1707         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))\n",
      "1708         {\n",
      "1709             Py_XDECREF(new_other);\n",
      "1710             return -1;\n",
      "1711         }\n",
      "1712         py_other = (PyObject *) new_other;\n",
      "1713     }\n",
      "1714 \n",
      "1715     CudaNdarray * self = (CudaNdarray *)py_self;\n",
      "1716     CudaNdarray * other = (CudaNdarray *)py_other;\n",
      "1717 \n",
      "1718     if (verbose)\n",
      "1719     {\n",
      "1720         fprintf(stderr,\n",
      "1721             \"INPLACE ADD/DIV for self->nd=%d other->nd=%d\\n\",\n",
      "1722             self->nd, other->nd);\n",
      "1723     }\n",
      "1724 \n",
      "1725     //standard elemwise nb dim checks\n",
      "1726     if (self->nd < other->nd)\n",
      "1727     {\n",
      "1728         PyErr_Format(\n",
      "1729             PyExc_TypeError,\n",
      "1730             \"CudaNdarray_inplace_elemwise: The destination need more or the\"\n",
      "1731             \" same number of dimensions then the source. Got %d and %d.\",\n",
      "1732             self->nd, other->nd);\n",
      "1733         Py_XDECREF(new_other);\n",
      "1734         return -1;\n",
      "1735     }\n",
      "1736 \n",
      "1737     //broadcast to the same number of dimensions.\n",
      "1738     int* other_dims = (int*) alloca(self->nd * sizeof(int));\n",
      "1739     int* other_strides = (int*) alloca(self->nd * sizeof(int));\n",
      "1740     int added_dims = self->nd - other->nd;\n",
      "1741     // Add the added broadcasted dimensions\n",
      "1742     for (int i = 0; i< added_dims; ++i)\n",
      "1743     {\n",
      "1744         other_dims[i] = 1;\n",
      "1745         other_strides[i] = 0;\n",
      "1746     }\n",
      "1747     // Copy the existing dimensions\n",
      "1748     for (int i = 0; i< other->nd; ++i)\n",
      "1749     {\n",
      "1750         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];\n",
      "1751         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];\n",
      "1752     }\n",
      "1753 \n",
      "1754     //standard elemwise dim checks\n",
      "1755     unsigned int size = 1;\n",
      "1756     for (int i = 0; i< self->nd; ++i)\n",
      "1757     {\n",
      "1758         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])\n",
      "1759             && (other_dims[i] != 1))\n",
      "1760         {\n",
      "1761             PyErr_SetString(\n",
      "1762                 PyExc_ValueError,\n",
      "1763                 \"CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)\");\n",
      "1764             Py_XDECREF(new_other);\n",
      "1765             return -1;\n",
      "1766         }\n",
      "1767         // if we're broadcasting other, then make sure it has stride 0\n",
      "1768         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])\n",
      "1769             || (other_strides[i] == 0));\n",
      "1770         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "1771     }\n",
      "1772 \n",
      "1773     if (size==0)\n",
      "1774     {\n",
      "1775         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);\n",
      "1776         if (!(other_size == 0 || other_size == 1))\n",
      "1777         {\n",
      "1778             PyErr_SetString(\n",
      "1779                 PyExc_ValueError,\n",
      "1780                 \"CudaNdarray_inplace_elemwise cannot work inplace on\"\n",
      "1781                 \" un-initialized array when the new value have more than\"\n",
      "1782                 \" 0 or 1 broadcastable dimensions\");\n",
      "1783             Py_XDECREF(new_other);\n",
      "1784             return 0;\n",
      "1785         }\n",
      "1786         Py_XDECREF(new_other);\n",
      "1787         return 0;\n",
      "1788     }\n",
      "1789 \n",
      "1790     switch(self->nd)\n",
      "1791     {\n",
      "1792         case 0:\n",
      "1793             {\n",
      "1794                 dim3 n_blocks(1, 1, 1);\n",
      "1795                 dim3 n_threads(1);\n",
      "1796                 k3<<<n_blocks, n_threads>>>(\n",
      "1797                         1, //d0\n",
      "1798                         1, //d1\n",
      "1799                         1, //d2\n",
      "1800                         CudaNdarray_DEV_DATA(self),\n",
      "1801                         1, //strides\n",
      "1802                         1,\n",
      "1803                         1,\n",
      "1804                         CudaNdarray_DEV_DATA(other),\n",
      "1805                         1, //strides\n",
      "1806                         1,\n",
      "1807                         1);\n",
      "1808                 CNDA_THREAD_SYNC;\n",
      "1809                 cudaError_t err = cudaGetLastError();\n",
      "1810                 if (cudaSuccess != err)\n",
      "1811                 {\n",
      "1812                     PyErr_Format(\n",
      "1813                         PyExc_RuntimeError,\n",
      "1814                         \"CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\\n\",\n",
      "1815                         \"k3\",\n",
      "1816                         cudaGetErrorString(err));\n",
      "1817                     Py_XDECREF(new_other);\n",
      "1818                     return -1;\n",
      "1819                 }\n",
      "1820             }\n",
      "1821             break;\n",
      "1822         case 1:\n",
      "1823             {\n",
      "1824                 dim3 n_blocks(1, 1, 1);\n",
      "1825                 dim3 n_threads(\n",
      "1826                         std::min(\n",
      "1827                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1828                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1829                 k3<<<n_blocks, n_threads>>>(\n",
      "1830                         1, //dimensions\n",
      "1831                         1,\n",
      "1832                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1833                         CudaNdarray_DEV_DATA(self),\n",
      "1834                         1, //strides\n",
      "1835                         1,\n",
      "1836                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1837                         CudaNdarray_DEV_DATA(other),\n",
      "1838                         1, //strides\n",
      "1839                         1,\n",
      "1840                         other_strides[0]);\n",
      "1841                 CNDA_THREAD_SYNC;\n",
      "1842                 cudaError_t err = cudaGetLastError();\n",
      "1843                 if (cudaSuccess != err)\n",
      "1844                 {\n",
      "1845                     PyErr_Format(\n",
      "1846                         PyExc_RuntimeError,\n",
      "1847                         \"CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\\n\",\n",
      "1848                         \"k3\",\n",
      "1849                         cudaGetErrorString(err));\n",
      "1850                     Py_XDECREF(new_other);\n",
      "1851                     return -1;\n",
      "1852                 }\n",
      "1853             }\n",
      "1854             break;\n",
      "1855         case 2:\n",
      "1856             {\n",
      "1857                 //TODO:  if both self and other are f-contiguous\n",
      "1858                 //       Then flip the block and thread dimensions\n",
      "1859                 //       to make contiguous reads & writes\n",
      "1860                 dim3 n_blocks(1,\n",
      "1861                         std::min(\n",
      "1862                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1863                             NUM_VECTOR_OP_BLOCKS));\n",
      "1864                 dim3 n_threads(\n",
      "1865                         std::min(\n",
      "1866                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1867                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1868                 k3<<<n_blocks, n_threads>>>(1,\n",
      "1869                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1870                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1871                         CudaNdarray_DEV_DATA(self),\n",
      "1872                         1,\n",
      "1873                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1874                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1875                         CudaNdarray_DEV_DATA(other),\n",
      "1876                         1,\n",
      "1877                         other_strides[0],\n",
      "1878                         other_strides[1]);\n",
      "1879                 CNDA_THREAD_SYNC;\n",
      "1880                 cudaError_t err = cudaGetLastError();\n",
      "1881                 if (cudaSuccess != err)\n",
      "1882                 {\n",
      "1883                     PyErr_Format(\n",
      "1884                         PyExc_RuntimeError,\n",
      "1885                         \"CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\\n\",\n",
      "1886                         \"k3\",\n",
      "1887                         cudaGetErrorString(err));\n",
      "1888                     Py_XDECREF(new_other);\n",
      "1889                     return -1;\n",
      "1890                 }\n",
      "1891             }\n",
      "1892             break;\n",
      "1893         case 3:\n",
      "1894             {\n",
      "1895                 //TODO:  Dimshuffle so that at least one of the arrays\n",
      "1896                 //       has a contiguous dimension on the thread idx.\n",
      "1897                 dim3 n_blocks(\n",
      "1898                         std::min(\n",
      "1899                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1900                             NUM_VECTOR_OP_BLOCKS),\n",
      "1901                         CudaNdarray_HOST_DIMS(self)[1]);\n",
      "1902                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1903                     n_blocks.y /= 2;\n",
      "1904                 dim3 n_threads(\n",
      "1905                         std::min(\n",
      "1906                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1907                             NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "1908                 k3<<<n_blocks, n_threads>>>(\n",
      "1909                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1910                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1911                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1912                         CudaNdarray_DEV_DATA(self),\n",
      "1913                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1914                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1915                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1916                         CudaNdarray_DEV_DATA(other),\n",
      "1917                         other_strides[0],\n",
      "1918                         other_strides[1],\n",
      "1919                         other_strides[2]);\n",
      "1920                 CNDA_THREAD_SYNC;\n",
      "1921                 cudaError_t err = cudaGetLastError();\n",
      "1922                 if (cudaSuccess != err)\n",
      "1923                 {\n",
      "1924                     PyErr_Format(\n",
      "1925                         PyExc_RuntimeError,\n",
      "1926                         \"CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\\n\",\n",
      "1927                         \"k3\",\n",
      "1928                         cudaGetErrorString(err));\n",
      "1929                     Py_XDECREF(new_other);\n",
      "1930                     return -1;\n",
      "1931                 }\n",
      "1932             }\n",
      "1933             break;\n",
      "1934         case 4:\n",
      "1935             {\n",
      "1936                 dim3 n_blocks(\n",
      "1937                         std::min(\n",
      "1938                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "1939                             NUM_VECTOR_OP_BLOCKS),\n",
      "1940                         CudaNdarray_HOST_DIMS(self)[1]\n",
      "1941                         );\n",
      "1942                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1943                     n_blocks.y /= 2;\n",
      "1944                 dim3 n_threads(\n",
      "1945                         std::min(\n",
      "1946                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1947                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1948                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1949                             );\n",
      "1950                 k4<<<n_blocks, n_threads>>>(\n",
      "1951                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "1952                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "1953                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "1954                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "1955                         CudaNdarray_DEV_DATA(self),\n",
      "1956                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "1957                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "1958                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "1959                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "1960                         CudaNdarray_DEV_DATA(other),\n",
      "1961                         other_strides[0],\n",
      "1962                         other_strides[1],\n",
      "1963                         other_strides[2],\n",
      "1964                         other_strides[3]);\n",
      "1965                 CNDA_THREAD_SYNC;\n",
      "1966                 cudaError_t err = cudaGetLastError();\n",
      "1967                 if (cudaSuccess != err)\n",
      "1968                 {\n",
      "1969                     PyErr_Format(\n",
      "1970                         PyExc_RuntimeError,\n",
      "1971                         \"CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\\n\",\n",
      "1972                         \"k4\",\n",
      "1973                         cudaGetErrorString(err));\n",
      "1974                     Py_XDECREF(new_other);\n",
      "1975                     return -1;\n",
      "1976                 }\n",
      "1977             }\n",
      "1978             break;\n",
      "1979         case 5:\n",
      "1980             {\n",
      "1981                 dim3 n_blocks(\n",
      "1982                         std::min(\n",
      "1983                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1984                             NUM_VECTOR_OP_BLOCKS),\n",
      "1985                         CudaNdarray_HOST_DIMS(self)[2]);\n",
      "1986                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "1987                     n_blocks.y /= 2;\n",
      "1988                 dim3 n_threads(\n",
      "1989                         std::min(\n",
      "1990                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "1991                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "1992                     //TODO: DON\"T YOU NEED OT PUT DIMS[3] in here???\n",
      "1993                     );\n",
      "1994                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)\n",
      "1995                 {\n",
      "1996                      k4<<<n_blocks, n_threads>>>(\n",
      "1997                             CudaNdarray_HOST_DIMS(self)[1],\n",
      "1998                             CudaNdarray_HOST_DIMS(self)[2],\n",
      "1999                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2000                             CudaNdarray_HOST_DIMS(self)[4],\n",
      "2001                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2002                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2003                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2004                             CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2005                             CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2006                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],\n",
      "2007                             other_strides[1],\n",
      "2008                             other_strides[2],\n",
      "2009                             other_strides[3],\n",
      "2010                             other_strides[4]);\n",
      "2011                     CNDA_THREAD_SYNC;\n",
      "2012                     cudaError_t err = cudaGetLastError();\n",
      "2013                     if( cudaSuccess != err)\n",
      "2014                     {\n",
      "2015                         PyErr_Format(\n",
      "2016                             PyExc_RuntimeError,\n",
      "2017                             \"CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\\n\",\n",
      "2018                             \"k5 with loop over k4\",\n",
      "2019                             cudaGetErrorString(err),\n",
      "2020                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);\n",
      "2021                         Py_XDECREF(new_other);\n",
      "2022                         return -1;\n",
      "2023                     }\n",
      "2024                 }\n",
      "2025             }\n",
      "2026             break;\n",
      "2027         case 6:\n",
      "2028             {\n",
      "2029                 dim3 n_blocks(\n",
      "2030                         std::min(\n",
      "2031                             CudaNdarray_HOST_DIMS(self)[0],\n",
      "2032                             NUM_VECTOR_OP_BLOCKS),\n",
      "2033                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2034                         CudaNdarray_HOST_DIMS(self)[2]\n",
      "2035                         );\n",
      "2036                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)\n",
      "2037                     n_blocks.y /= 2;\n",
      "2038                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1\n",
      "2039                 // (compute capabilities 2.0) support 65535 for n_blocks.z\n",
      "2040                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)\n",
      "2041                 //    n_blocks.z /= 2;\n",
      "2042                 n_blocks.z = 1;\n",
      "2043                 dim3 n_threads(\n",
      "2044                         std::min(\n",
      "2045                             CudaNdarray_HOST_DIMS(self)[3],\n",
      "2046                             NUM_VECTOR_OP_THREADS_PER_BLOCK)\n",
      "2047                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???\n",
      "2048                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???\n",
      "2049                             );\n",
      "2050                 k6<<<n_blocks, n_threads>>>(\n",
      "2051                         CudaNdarray_HOST_DIMS(self)[0],\n",
      "2052                         CudaNdarray_HOST_DIMS(self)[1],\n",
      "2053                         CudaNdarray_HOST_DIMS(self)[2],\n",
      "2054                         CudaNdarray_HOST_DIMS(self)[3],\n",
      "2055                         CudaNdarray_HOST_DIMS(self)[4],\n",
      "2056                         CudaNdarray_HOST_DIMS(self)[5],\n",
      "2057                         CudaNdarray_DEV_DATA(self),\n",
      "2058                         CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2059                         CudaNdarray_HOST_STRIDES(self)[1],\n",
      "2060                         CudaNdarray_HOST_STRIDES(self)[2],\n",
      "2061                         CudaNdarray_HOST_STRIDES(self)[3],\n",
      "2062                         CudaNdarray_HOST_STRIDES(self)[4],\n",
      "2063                         CudaNdarray_HOST_STRIDES(self)[5],\n",
      "2064                         CudaNdarray_DEV_DATA(other),\n",
      "2065                         other_strides[0],\n",
      "2066                         other_strides[1],\n",
      "2067                         other_strides[2],\n",
      "2068                         other_strides[3],\n",
      "2069                         other_strides[4],\n",
      "2070                         other_strides[5]);\n",
      "2071                 CNDA_THREAD_SYNC;\n",
      "2072                 cudaError_t err = cudaGetLastError();\n",
      "2073                 if (cudaSuccess != err)\n",
      "2074                 {\n",
      "2075                     PyErr_Format(\n",
      "2076                         PyExc_RuntimeError,\n",
      "2077                         \"CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\\n\",\n",
      "2078                         \"k6\",\n",
      "2079                         cudaGetErrorString(err),\n",
      "2080                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,\n",
      "2081                         (long) n_threads.x);\n",
      "2082                     Py_XDECREF(new_other);\n",
      "2083                     return -1;\n",
      "2084                 }\n",
      "2085             }\n",
      "2086             break;\n",
      "2087         default:\n",
      "2088         {\n",
      "2089             PyErr_Format(\n",
      "2090                 PyExc_NotImplementedError,\n",
      "2091                 \"inplace_elemwise w nd=%i\\n\",\n",
      "2092                 self->nd);\n",
      "2093             Py_XDECREF(new_other);\n",
      "2094             return -1;\n",
      "2095         }\n",
      "2096     }\n",
      "2097     if (verbose)\n",
      "2098         fprintf(stderr, \"INPLACE ADD/DIV end\\n\");\n",
      "2099     Py_XDECREF(new_other);\n",
      "2100     return 0;\n",
      "2101 }\n",
      "2102 \n",
      "2103 /*\n",
      "2104  * We need this inplace Add to support IncSubTensor\n",
      "2105  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2106  */\n",
      "2107 // Will be called by __iadd__ in Python\n",
      "2108 PyObject *\n",
      "2109 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)\n",
      "2110 {\n",
      "2111     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))\n",
      "2112     {\n",
      "2113         return NULL;\n",
      "2114     }\n",
      "2115     Py_INCREF(py_self);\n",
      "2116     return py_self;\n",
      "2117 }\n",
      "2118 \n",
      "2119 /*\n",
      "2120  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options\n",
      "2121  * It returns py_self on success with an additional reference. Else NULL.\n",
      "2122  */\n",
      "2123 // Will be called by __idiv__ in Python\n",
      "2124 static PyObject *\n",
      "2125 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)\n",
      "2126 {\n",
      "2127     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))\n",
      "2128     {\n",
      "2129         return NULL;\n",
      "2130     }\n",
      "2131     Py_INCREF(py_self);\n",
      "2132     return py_self;\n",
      "2133 }\n",
      "2134 \n",
      "2135 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.\n",
      "2136 #if PY_MAJOR_VERSION == 3\n",
      "2137 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2138 {\n",
      "2139     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2140     0,  //binaryfunc nb_subtract;\n",
      "2141     0,  //binaryfunc nb_multiply;\n",
      "2142     0,  //binaryfunc nb_remainder;\n",
      "2143     0,  //binaryfunc nb_divmod;\n",
      "2144     0,  //ternaryfunc nb_power;\n",
      "2145     0,  //unaryfunc nb_negative;\n",
      "2146     0,  //unaryfunc nb_positive;\n",
      "2147     0,  //unaryfunc nb_absolute;\n",
      "2148     0,  //inquiry nb_bool;\n",
      "2149     0,  //unaryfunc nb_invert;\n",
      "2150     0,  //binaryfunc nb_lshift;\n",
      "2151     0,  //binaryfunc nb_rshift;\n",
      "2152     0,  //binaryfunc nb_and;\n",
      "2153     0,  //binaryfunc nb_xor;\n",
      "2154     0,  //binaryfunc nb_or;\n",
      "2155     0,  //unaryfunc nb_int;\n",
      "2156     0,  //void *nb_reserved;\n",
      "2157     0,  //unaryfunc nb_float;\n",
      "2158 \n",
      "2159     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2160     0,  //binaryfunc nb_inplace_subtract;\n",
      "2161     0,  //binaryfunc nb_inplace_multiply;\n",
      "2162     0,  //binaryfunc nb_inplace_remainder;\n",
      "2163     0,  //ternaryfunc nb_inplace_power;\n",
      "2164     0,  //binaryfunc nb_inplace_lshift;\n",
      "2165     0,  //binaryfunc nb_inplace_rshift;\n",
      "2166     0,  //binaryfunc nb_inplace_and;\n",
      "2167     0,  //binaryfunc nb_inplace_xor;\n",
      "2168     0,  //binaryfunc nb_inplace_or;\n",
      "2169 \n",
      "2170     0,  //binaryfunc nb_floor_divide;\n",
      "2171     0,  //binaryfunc nb_true_divide;\n",
      "2172     0,  //binaryfunc nb_inplace_floor_divide;\n",
      "2173     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__\n",
      "2174 \n",
      "2175     0,  //unaryfunc nb_index\n",
      "2176 };\n",
      "2177 #else\n",
      "2178 static PyNumberMethods CudaNdarrayNumberMethods =\n",
      "2179 {\n",
      "2180     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__\n",
      "2181     0,  //binaryfunc nb_subtract;      __sub__\n",
      "2182     0,  //binaryfunc nb_multiply;      __mul__\n",
      "2183     0,  //binaryfunc nb_divide;        __div__\n",
      "2184     0,  //binaryfunc nb_remainder;     __mod__\n",
      "2185     0,  //binaryfunc nb_divmod;        __divmod__\n",
      "2186     0,  //ternaryfunc nb_power;        __pow__\n",
      "2187     0,  //unaryfunc nb_negative;       __neg__\n",
      "2188     0,  //unaryfunc nb_positive;       __pos__\n",
      "2189     0,  //unaryfunc nb_absolute;       __abs__\n",
      "2190     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */\n",
      "2191     0,  //unaryfunc nb_invert;         __invert__\n",
      "2192     0,  //binaryfunc nb_lshift;        __lshift__\n",
      "2193     0,  //binaryfunc nb_rshift;        __rshift__\n",
      "2194     0,  //binaryfunc nb_and;           __and__\n",
      "2195     0,  //binaryfunc nb_xor;           __xor__\n",
      "2196     0,  //binaryfunc nb_or;            __or__\n",
      "2197     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */\n",
      "2198     0,  //unaryfunc nb_int;            __int__\n",
      "2199     0,  //unaryfunc nb_long;           __long__\n",
      "2200     0,  //unaryfunc nb_float;          __float__\n",
      "2201     0,  //unaryfunc nb_oct;            __oct__\n",
      "2202     0,  //unaryfunc nb_hex;            __hex__\n",
      "2203 \n",
      "2204     /* Added in release 2.0 */\n",
      "2205     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__\n",
      "2206     0,  //binaryfunc nb_inplace_subtract;      __isub__\n",
      "2207     0,  //binaryfunc nb_inplace_multiply;      __imul__\n",
      "2208     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__\n",
      "2209     0,  //binaryfunc nb_inplace_remainder;     __imod__\n",
      "2210     0,  //ternaryfunc nb_inplace_power;        __ipow__\n",
      "2211     0,  //binaryfunc nb_inplace_lshift;        __ilshift__\n",
      "2212     0,  //binaryfunc nb_inplace_rshift;        __irshift__\n",
      "2213     0,  //binaryfunc nb_inplace_and;           __iand__\n",
      "2214     0,  //binaryfunc nb_inplace_xor;           __ixor__\n",
      "2215     0,  //binaryfunc nb_inplace_or;            __ior__\n",
      "2216 \n",
      "2217     /* Added in release 2.2 */\n",
      "2218     0,  //binaryfunc nb_floor_divide;          __floordiv__\n",
      "2219     0,  //binaryfunc nb_true_divide;           __truediv__\n",
      "2220     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__\n",
      "2221     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;   __itruediv__\n",
      "2222 \n",
      "2223 #if PY_MINOR_VERSION > 4\n",
      "2224     /* Added in release 2.5 */\n",
      "2225     0  //unaryfunc nb_index;  __index__\n",
      "2226 #endif\n",
      "2227 };\n",
      "2228 #endif\n",
      "2229 \n",
      "2230 \n",
      "2231 /////////////////////\n",
      "2232 // Mapping protocol\n",
      "2233 /////////////////////\n",
      "2234 \n",
      "2235 // Will by called by __len__ in Python\n",
      "2236 static Py_ssize_t\n",
      "2237 CudaNdarray_len(PyObject * py_self)\n",
      "2238 {\n",
      "2239     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2240     if (self->nd <= 0)\n",
      "2241     {\n",
      "2242         return (Py_ssize_t) 0;\n",
      "2243     }\n",
      "2244     else\n",
      "2245     {\n",
      "2246         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];\n",
      "2247     }\n",
      "2248 }\n",
      "2249 \n",
      "2250 // Will by called by __getitem__ in Python\n",
      "2251 PyObject *\n",
      "2252 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)\n",
      "2253 {\n",
      "2254     int verbose = 0;\n",
      "2255     if (verbose) fprintf(stderr, \"Subscript .... \\n\");\n",
      "2256     CudaNdarray * self = (CudaNdarray*) py_self;\n",
      "2257     PyObject * py_rval = NULL;\n",
      "2258     CudaNdarray * rval = NULL;\n",
      "2259     PyObject * intobj = NULL;\n",
      "2260 \n",
      "2261     //PyObject_Print(key, stderr, 0);\n",
      "2262 \n",
      "2263     if (key == Py_Ellipsis)\n",
      "2264     {\n",
      "2265         Py_INCREF(py_self);\n",
      "2266         return py_self;\n",
      "2267     }\n",
      "2268     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER\n",
      "2269     //else if (PyInt_Check(key)) //INDEXING BY INTEGER\n",
      "2270     {\n",
      "2271         int d_idx = PyInt_AsLong(intobj);\n",
      "2272         Py_DECREF(intobj); intobj=NULL;\n",
      "2273         //int d_idx = PyInt_AsLong(key);\n",
      "2274         if (self->nd == 0)\n",
      "2275         {\n",
      "2276             PyErr_SetString(PyExc_IndexError, \"0-d arrays can't be indexed\");\n",
      "2277             return NULL;\n",
      "2278         }\n",
      "2279         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2280         int offset = 0;\n",
      "2281 \n",
      "2282         if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2283         {\n",
      "2284             //normal indexing\n",
      "2285             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2286         }\n",
      "2287         else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2288         {\n",
      "2289             //end-based indexing\n",
      "2290             // d_idx is negative\n",
      "2291             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];\n",
      "2292         }\n",
      "2293         else\n",
      "2294         {\n",
      "2295             PyErr_Format(PyExc_IndexError,\n",
      "2296                          \"index out of bounds. Asked %d, but size of %d\",\n",
      "2297                          d_idx, d_dim);\n",
      "2298             return NULL;\n",
      "2299         }\n",
      "2300 \n",
      "2301         //allocate our subtensor view\n",
      "2302         py_rval = CudaNdarray_new_nd(self->nd - 1);\n",
      "2303         rval = (CudaNdarray*) py_rval;\n",
      "2304         if (!rval) return NULL;\n",
      "2305         assert (0 == rval->data_allocated);\n",
      "2306 \n",
      "2307         //initialize the view's data pointer to our own.\n",
      "2308         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))\n",
      "2309         {\n",
      "2310             Py_DECREF(rval);\n",
      "2311             return NULL;\n",
      "2312         }\n",
      "2313         for (int d = 1; d < self->nd; ++d)\n",
      "2314         {\n",
      "2315             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2316             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2317         }\n",
      "2318     }\n",
      "2319     else\n",
      "2320     {\n",
      "2321         PyErr_Clear();\n",
      "2322     }\n",
      "2323     if (PySlice_Check(key)) //INDEXING BY SLICE\n",
      "2324     {\n",
      "2325         if (verbose) fprintf(stderr, \"by slice\\n\");\n",
      "2326         if (self->nd == 0)\n",
      "2327         {\n",
      "2328             PyErr_SetString(PyExc_ValueError, \"cannot slice a 0-d array\");\n",
      "2329             return NULL;\n",
      "2330         }\n",
      "2331 \n",
      "2332         int d_dim = CudaNdarray_HOST_DIMS(self)[0];\n",
      "2333         Py_ssize_t start, stop, step, slen;\n",
      "2334         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))\n",
      "2335         {\n",
      "2336             if (verbose)\n",
      "2337                 fprintf(stderr, \"PySlice_GetIndicesEx failed\\n\");\n",
      "2338             return NULL;\n",
      "2339         }\n",
      "2340         if (verbose)\n",
      "2341         {\n",
      "2342             std::cerr << \"start \" << start << \"\\n\";\n",
      "2343             std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2344             std::cerr << \"step \" << step << \"\\n\";\n",
      "2345             std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2346         }\n",
      "2347 \n",
      "2348         //allocate our subtensor view\n",
      "2349         py_rval = CudaNdarray_new_nd(self->nd);\n",
      "2350         rval = (CudaNdarray*) py_rval;\n",
      "2351         if (!rval) return NULL;\n",
      "2352         assert (0 == rval->data_allocated);\n",
      "2353 \n",
      "2354 \n",
      "2355         //initialize the view's data pointer to our own.\n",
      "2356         if (CudaNdarray_set_device_data(rval,\n",
      "2357                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],\n",
      "2358                     self))\n",
      "2359         {\n",
      "2360             Py_DECREF(rval);\n",
      "2361             return NULL;\n",
      "2362         }\n",
      "2363         //initialize dimension 0 of rval\n",
      "2364         CudaNdarray_set_stride(rval, 0,\n",
      "2365                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "2366         CudaNdarray_set_dim(rval, 0, slen);\n",
      "2367         if (verbose) std::cerr << \"rval stride \" << CudaNdarray_HOST_STRIDES(rval)[0] << \"\\n\";\n",
      "2368         // initialize dimensions > 0 of rval\n",
      "2369         for (int d = 1; d < self->nd; ++d)\n",
      "2370         {\n",
      "2371             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2372             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2373         }\n",
      "2374     }\n",
      "2375     if (PyTuple_Check(key)) //INDEXING BY TUPLE\n",
      "2376     {\n",
      "2377         if (verbose) fprintf(stderr, \"by tuple\\n\");\n",
      "2378         //elements of the tuple can be either integers or slices\n",
      "2379         //the dimensionality of the view we will return is diminished for each slice in the tuple\n",
      "2380 \n",
      "2381         if (PyTuple_Size(key) > self->nd)\n",
      "2382         {\n",
      "2383             PyErr_SetString(PyExc_IndexError, \"index error\");\n",
      "2384             return NULL;\n",
      "2385         }\n",
      "2386 \n",
      "2387         //calculate the number of dimensions in the return value\n",
      "2388         int rval_nd = self->nd;\n",
      "2389         for (int d = 0; d < PyTuple_Size(key); ++d)\n",
      "2390         {\n",
      "2391             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.\n",
      "2392             //So we use PyArray_IsAnyScalar that should covert everything.\n",
      "2393             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));\n",
      "2394         }\n",
      "2395 \n",
      "2396         //allocate our subtensor view\n",
      "2397         py_rval = CudaNdarray_new_nd(rval_nd);\n",
      "2398         rval = (CudaNdarray*) py_rval;\n",
      "2399         if (!rval) return NULL;\n",
      "2400         assert (0 == rval->data_allocated);\n",
      "2401 \n",
      "2402         //initialize the view's data pointer to our own.\n",
      "2403         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))\n",
      "2404         {\n",
      "2405             Py_DECREF(rval);\n",
      "2406             return NULL;\n",
      "2407         }\n",
      "2408 \n",
      "2409         // rval_d will refer to the current dimension in the rval.\n",
      "2410         // It will not be incremented for integer keys, but will be incremented for slice\n",
      "2411         // keys\n",
      "2412         int rval_d = 0;\n",
      "2413 \n",
      "2414         for (int d = 0; d < self->nd; ++d)\n",
      "2415         {\n",
      "2416             // keys can be shorter than self->nd.\n",
      "2417             // when that happens, it means that the remaining dimensions are \"full slices\"\n",
      "2418             if (d >=PyTuple_Size(key))\n",
      "2419             {\n",
      "2420                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2421                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);\n",
      "2422                 ++rval_d;\n",
      "2423             }\n",
      "2424             else\n",
      "2425             {\n",
      "2426                 PyObject * key_d = PyTuple_GetItem(key, d);\n",
      "2427 \n",
      "2428                 if (PySlice_Check(key_d))\n",
      "2429                 {\n",
      "2430                     Py_ssize_t start, stop, step, slen;\n",
      "2431                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))\n",
      "2432                     {\n",
      "2433                         Py_DECREF(rval);\n",
      "2434                         return NULL;\n",
      "2435                     }\n",
      "2436                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2437                     CudaNdarray_set_stride(rval, rval_d,\n",
      "2438                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);\n",
      "2439                     CudaNdarray_set_dim(rval, rval_d, slen);\n",
      "2440                     if (0)\n",
      "2441                     {\n",
      "2442                         std::cerr << \"start \" << start << \"\\n\";\n",
      "2443                         std::cerr << \"stop \" << stop << \"\\n\";\n",
      "2444                         std::cerr << \"step \" << step << \"\\n\";\n",
      "2445                         std::cerr << \"slen \" << slen << \"\\n\";\n",
      "2446                     }\n",
      "2447                     ++rval_d;\n",
      "2448                 }\n",
      "2449                 else if ((intobj=PyNumber_Int(key_d)))\n",
      "2450                 {\n",
      "2451                     assert(PyArray_IsAnyScalar(key_d));\n",
      "2452                     int d_idx = PyInt_AsLong(intobj);\n",
      "2453                     Py_DECREF(intobj);\n",
      "2454                     intobj = NULL;\n",
      "2455                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];\n",
      "2456 \n",
      "2457                     if ((d_idx >= 0) && (d_idx < d_dim))\n",
      "2458                     {\n",
      "2459                         //normal indexing\n",
      "2460                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2461                     }\n",
      "2462                     else if ((d_idx < 0) && (d_idx >= -d_dim))\n",
      "2463                     {\n",
      "2464                         //end-based indexing\n",
      "2465                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];\n",
      "2466                     }\n",
      "2467                     else\n",
      "2468                     {\n",
      "2469                         PyErr_Format(PyExc_IndexError,\n",
      "2470                                      \"index out of bounds. Asked %d for dimensions %d, but size of %d\",\n",
      "2471                                      d_idx, d, d_dim);\n",
      "2472                         Py_DECREF(rval);\n",
      "2473                         return NULL;\n",
      "2474                     }\n",
      "2475                 }\n",
      "2476                 else\n",
      "2477                 {\n",
      "2478                     PyErr_Clear(); // clear the error set by PyNumber_Int\n",
      "2479                     PyErr_SetString(PyExc_IndexError, \"index must be either int or slice\");\n",
      "2480                     Py_DECREF(rval);\n",
      "2481                     return NULL;\n",
      "2482                 }\n",
      "2483             }\n",
      "2484         }\n",
      "2485     }\n",
      "2486     if (py_rval)\n",
      "2487     {\n",
      "2488         if (verbose) fprint_CudaNdarray(stderr, self);\n",
      "2489         if (verbose) fprint_CudaNdarray(stderr, rval);\n",
      "2490     }\n",
      "2491     else\n",
      "2492     {\n",
      "2493         PyErr_SetString(PyExc_NotImplementedError, \"Unknown key type\");\n",
      "2494         return NULL;\n",
      "2495     }\n",
      "2496     return py_rval;\n",
      "2497 }\n",
      "2498 \n",
      "2499 // Will by called by __setitem__ in Python\n",
      "2500 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem\n",
      "2501 // Doesn't handle broadcasting, e.g. a[:] = 5\n",
      "2502 // Can only be assigned from a CudaNdarray on the right side\n",
      "2503 // Or a ndarray\n",
      "2504 // Or a python scalar with value 0 when the left side part is c contiguous.\n",
      "2505 static int\n",
      "2506 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)\n",
      "2507 {\n",
      "2508     int verbose = 0;\n",
      "2509     if (verbose) fprintf(stderr, \"CudaNdarray_setitem start\\n\");\n",
      "2510     // We try to copy directly into this CudaNdarray from the ndarray\n",
      "2511     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);\n",
      "2512     CudaNdarray* new_value = NULL;\n",
      "2513 \n",
      "2514     if(!rval){\n",
      "2515         // CudaNdarray_Subscript failed and set the error msg.\n",
      "2516         Py_XDECREF(rval);\n",
      "2517         return -1;\n",
      "2518     }\n",
      "2519 \n",
      "2520     if(rval != (CudaNdarray*)o &&\n",
      "2521                 (rval->data_allocated ||\n",
      "2522                  // The new array should have a base\n",
      "2523                  !(((CudaNdarray*)rval)->base) ||\n",
      "2524                  // If the original array has no base, the base of the new\n",
      "2525                  // array should be the original one\n",
      "2526                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||\n",
      "2527                  // Else, the two arrays should have the same base\n",
      "2528                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))\n",
      "2529     {\n",
      "2530         // This case shouldn't happen, based on what I see in Subscript\n",
      "2531         // but just in case it happens sometime in the future\n",
      "2532 \n",
      "2533         PyErr_Format(PyExc_RuntimeError,\n",
      "2534                      \"__getitem__ must return a CudaNdarray that refers to\"\n",
      "2535                      \" the original CudaNdarray, not a copy. rval.base=%p\"\n",
      "2536                      \" o.base=%p o=%p\",\n",
      "2537                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);\n",
      "2538         Py_DECREF(rval);\n",
      "2539         return -1;\n",
      "2540     }\n",
      "2541 \n",
      "2542     PyObject * intobj = NULL;\n",
      "2543     if (CudaNdarray_Check(o)  && PyArray_Check(value)){\n",
      "2544         if (verbose)\n",
      "2545             fprintf(stderr,\n",
      "2546                     \"CudaNdarray_setitem dest is a CudaNdarray and\"\n",
      "2547                     \" value is a ndarray\\n\");\n",
      "2548         new_value = (CudaNdarray*) CudaNdarray_New();\n",
      "2549         if(!new_value)\n",
      "2550         {\n",
      "2551             return -1;\n",
      "2552         }\n",
      "2553         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))\n",
      "2554         {\n",
      "2555             Py_XDECREF(new_value);\n",
      "2556             Py_XDECREF(rval);\n",
      "2557             return -1;\n",
      "2558         }\n",
      "2559         value = (PyObject *) new_value;\n",
      "2560     }\n",
      "2561     else if ((intobj=PyNumber_Int(value)))\n",
      "2562     {\n",
      "2563         if (verbose)\n",
      "2564             fprintf(stderr,\n",
      "2565                     \"CudaNdarray_setitem dest and value is a python number\\n\");\n",
      "2566         if(! CudaNdarray_is_c_contiguous(rval)){\n",
      "2567             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2568                  \"CudaNdarray.__setitem__: When the new value is a scalar\"\n",
      "2569                  \" of value 0 the part where we copy to must be c contiguous.\");\n",
      "2570             Py_XDECREF(rval);\n",
      "2571             return -1;\n",
      "2572         }\n",
      "2573 \n",
      "2574         long val = PyInt_AsLong(intobj);\n",
      "2575         Py_DECREF(intobj); intobj=NULL;\n",
      "2576         if (val == 0)\n",
      "2577         {\n",
      "2578             cudaError_t err = cudaMemset(rval->devdata, 0,\n",
      "2579                                          CudaNdarray_SIZE(rval) * sizeof(real));\n",
      "2580             Py_XDECREF(rval);\n",
      "2581             if (err)\n",
      "2582             {\n",
      "2583                 // Clear the error flag, cudaMemset doesn't do it.\n",
      "2584                 // Currently this returns the same thing as err, but if in future\n",
      "2585                 // it returns something else I still don't see why we should ignore\n",
      "2586                 // it.  All we want to do here is reset the flag.\n",
      "2587                 cudaGetLastError();\n",
      "2588                 PyErr_SetString(PyExc_RuntimeError,\n",
      "2589                                 \"CudaNdarray.__setitem__: cudaMemset failed\");\n",
      "2590                 return -1;\n",
      "2591             }\n",
      "2592             return 0;\n",
      "2593         } else {\n",
      "2594             Py_XDECREF(rval);\n",
      "2595             PyErr_SetString(PyExc_NotImplementedError,\n",
      "2596                   \"CudaNdarray.__setitem__: we support setting only python\"\n",
      "2597                   \" scalar of value 0, numpy nd array and CudaNdarray.\");\n",
      "2598                 return -1;\n",
      "2599         }\n",
      "2600     }\n",
      "2601 \n",
      "2602     PyErr_Clear(); // clear PyNumber_Int error.\n",
      "2603 \n",
      "2604     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))\n",
      "2605     {\n",
      "2606         PyErr_SetString(PyExc_TypeError,\n",
      "2607           \"CudaNdarray.__setitem__: left must be a CudaNdarrays and right\"\n",
      "2608           \" must be a CudaNdarrays, an ndarray or a python scalar of value 0.\");\n",
      "2609         Py_XDECREF(new_value);\n",
      "2610         return -1;\n",
      "2611     }\n",
      "2612 \n",
      "2613     if (verbose)\n",
      "2614         fprintf(stderr, \"CudaNdarray_setitem dest and value are CudaNdarray\\n\");\n",
      "2615 \n",
      "2616     if (cnda_copy_structure_to_device(rval))\n",
      "2617     {\n",
      "2618         PyErr_SetString(PyExc_RuntimeError,\n",
      "2619                 \"CudaNdarray.__setitem__: syncing structure to device failed\");\n",
      "2620         Py_DECREF(rval);\n",
      "2621         Py_XDECREF(new_value);\n",
      "2622 \n",
      "2623         if (verbose)\n",
      "2624             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2625         return -1;\n",
      "2626     }\n",
      "2627 \n",
      "2628     PyObject *baseSavedForComparison = rval->base;\n",
      "2629 \n",
      "2630     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))\n",
      "2631     {\n",
      "2632         Py_DECREF((PyObject*)rval);\n",
      "2633         Py_XDECREF(new_value);\n",
      "2634 \n",
      "2635         if (verbose)\n",
      "2636             fprintf(stderr, \"CudaNdarray_setitem error end\\n\");\n",
      "2637         return -1;\n",
      "2638     }\n",
      "2639 \n",
      "2640     assert (rval->base == baseSavedForComparison);\n",
      "2641     assert (rval->dev_structure_fresh);\n",
      "2642 \n",
      "2643     // Clean up locally-created references\n",
      "2644     Py_DECREF(rval);\n",
      "2645     Py_XDECREF(new_value);\n",
      "2646 \n",
      "2647     return 0;\n",
      "2648 }\n",
      "2649 \n",
      "2650 \n",
      "2651 PyMappingMethods CudaNdarrayMappingMethods = {\n",
      "2652     CudaNdarray_len, //lenfunc mp_length;               __len__\n",
      "2653     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__\n",
      "2654     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__\n",
      "2655 };\n",
      "2656 \n",
      "2657 ////////////////////\n",
      "2658 //\n",
      "2659 ////////////////////\n",
      "2660 \n",
      "2661 static PyObject *\n",
      "2662 CudaNdarray_get_shape(CudaNdarray *self, void *closure)\n",
      "2663 {\n",
      "2664     if (self->nd < 0)\n",
      "2665     {\n",
      "2666         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2667         return NULL;\n",
      "2668     }\n",
      "2669     PyObject * rval = PyTuple_New(self->nd);\n",
      "2670     for (int i = 0; i < self->nd; ++i)\n",
      "2671     {\n",
      "2672         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))\n",
      "2673         {\n",
      "2674             Py_XDECREF(rval);\n",
      "2675             return NULL;\n",
      "2676         }\n",
      "2677 \n",
      "2678     }\n",
      "2679     return rval;\n",
      "2680 }\n",
      "2681 \n",
      "2682 static int\n",
      "2683 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2684 {\n",
      "2685     PyErr_SetString(PyExc_NotImplementedError, \"TODO: call reshape\");\n",
      "2686     return -1;\n",
      "2687 }\n",
      "2688 \n",
      "2689 static PyObject *\n",
      "2690 CudaNdarray_get_strides(CudaNdarray *self, void *closure)\n",
      "2691 {\n",
      "2692     if (self->nd < 0)\n",
      "2693     {\n",
      "2694         PyErr_SetString(PyExc_ValueError, \"CudaNdarray not initialized\");\n",
      "2695         return NULL;\n",
      "2696     }\n",
      "2697     PyObject * rval = PyTuple_New(self->nd);\n",
      "2698     for (int i = 0; i < self->nd; ++i)\n",
      "2699     {\n",
      "2700         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))\n",
      "2701         {\n",
      "2702             Py_XDECREF(rval);\n",
      "2703             return NULL;\n",
      "2704         }\n",
      "2705 \n",
      "2706     }\n",
      "2707     return rval;\n",
      "2708 }\n",
      "2709 \n",
      "2710 static int\n",
      "2711 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2712 {\n",
      "2713     //npy_intp newstrides_bytes[PyTuple_Size(value)];\n",
      "2714     if (PyTuple_Check(value)){\n",
      "2715         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2716             PyErr_SetString(PyExc_ValueError,\n",
      "2717                             \"The new strides tuple must have the same length\"\n",
      "2718                             \" as the number of dimensions\");\n",
      "2719             return -1;\n",
      "2720         }\n",
      "2721     }else if (PyList_Check(value)){\n",
      "2722         if (PyList_Size(value) != CudaNdarray_NDIM(self)){\n",
      "2723             PyErr_SetString(PyExc_ValueError,\n",
      "2724                             \"The new strides list must have the same length\"\n",
      "2725                             \" as the number of dimensions\");\n",
      "2726             return -1;\n",
      "2727         }\n",
      "2728     }else{\n",
      "2729         PyErr_SetString(PyExc_ValueError,\n",
      "2730                         \"The new strides need to be encoded in a tuple or list\");\n",
      "2731         return -1;\n",
      "2732     }\n",
      "2733     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));\n",
      "2734     if (PyTuple_Check(value)){\n",
      "2735         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2736             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));\n",
      "2737             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2738         }\n",
      "2739     }else if (PyList_Check(value)){\n",
      "2740         for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2741             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));\n",
      "2742             //newstrides_bytes[i] = newstrides[i] * 4;\n",
      "2743         }\n",
      "2744     }\n",
      "2745     /*\n",
      "2746     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem\n",
      "2747     // to do it.\n",
      "2748     npy_intp dims[PyTuple_Size(value)];\n",
      "2749     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2750         dims[i] = CudaNdarray_HOST_DIMS(self)[i];\n",
      "2751     }\n",
      "2752     if (!PyArray_CheckStrides(4,\n",
      "2753                               CudaNdarray_NDIM(self),\n",
      "2754                               0, 0,\n",
      "2755                               dims,\n",
      "2756                               newstrides_bytes)){\n",
      "2757         PyErr_SetString(PyExc_ValueError, \"bad new strides\");\n",
      "2758         return -1;\n",
      "2759         }\n",
      "2760     */\n",
      "2761     for(int i=0; i < CudaNdarray_NDIM(self); i++){\n",
      "2762         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "2763     }\n",
      "2764     return 0;\n",
      "2765 }\n",
      "2766 \n",
      "2767 static PyObject *\n",
      "2768 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)\n",
      "2769 {\n",
      "2770     float * p =  CudaNdarray_DEV_DATA(self);\n",
      "2771     //printf(\"get_dev_data %p %li \\n\", p, (long int)p );\n",
      "2772     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));\n",
      "2773 }\n",
      "2774 \n",
      "2775 static int\n",
      "2776 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)\n",
      "2777 {\n",
      "2778     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);\n",
      "2779     //printf(\"set_dev_data %p %li \\n\",(float*)newdevdata ,newdevdata);\n",
      "2780     if (PyErr_Occurred())\n",
      "2781     {\n",
      "2782         return -1;\n",
      "2783     }\n",
      "2784     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);\n",
      "2785 }\n",
      "2786 \n",
      "2787 static PyObject *\n",
      "2788 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)\n",
      "2789 {\n",
      "2790     return PyString_FromString(\"float32\");\n",
      "2791 }\n",
      "2792 \n",
      "2793 static PyObject *\n",
      "2794 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)\n",
      "2795 {\n",
      "2796     return PyInt_FromLong(self->nd);\n",
      "2797 }\n",
      "2798 \n",
      "2799 static PyObject *\n",
      "2800 CudaNdarray_get_base(CudaNdarray *self, void *closure)\n",
      "2801 {\n",
      "2802     PyObject * base = self->base;\n",
      "2803     if (!base)\n",
      "2804     {\n",
      "2805         // We cannot return a NULL pointer, use None instead\n",
      "2806         base = Py_None;\n",
      "2807     }\n",
      "2808     Py_INCREF(base);\n",
      "2809     return base;\n",
      "2810 }\n",
      "2811 \n",
      "2812 void put_in_dict(PyObject * dict, const char * key, int val)\n",
      "2813 {\n",
      "2814   PyObject * k = PyString_FromString(key);\n",
      "2815   PyObject * v = PyInt_FromLong(val);\n",
      "2816   PyDict_SetItem(dict, k, v);\n",
      "2817   Py_DECREF(k);\n",
      "2818   Py_DECREF(v);\n",
      "2819 }\n",
      "2820 \n",
      "2821 PyObject *\n",
      "2822 GetDeviceProperties(PyObject* _unused, PyObject* args)\n",
      "2823 {\n",
      "2824   int dev_id = -1;\n",
      "2825   if (! PyArg_ParseTuple(args, \"i\", &dev_id))\n",
      "2826     return NULL;\n",
      "2827   cudaDeviceProp deviceProp;\n",
      "2828   cudaGetDeviceProperties(&deviceProp, dev_id);\n",
      "2829 \n",
      "2830   PyObject * dict = PyDict_New();\n",
      "2831   PyObject * str= PyString_FromString(\"name\");\n",
      "2832   PyObject * i = PyString_FromString(deviceProp.name);\n",
      "2833   PyDict_SetItem(dict, str, i);\n",
      "2834   Py_DECREF(str);\n",
      "2835   Py_DECREF(i);\n",
      "2836 \n",
      "2837   put_in_dict(dict, \"major\", deviceProp.major);\n",
      "2838   put_in_dict(dict, \"minor\", deviceProp.minor);\n",
      "2839 #if CUDART_VERSION >= 2020\n",
      "2840   int driverVersion = 0, runtimeVersion = 0;\n",
      "2841   cudaDriverGetVersion(&driverVersion);\n",
      "2842   cudaRuntimeGetVersion(&runtimeVersion);\n",
      "2843   put_in_dict(dict, \"driverVersion\", driverVersion);\n",
      "2844   put_in_dict(dict, \"runtimeVersion\", runtimeVersion);\n",
      "2845 #endif\n",
      "2846 #if CUDART_VERSION >= 2000\n",
      "2847 \n",
      "2848   put_in_dict(dict, \"multiProcessorCount\", deviceProp.multiProcessorCount);\n",
      "2849   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.\n",
      "2850   int sm_cores = -1;\n",
      "2851   if(deviceProp.major==1)\n",
      "2852     sm_cores = 32;\n",
      "2853   else if(deviceProp.major==2 && deviceProp.minor==0)\n",
      "2854     sm_cores = 32;\n",
      "2855   else if(deviceProp.major==2 && deviceProp.minor==1)\n",
      "2856     sm_cores = 48;\n",
      "2857   put_in_dict(dict, \"coresCount\", sm_cores * deviceProp.multiProcessorCount);\n",
      "2858 #endif\n",
      "2859   put_in_dict(dict, \"totalConstMem\", deviceProp.totalConstMem);\n",
      "2860   put_in_dict(dict, \"sharedMemPerBlock\", deviceProp.sharedMemPerBlock);\n",
      "2861   put_in_dict(dict, \"regsPerBlock\", deviceProp.regsPerBlock);\n",
      "2862   put_in_dict(dict, \"warpSize\", deviceProp.warpSize);\n",
      "2863   put_in_dict(dict, \"maxThreadsPerBlock\", deviceProp.maxThreadsPerBlock);\n",
      "2864   put_in_dict(dict, \"maxThreadsDim0\", deviceProp.maxThreadsDim[0]);\n",
      "2865   put_in_dict(dict, \"maxThreadsDim1\", deviceProp.maxThreadsDim[1]);\n",
      "2866   put_in_dict(dict, \"maxThreadsDim2\", deviceProp.maxThreadsDim[2]);\n",
      "2867   put_in_dict(dict, \"maxGridSize0\", deviceProp.maxGridSize[0]);\n",
      "2868   put_in_dict(dict, \"maxGridSize1\", deviceProp.maxGridSize[1]);\n",
      "2869   put_in_dict(dict, \"maxGridSize2\", deviceProp.maxGridSize[2]);\n",
      "2870   put_in_dict(dict, \"memPitch\", deviceProp.memPitch);\n",
      "2871   put_in_dict(dict, \"textureAlignment\", deviceProp.textureAlignment);\n",
      "2872   put_in_dict(dict, \"clockRate\", deviceProp.clockRate);\n",
      "2873 #if CUDART_VERSION >= 2000\n",
      "2874   put_in_dict(dict, \"deviceOverlap\", deviceProp.deviceOverlap);\n",
      "2875 #endif\n",
      "2876 #if CUDART_VERSION >= 2020\n",
      "2877   put_in_dict(dict, \"kernelExecTimeoutEnabled\", deviceProp.kernelExecTimeoutEnabled);\n",
      "2878   put_in_dict(dict, \"integrated\", deviceProp.integrated);\n",
      "2879   put_in_dict(dict, \"canMapHostMemory\", deviceProp.canMapHostMemory);\n",
      "2880   put_in_dict(dict, \"computeMode\", deviceProp.computeMode);\n",
      "2881   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context\n",
      "2882 #endif\n",
      "2883 #if CUDART_VERSION >= 3000\n",
      "2884   put_in_dict(dict, \"concurrentKernels\", deviceProp.concurrentKernels);\n",
      "2885 #endif\n",
      "2886 #if CUDART_VERSION >= 3010\n",
      "2887   put_in_dict(dict, \"ECCEnabled\", deviceProp.ECCEnabled);\n",
      "2888 #endif\n",
      "2889 #if CUDART_VERSION >= 3020\n",
      "2890   put_in_dict(dict, \"tccDriver\", deviceProp.tccDriver);\n",
      "2891 #endif\n",
      "2892 \n",
      "2893   return dict;\n",
      "2894 }\n",
      "2895 \n",
      "2896 /*\n",
      "2897  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.\n",
      "2898  */\n",
      "2899 PyObject *\n",
      "2900 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)\n",
      "2901 {\n",
      "2902     size_t free = 0, total = 0;\n",
      "2903     if(g_gpu_context_active == 0){\n",
      "2904         PyErr_Format(PyExc_RuntimeError, \"No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.\");\n",
      "2905         return NULL;\n",
      "2906     }\n",
      "2907 \n",
      "2908     cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "2909     if (err != cudaSuccess){\n",
      "2910         // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "2911         // Currently this returns the same thing as err, but if in future\n",
      "2912         // it returns something else I still don't see why we should ignore\n",
      "2913         // it.  All we want to do here is reset the flag.\n",
      "2914         cudaGetLastError();\n",
      "2915         PyErr_Format(PyExc_RuntimeError,\n",
      "2916                      \"Error while getting memory info about the gpu: %s\",\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2917                      cudaGetErrorString(err));\n",
      "2918         return NULL;\n",
      "2919     }\n",
      "2920     return PyTuple_Pack(2, PyLong_FromSize_t(free), PyLong_FromSize_t(total));\n",
      "2921 }\n",
      "2922 \n",
      "2923 /*\n",
      "2924  * Synchronize with all the gpu device stream.\n",
      "2925  */\n",
      "2926 PyObject *\n",
      "2927 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)\n",
      "2928 {\n",
      "2929     CNDA_BEGIN_ALLOW_THREADS\n",
      "2930     cudaThreadSynchronize();\n",
      "2931     CNDA_END_ALLOW_THREADS\n",
      "2932     Py_INCREF(Py_None);\n",
      "2933     return Py_None;\n",
      "2934 }\n",
      "2935 \n",
      "2936 /*\n",
      "2937  * Exist and return true if we link with cublas v2.\n",
      "2938  */\n",
      "2939 PyObject *\n",
      "2940 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)\n",
      "2941 {\n",
      "2942     Py_INCREF(Py_True);\n",
      "2943     return Py_True;\n",
      "2944 }\n",
      "2945 \n",
      "2946 PyObject *\n",
      "2947 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)\n",
      "2948 {\n",
      "2949     void * rval = NULL;\n",
      "2950     cudaError_t err;\n",
      "2951     int num_gpus = 0;\n",
      "2952 \n",
      "2953     err = cudaGetDeviceCount(&num_gpus);\n",
      "2954     if (cudaSuccess != err){\n",
      "2955         printf(\"ERR!\\\\n\");\n",
      "2956             PyErr_Format(PyExc_RuntimeError,\n",
      "2957                          \"Not able to get number of GPUs (%s).\",\n",
      "2958                          cudaGetErrorString(err));\n",
      "2959             return NULL;\n",
      "2960     }\n",
      "2961 \n",
      "2962     for (int device = 0; device < num_gpus; device++) {\n",
      "2963         cudaSetDevice(device);\n",
      "2964         err = cudaDeviceSynchronize(); // << CUDA context gets created here.\n",
      "2965         cudaGetLastError(); // reset the error state\n",
      "2966         if (cudaSuccess == err)\n",
      "2967             break;\n",
      "2968     }\n",
      "2969 \n",
      "2970     if (cudaSuccess != err){\n",
      "2971             printf(\"ERR!\\\\n\");\n",
      "2972                 PyErr_Format(PyExc_RuntimeError,\n",
      "2973                              \"Not able to select available GPU from %d cards (%s).\",\n",
      "2974                              num_gpus, cudaGetErrorString(err));\n",
      "2975                 return NULL;\n",
      "2976     }\n",
      "2977 \n",
      "2978     Py_INCREF(Py_None);\n",
      "2979     return Py_None;\n",
      "2980 }\n",
      "2981 \n",
      "2982 #if COMPUTE_GPU_MEM_USED\n",
      "2983 /*\n",
      "2984  * Return the size in bytes that Theano currently have allocated on the gpu.\n",
      "2985  */\n",
      "2986 PyObject *\n",
      "2987 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)\n",
      "2988 {\n",
      "2989     PyObject* a = PyLong_FromSize_t(_allocated_size);\n",
      "2990     PyObject* b = PyLong_FromSize_t(_max_allocated_size);\n",
      "2991 \n",
      "2992     PyObject* tuple = PyTuple_New(2);\n",
      "2993     PyTuple_SetItem(tuple, 0, a);\n",
      "2994     PyTuple_SetItem(tuple, 1, b);\n",
      "2995     return tuple;\n",
      "2996 }\n",
      "2997 #endif\n",
      "2998 \n",
      "2999 static PyGetSetDef CudaNdarray_getset[] = {\n",
      "3000     {\"shape\",\n",
      "3001         (getter)CudaNdarray_get_shape,\n",
      "3002         (setter)CudaNdarray_set_shape,\n",
      "3003         \"shape of this ndarray (tuple)\",\n",
      "3004         NULL},\n",
      "3005     {\"_strides\",\n",
      "3006         (getter)CudaNdarray_get_strides,\n",
      "3007         (setter)CudaNdarray_set_strides,\n",
      "3008         \"data pointer strides (in elements)\",\n",
      "3009         NULL},\n",
      "3010     {\"strides\",\n",
      "3011         (getter)CudaNdarray_get_strides,\n",
      "3012         (setter)CudaNdarray_set_strides,\n",
      "3013         \"data pointer strides (in elements)\",\n",
      "3014         NULL},\n",
      "3015     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.\n",
      "3016     {\"gpudata\",\n",
      "3017         (getter)CudaNdarray_get_dev_data,\n",
      "3018         NULL,\n",
      "3019         \"device data pointer\",\n",
      "3020         NULL},\n",
      "3021     {\"_dev_data\",\n",
      "3022         (getter)CudaNdarray_get_dev_data,\n",
      "3023         (setter)CudaNdarray_set_dev_data,\n",
      "3024         \"device data pointer\",\n",
      "3025         NULL},\n",
      "3026     {\"dtype\",\n",
      "3027         (getter)CudaNdarray_get_dtype,\n",
      "3028         NULL,\n",
      "3029         \"The dtype of the element. Now always float32\",\n",
      "3030         NULL},\n",
      "3031     {\"size\",\n",
      "3032         (getter)CudaNdarray_SIZE_Object,\n",
      "3033         NULL,\n",
      "3034         \"The number of elements in this object.\",\n",
      "3035         NULL},\n",
      "3036     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?\n",
      "3037     {\"mem_size\",\n",
      "3038         (getter)CudaNdarray_SIZE_Object,\n",
      "3039         NULL,\n",
      "3040         \"The number of elements in this object.\",\n",
      "3041         NULL},\n",
      "3042     {\"ndim\",\n",
      "3043         (getter)CudaNdarray_get_ndim,\n",
      "3044         NULL,\n",
      "3045         \"The number of dimensions in this object.\",\n",
      "3046         NULL},\n",
      "3047     {\"base\",\n",
      "3048         (getter)CudaNdarray_get_base,\n",
      "3049         NULL,\n",
      "3050         \"If this ndarray is a view, base is the original ndarray.\",\n",
      "3051         NULL},\n",
      "3052 \n",
      "3053     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3054 };\n",
      "3055 \n",
      "3056 PyObject *CudaNdarray_repr(PyObject *self)\n",
      "3057 {\n",
      "3058     CudaNdarray *object = (CudaNdarray *)self;\n",
      "3059     PyObject * np_object = CudaNdarray_CreateArrayObj(object);\n",
      "3060     PyObject * str = PyObject_Str((PyObject *) np_object);\n",
      "3061     char * cstr = PyString_AsString(str);\n",
      "3062     PyObject * out = PyString_FromFormat(\"%s%s%s\",\n",
      "3063                         \"CudaNdarray(\",\n",
      "3064                         cstr,\n",
      "3065                         \")\");\n",
      "3066     Py_DECREF(str);\n",
      "3067     Py_DECREF(np_object);\n",
      "3068     #if PY_MAJOR_VERSION >= 3\n",
      "3069     // In Python 3 PyString_FromFormat return a Bytes object\n",
      "3070     PyObject* out2 = PyObject_Str(out);\n",
      "3071     Py_DECREF(out);\n",
      "3072     return out2;\n",
      "3073     #endif\n",
      "3074     return out;\n",
      "3075 }\n",
      "3076 \n",
      "3077 static PyTypeObject CudaNdarrayType =\n",
      "3078 {\n",
      "3079 #if PY_MAJOR_VERSION >= 3\n",
      "3080     PyVarObject_HEAD_INIT(NULL, 0)\n",
      "3081 #else\n",
      "3082     PyObject_HEAD_INIT(NULL)\n",
      "3083     0,                         /*ob_size*/\n",
      "3084 #endif\n",
      "3085     \"CudaNdarray\",             /*tp_name*/\n",
      "3086     sizeof(CudaNdarray),       /*tp_basicsize*/\n",
      "3087     0,                         /*tp_itemsize*/\n",
      "3088     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/\n",
      "3089     0,                         /*tp_print*/\n",
      "3090     0,                         /*tp_getattr*/\n",
      "3091     0,                         /*tp_setattr*/\n",
      "3092     0,                         /*tp_compare*/\n",
      "3093     CudaNdarray_repr,          /*tp_repr*/\n",
      "3094     &CudaNdarrayNumberMethods, /*tp_as_number*/\n",
      "3095     0,                         /*tp_as_sequence*/\n",
      "3096     &CudaNdarrayMappingMethods,/*tp_as_mapping*/\n",
      "3097     0,                         /*tp_hash */\n",
      "3098     0,                         /*tp_call*/\n",
      "3099     0,                         /*tp_str*/\n",
      "3100     0,                         /*tp_getattro*/\n",
      "3101     0,                         /*tp_setattro*/\n",
      "3102     0,                         /*tp_as_buffer*/\n",
      "3103 #if PY_MAJOR_VERSION >= 3\n",
      "3104     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.\n",
      "3105     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/\n",
      "3106 #else\n",
      "3107     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/\n",
      "3108 #endif\n",
      "3109     \"CudaNdarray objects\",     /* tp_doc */\n",
      "3110     0,                         /* tp_traverse */\n",
      "3111     0,                         /* tp_clear */\n",
      "3112     0,                         /* tp_richcompare */\n",
      "3113     0,                         /* tp_weaklistoffset */\n",
      "3114     0,                         /* tp_iter */\n",
      "3115     0,                         /* tp_iternext */\n",
      "3116     CudaNdarray_methods,       /* tp_methods */\n",
      "3117     CudaNdarray_members,       /* tp_members */\n",
      "3118     CudaNdarray_getset,        /* tp_getset */\n",
      "3119     0,                         /* tp_base */\n",
      "3120     0,                         /* tp_dict */\n",
      "3121     0,                         /* tp_descr_get */\n",
      "3122     0,                         /* tp_descr_set */\n",
      "3123     0,                         /* tp_dictoffset */\n",
      "3124     (initproc)CudaNdarray_init,/* tp_init */\n",
      "3125     0,                         /* tp_alloc */\n",
      "3126     CudaNdarray_new,           /* tp_new */\n",
      "3127 };\n",
      "3128 \n",
      "3129 static __global__ void get_gpu_ptr_size(int* dst)\n",
      "3130 {\n",
      "3131     dst[0] = sizeof(float*);\n",
      "3132     dst[1] = sizeof(int);\n",
      "3133 }\n",
      "3134 \n",
      "3135 PyObject *\n",
      "3136 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)\n",
      "3137 {\n",
      "3138     int *gpu_data = (int*)device_malloc(sizeof(int)*2);\n",
      "3139     if(gpu_data == NULL){\n",
      "3140         return NULL;\n",
      "3141     }\n",
      "3142     get_gpu_ptr_size<<<1,1>>>(gpu_data);\n",
      "3143 \n",
      "3144     cudaError_t cudaErr = cudaGetLastError();\n",
      "3145     if (cudaSuccess != cudaErr){\n",
      "3146 \n",
      "3147         device_free(gpu_data);\n",
      "3148         return PyErr_Format(PyExc_RuntimeError,\n",
      "3149                             \"CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)\",\n",
      "3150                             cudaGetErrorString(cudaErr));\n",
      "3151     }\n",
      "3152 \n",
      "3153     // Transfer the result to cpu\n",
      "3154     int gpu_sizes[] = {-1,-1};\n",
      "3155     cublasStatus_t err;\n",
      "3156     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);\n",
      "3157     device_free(gpu_data);\n",
      "3158 \n",
      "3159     if (CUBLAS_STATUS_SUCCESS != err){\n",
      "3160         PyErr_SetString(PyExc_RuntimeError, \"error copying data to from memory\");\n",
      "3161         return NULL;\n",
      "3162     }\n",
      "3163     return Py_BuildValue(\"iiii\", (int) gpu_sizes[0], (int)sizeof(float*),\n",
      "3164                          (int)sizeof(int), (int) gpu_sizes[1]);\n",
      "3165 }\n",
      "3166 \n",
      "3167 static int cublas_init();\n",
      "3168 static void cublas_shutdown();\n",
      "3169 // Initialize the gpu.\n",
      "3170 // Takes two optional parameters, the device number and if we should use cnmem.\n",
      "3171 // If the device number is provided, it sets that device to be the active device.\n",
      "3172 // If not provided (usually just to test whether the gpu is available at all),\n",
      "3173 // it does not set an active device.\n",
      "3174 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.\n",
      "3175 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.\n",
      "3176 PyObject *\n",
      "3177 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)\n",
      "3178 {\n",
      "3179     int card_nb = 0;\n",
      "3180     int card_number_provided = 1;\n",
      "3181     float cnmem = 0; // Theano flag lib.cnmem\n",
      "3182     // if we're given something wildly invalid, this will throw a TypeError\n",
      "3183     if(!PyArg_ParseTuple(args, \"|if\", &card_nb, &cnmem))\n",
      "3184         return NULL;\n",
      "3185     if(cnmem)\n",
      "3186         g_use_cnmem = true;\n",
      "3187 \n",
      "3188     if(PyTuple_Size(args) == 0) {\n",
      "3189         card_number_provided = 0;\n",
      "3190         card_nb = 0;\n",
      "3191     }\n",
      "3192 \n",
      "3193     int deviceCount;\n",
      "3194     cudaError err = cudaGetDeviceCount(&deviceCount);\n",
      "3195     if(cudaSuccess != err) {\n",
      "3196         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3197                             \"Unable to get the number of gpus available: %s\",\n",
      "3198                             cudaGetErrorString(cudaGetLastError()));\n",
      "3199     }\n",
      "3200 \n",
      "3201     // as soon as the first successful call to a cuda* function is made, a\n",
      "3202     // gpu context has been created\n",
      "3203     g_gpu_context_active = 1;\n",
      "3204 \n",
      "3205     if(deviceCount <= 0) {\n",
      "3206         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3207                             \"Can't use the GPU, no devices support CUDA\");\n",
      "3208     }\n",
      "3209     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {\n",
      "3210         return PyErr_Format(PyExc_ValueError,\n",
      "3211                             \"Bad device number %d. Only %d devices available.\",\n",
      "3212                             card_nb,\n",
      "3213                             deviceCount);\n",
      "3214     }\n",
      "3215 \n",
      "3216     cudaDeviceProp deviceProp;\n",
      "3217     err = cudaGetDeviceProperties(&deviceProp, card_nb);\n",
      "3218     if(cudaSuccess != err) {\n",
      "3219         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3220                             \"Unable to get properties of gpu %i: %s\",\n",
      "3221                             card_nb,\n",
      "3222                             cudaGetErrorString(cudaGetLastError()));\n",
      "3223     }\n",
      "3224 \n",
      "3225     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){\n",
      "3226         return PyErr_Format(PyExc_EnvironmentError,\n",
      "3227                             \"There is no device that supports CUDA\");\n",
      "3228     }\n",
      "3229 \n",
      "3230     if(card_number_provided) {\n",
      "3231         err = cudaSetDevice(card_nb);\n",
      "3232         if(cudaSuccess != err) {\n",
      "3233             return PyErr_Format(PyExc_EnvironmentError,\n",
      "3234                                 \"Unable to set device %i: %s\",\n",
      "3235                                 card_nb,\n",
      "3236                                 cudaGetErrorString(cudaGetLastError()));\n",
      "3237         }\n",
      "3238         if (cublas_init() == -1)\n",
      "3239             return NULL;\n",
      "3240     }\n",
      "3241     if(card_number_provided && g_use_cnmem) {\n",
      "3242         size_t mem = 0;\n",
      "3243         if (cnmem > 1)\n",
      "3244             mem = cnmem * 1024 * 1024;\n",
      "3245         else{\n",
      "3246             // Clip to 95% to let memory for the driver.\n",
      "3247             // 98% didn't worked in some cases.\n",
      "3248             if (cnmem > .95){\n",
      "3249                 cnmem = .95;\n",
      "3250             }\n",
      "3251             size_t free = 0, total = 0;\n",
      "3252             cudaError_t err = cudaMemGetInfo(&free, &total);\n",
      "3253             if (err != cudaSuccess){\n",
      "3254                 // Clear the error flag, cudaMemGetInfo doesn't do it.\n",
      "3255                 // Currently this returns the same thing as err, but if in future\n",
      "3256                 // it returns something else I still don't see why we should ignore\n",
      "3257                 // it.  All we want to do here is reset the flag.\n",
      "3258                 cudaGetLastError();\n",
      "3259                 PyErr_Format(PyExc_RuntimeError,\n",
      "3260                              \"Error while getting memory info about the gpu: %s\",\n",
      "3261                              cudaGetErrorString(err));\n",
      "3262                 return NULL;\n",
      "3263             }\n",
      "3264             mem = total * cnmem;\n",
      "3265         }\n",
      "3266         if(initCnmem(card_number_provided, card_nb, mem) == -1){\n",
      "3267             return NULL;\n",
      "3268         }\n",
      "3269     }\n",
      "3270 \n",
      "3271     Py_INCREF(Py_None);\n",
      "3272     return Py_None;\n",
      "3273 }\n",
      "3274 \n",
      "3275 PyObject *\n",
      "3276 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {\n",
      "3277     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3278     // really necessary.\n",
      "3279     int currentDevice;\n",
      "3280     cudaGetDevice(&currentDevice);\n",
      "3281     return PyInt_FromLong(currentDevice);\n",
      "3282 }\n",
      "3283 \n",
      "3284 PyObject *\n",
      "3285 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {\n",
      "3286     // NB: No cuda error checking here; keeps things simple, and it's not\n",
      "3287     // really necessary.\n",
      "3288     int currentDevice;\n",
      "3289     cudaGetDevice(&currentDevice);\n",
      "3290 \n",
      "3291     cudaDeviceProp deviceProp;\n",
      "3292     cudaGetDeviceProperties(&deviceProp, currentDevice);\n",
      "3293     return PyString_FromString(deviceProp.name);\n",
      "3294 }\n",
      "3295 \n",
      "3296 PyObject *\n",
      "3297 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {\n",
      "3298     // Don't handle errors here\n",
      "3299     cublas_shutdown();\n",
      "3300     g_gpu_context_active = 0; // context has now been closed down\n",
      "3301     if(g_use_cnmem) {\n",
      "3302         cnmemStatus_t status = cnmemFinalize();\n",
      "3303         if(status != CNMEM_STATUS_SUCCESS) {\n",
      "3304             fprintf(stderr, \"CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\\n\",\n",
      "3305                     cnmemGetErrorString(status));\n",
      "3306             if(status == CNMEM_STATUS_CUDA_ERROR) {\n",
      "3307                 fprintf(stderr, \"  Cuda-Reason=%s\\n\",\n",
      "3308                         cudaGetErrorString(cudaGetLastError()));\n",
      "3309             }\n",
      "3310         }\n",
      "3311     }\n",
      "3312 \n",
      "3313     Py_INCREF(Py_None);\n",
      "3314     return Py_None;\n",
      "3315 }\n",
      "3316 \n",
      "3317 /*\n",
      "3318  * This function is tested in theano/misc/test_pycuda_theano_simple.py\n",
      "3319  */\n",
      "3320 PyObject *\n",
      "3321 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)\n",
      "3322 {\n",
      "3323     int verbose = 0;\n",
      "3324     PyObject *gpu_ptr = NULL;\n",
      "3325     PyObject *shapes = NULL;\n",
      "3326     PyObject *strides = NULL;\n",
      "3327     PyObject *base = NULL;\n",
      "3328     PyObject *rval = NULL;\n",
      "3329 \n",
      "3330     //args should consist of 3 python objects\n",
      "3331     //The first is the gpu ptr\n",
      "3332     //The second if the shape\n",
      "3333     //The third if the strides\n",
      "3334     if (! PyArg_ParseTuple(args, \"OOOO\", &gpu_ptr, &shapes, &strides, &base))\n",
      "3335         return NULL;\n",
      "3336 \n",
      "3337     if (verbose) printf(\"In CudaNdarray_from_gpu_pointer\\n\");\n",
      "3338     if (!PyLong_Check(gpu_ptr))\n",
      "3339     {\n",
      "3340         PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: The gpu pointor is not an long\");\n",
      "3341         return NULL;\n",
      "3342     }\n",
      "3343 \n",
      "3344     Py_ssize_t nd =  PyObject_Length(shapes);\n",
      "3345     if (nd < 0)\n",
      "3346     {\n",
      "3347         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of second argument\");\n",
      "3348         return NULL;\n",
      "3349     }\n",
      "3350     Py_ssize_t nd_stride =  PyObject_Length(strides);\n",
      "3351     if (nd_stride < 0)\n",
      "3352     {\n",
      "3353         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Couldn't get length of third argument\");\n",
      "3354         return NULL;\n",
      "3355     }\n",
      "3356 \n",
      "3357     if (nd != nd_stride)\n",
      "3358     {\n",
      "3359         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides\");\n",
      "3360         return NULL;\n",
      "3361     }\n",
      "3362 \n",
      "3363     rval = CudaNdarray_New();\n",
      "3364 \n",
      "3365     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))\n",
      "3366     {\n",
      "3367         //CudaNdarray_set_nd set the error msg\n",
      "3368         return NULL;\n",
      "3369     }\n",
      "3370     // set gpu pointeur\n",
      "3371     assert(((CudaNdarray *)rval)->data_allocated == 0);\n",
      "3372     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))\n",
      "3373     {\n",
      "3374         PyErr_SetString(PyExc_TypeError, \"CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor\");\n",
      "3375         return NULL;\n",
      "3376 \n",
      "3377     }\n",
      "3378 \n",
      "3379     // Set dims and strides\n",
      "3380     for (int i = nd-1; i >= 0; --i)\n",
      "3381     {\n",
      "3382         PyObject * idx = PyLong_FromLong(i);\n",
      "3383         if (idx == NULL)\n",
      "3384         {\n",
      "3385             PyErr_SetString(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple\");\n",
      "3386             return NULL;\n",
      "3387         }\n",
      "3388         PyObject* dim_ = PyObject_GetItem(shapes, idx);\n",
      "3389         PyObject* strd_ = PyObject_GetItem(strides, idx);\n",
      "3390         if (!PyInt_Check(dim_))\n",
      "3391         {\n",
      "3392             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: shapes[%d] is not an int\", i);\n",
      "3393             return NULL;\n",
      "3394         }\n",
      "3395         if (!PyInt_Check(strd_))\n",
      "3396         {\n",
      "3397             PyErr_Format(PyExc_Exception, \"CudaNdarray_from_gpu_pointer: strides[%d] is not an int\", i);\n",
      "3398             return NULL;\n",
      "3399         }\n",
      "3400         int dim = PyInt_AsLong(dim_);\n",
      "3401         int strd = PyInt_AsLong(strd_);\n",
      "3402         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);\n",
      "3403         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);\n",
      "3404         Py_DECREF(idx);\n",
      "3405         Py_DECREF(dim_);\n",
      "3406         Py_DECREF(strd_);\n",
      "3407     }\n",
      "3408     if (verbose) printf(\"CudaNdarray_from_gpu_pointer normal return\\n\");\n",
      "3409     return rval;\n",
      "3410 }\n",
      "3411 \n",
      "3412 PyObject *\n",
      "3413 CudaNdarray_Dot(PyObject* _unused, PyObject* args)\n",
      "3414 {\n",
      "3415     PyObject *l=NULL;\n",
      "3416     PyObject *r=NULL;\n",
      "3417     PyObject * rval = NULL;\n",
      "3418 \n",
      "3419     //args should consist of two python objects (\"OO\")\n",
      "3420     if (! PyArg_ParseTuple(args, \"OO\", &l, &r))\n",
      "3421         return NULL;\n",
      "3422 \n",
      "3423     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))\n",
      "3424     {\n",
      "3425         PyErr_SetString(PyExc_TypeError, \"CudaNdarray arguments required \");\n",
      "3426         goto CudaNdarray_dot_fail;\n",
      "3427     }\n",
      "3428     if (((CudaNdarray*)l)->nd != 2)\n",
      "3429     {\n",
      "3430         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3431         goto CudaNdarray_dot_fail;\n",
      "3432     }\n",
      "3433     if (((CudaNdarray*)r)->nd != 2)\n",
      "3434     {\n",
      "3435         PyErr_SetString(PyExc_TypeError, \"need 2d CudaNdarray arg for now\");\n",
      "3436         goto CudaNdarray_dot_fail;\n",
      "3437     }\n",
      "3438     rval = CudaNdarray_New();\n",
      "3439     if (!rval)\n",
      "3440     {\n",
      "3441         goto CudaNdarray_dot_fail;\n",
      "3442     }\n",
      "3443     int dims[2];\n",
      "3444     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];\n",
      "3445     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];\n",
      "3446     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))\n",
      "3447     {\n",
      "3448         goto CudaNdarray_dot_fail;\n",
      "3449     }\n",
      "3450     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))\n",
      "3451     {\n",
      "3452         goto CudaNdarray_dot_fail;\n",
      "3453     }\n",
      "3454 \n",
      "3455     return rval;\n",
      "3456 \n",
      "3457     CudaNdarray_dot_fail:\n",
      "3458     Py_XDECREF(rval);\n",
      "3459     return NULL;\n",
      "3460 }\n",
      "3461 \n",
      "3462 static PyObject *\n",
      "3463 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)\n",
      "3464 {\n",
      "3465     /*\n",
      "3466      * TODO: DOC what this function should do in the various cases of\n",
      "3467      * What is 'strict' supposed to mean in the context of this function?\n",
      "3468      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?\n",
      "3469      *\n",
      "3470      */\n",
      "3471     PyObject *py_data=NULL;\n",
      "3472     PyArrayObject * data = NULL;\n",
      "3473     int strict = 0;\n",
      "3474     PyObject * broadcastable=NULL;\n",
      "3475     PyObject * storage=NULL;\n",
      "3476     CudaNdarray * rval=NULL;\n",
      "3477 \n",
      "3478     //Python object references which are provided to the caller are borrowed references\n",
      "3479     if (!PyArg_ParseTuple(args, \"OOiO\", &py_data, &broadcastable, &strict, &storage)) return NULL;\n",
      "3480 \n",
      "3481     if (!PyTuple_Check(broadcastable)){\n",
      "3482         PyErr_SetString(PyExc_TypeError, \"broadcastable arg should be a tuple of int.\");\n",
      "3483         return NULL;\n",
      "3484     }\n",
      "3485     Py_INCREF(py_data);\n",
      "3486     Py_INCREF(broadcastable);\n",
      "3487 \n",
      "3488     CudaNdarray * cnda = (CudaNdarray*)py_data;\n",
      "3489 \n",
      "3490     if (strict || CudaNdarray_Check(py_data))\n",
      "3491     {\n",
      "3492         //TODO: support non-strict \"casting\" from a vt to the broadcastable/type/size that we need.\n",
      "3493         if (!CudaNdarray_Check(py_data))\n",
      "3494         {\n",
      "3495             Py_DECREF(py_data);\n",
      "3496             Py_DECREF(broadcastable);\n",
      "3497             PyErr_SetString(PyExc_TypeError, \"strict mode requires CudaNdarray\");\n",
      "3498             return NULL;\n",
      "3499         }\n",
      "3500         if (cnda->nd != PyTuple_Size(broadcastable))\n",
      "3501         {\n",
      "3502             Py_DECREF(py_data);\n",
      "3503             Py_DECREF(broadcastable);\n",
      "3504             PyErr_Format(PyExc_TypeError, \"Wrong rank: %i vs %li\", cnda->nd, (long)PyTuple_Size(broadcastable));\n",
      "3505             return NULL;\n",
      "3506         }\n",
      "3507         for (int i = 0; i < cnda->nd; ++i)\n",
      "3508         {\n",
      "3509             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3510             {\n",
      "3511                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable vt dimension %i\", i);\n",
      "3512                 Py_DECREF(py_data);\n",
      "3513                 Py_DECREF(broadcastable);\n",
      "3514                 return NULL;\n",
      "3515             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){\n",
      "3516                 PyErr_Format(PyExc_TypeError, \"Non-zeros strides(%d) on dimension %d of size 1\",\n",
      "3517                              CudaNdarray_HOST_STRIDES(cnda)[i], i);\n",
      "3518                 Py_DECREF(py_data);\n",
      "3519                 Py_DECREF(broadcastable);\n",
      "3520                 return NULL;\n",
      "3521             }\n",
      "3522         }\n",
      "3523         Py_DECREF(broadcastable);\n",
      "3524         return py_data;\n",
      "3525     }\n",
      "3526     else\n",
      "3527     {\n",
      "3528         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));\n",
      "3529         if (!data)\n",
      "3530         {\n",
      "3531             //err message already defined\n",
      "3532             Py_DECREF(py_data);\n",
      "3533             Py_DECREF(broadcastable);\n",
      "3534             return NULL;\n",
      "3535         }\n",
      "3536         for (int i = 0; i < PyArray_NDIM(data); ++i)\n",
      "3537         {\n",
      "3538             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))\n",
      "3539             {\n",
      "3540                 PyErr_Format(PyExc_TypeError, \"Non-unit size in broadcastable dimension %i\", i);\n",
      "3541                 Py_DECREF(data);\n",
      "3542                 Py_DECREF(py_data);\n",
      "3543                 Py_DECREF(broadcastable);\n",
      "3544                 return NULL;\n",
      "3545             }\n",
      "3546         }\n",
      "3547         if (storage && CudaNdarray_Check(storage))\n",
      "3548         {\n",
      "3549             rval = (CudaNdarray*) storage;\n",
      "3550             Py_INCREF(rval);\n",
      "3551         }\n",
      "3552         else\n",
      "3553         {\n",
      "3554             rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3555         }\n",
      "3556         if (rval)\n",
      "3557         {\n",
      "3558             if (CudaNdarray_CopyFromArray(rval, data))\n",
      "3559             {\n",
      "3560                 Py_DECREF(rval);\n",
      "3561                 rval = NULL;\n",
      "3562             }\n",
      "3563         }\n",
      "3564         Py_DECREF(data);\n",
      "3565         Py_DECREF(py_data);\n",
      "3566         Py_DECREF(broadcastable);\n",
      "3567         return (PyObject*)rval;\n",
      "3568     }\n",
      "3569 }\n",
      "3570 \n",
      "3571 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.\n",
      "3572 //       Pick one and standardize it, this file is already annoying enough to grep through\n",
      "3573 static PyMethodDef module_methods[] = {\n",
      "3574     {\"dimshuffle\", CudaNdarray_Dimshuffle, METH_VARARGS, \"Returns the dimshuffle of a CudaNdarray.\"},\n",
      "3575     {\"dot\", CudaNdarray_Dot, METH_VARARGS, \"Returns the matrix product of two CudaNdarray arguments.\"},\n",
      "3576     {\"gpu_init\", CudaNdarray_gpu_init, METH_VARARGS, \"Select the gpu card to use; also usable to test whether CUDA is available.\"},\n",
      "3577     {\"select_a_gpu\", CudaNdarray_select_a_gpu, METH_NOARGS, \"Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU.\"},\n",
      "3578     {\"active_device_name\", CudaNdarray_active_device_name, METH_VARARGS, \"Get the name of the active device.\"},\n",
      "3579     {\"active_device_number\", CudaNdarray_active_device_number, METH_VARARGS, \"Get the number of the active device.\"},\n",
      "3580     {\"gpu_shutdown\", CudaNdarray_gpu_shutdown, METH_VARARGS, \"Shut down the gpu.\"},\n",
      "3581     {\"device_properties\", GetDeviceProperties, METH_VARARGS, \"Return a dictionary with the device properties.\"},\n",
      "3582     {\"mem_info\", GetDeviceMemInfo, METH_NOARGS, \"Return a tuple with the free and total memory on the gpu in bytes.\"},\n",
      "3583 #if COMPUTE_GPU_MEM_USED\n",
      "3584     {\"theano_allocated\", GetTheanoAllocInfo, METH_NOARGS, \"Return the size in bytes of memory Theano currently have allocated on the gpu.\"},\n",
      "3585 #endif\n",
      "3586     {\"ptr_int_size\", CudaNdarray_ptr_int_size, METH_VARARGS, \"Return a tuple with the size of gpu pointer, cpu pointer and int in bytes.\"},\n",
      "3587     {\"filter\", filter, METH_VARARGS, \"filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadcastable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value.\"},\n",
      "3588     {\"outstanding_mallocs\", outstanding_mallocs, METH_VARARGS, \"how many more mallocs have been called than free's\"},\n",
      "3589     {\"from_gpu_pointer\", CudaNdarray_from_gpu_pointer, METH_VARARGS, \"Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)\"},\n",
      "3590     {\"synchronize\", CudaNdarray_synchronize, METH_NOARGS, \"Used to synchronize the device\"},\n",
      "3591     {\"cublas_v2\", CudaNdarray_cublasv2, METH_NOARGS,\n",
      "3592      \"Used to know if this version of cuda_ndarray is linked with cublas v2.\"},\n",
      "3593     {NULL, NULL, NULL, NULL}  /* Sentinel */\n",
      "3594 };\n",
      "3595 \n",
      "3596 #define CNDA_MOD_NAME \"cuda_ndarray\"\n",
      "3597 #define CNDA_DOCSTRING \"CUDA implementation of a numpy ndarray-like object.\"\n",
      "3598 \n",
      "3599 #if PY_MAJOR_VERSION == 3\n",
      "3600 static struct PyModuleDef cuda_ndarray_moduledef =\n",
      "3601 {\n",
      "3602     PyModuleDef_HEAD_INIT,\n",
      "3603     CNDA_MOD_NAME,\n",
      "3604     CNDA_DOCSTRING,\n",
      "3605     -1,     /* size of per-interpreter state of the module,\n",
      "3606                or -1 if the module keeps state in global variables. */\n",
      "3607     module_methods\n",
      "3608 };\n",
      "3609 \n",
      "3610 PyMODINIT_FUNC\n",
      "3611 PyInit_cuda_ndarray(void)\n",
      "3612 #else\n",
      "3613 PyMODINIT_FUNC\n",
      "3614 initcuda_ndarray(void)\n",
      "3615 #endif\n",
      "3616 {\n",
      "3617     import_array();\n",
      "3618 \n",
      "3619     PyObject* m;\n",
      "3620 \n",
      "3621     if (PyType_Ready(&CudaNdarrayType) < 0) {\n",
      "3622 #if PY_MAJOR_VERSION == 3\n",
      "3623         return NULL;\n",
      "3624 #else\n",
      "3625         return;\n",
      "3626 #endif\n",
      "3627     }\n",
      "3628 \n",
      "3629 #if PY_MAJOR_VERSION == 3\n",
      "3630     m = PyModule_Create(&cuda_ndarray_moduledef);\n",
      "3631 #else\n",
      "3632     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);\n",
      "3633 #endif\n",
      "3634 \n",
      "3635     if (m == NULL) {\n",
      "3636 #if PY_MAJOR_VERSION == 3\n",
      "3637         return NULL;\n",
      "3638 #else\n",
      "3639         return;\n",
      "3640 #endif\n",
      "3641     }\n",
      "3642 \n",
      "3643     Py_INCREF(&CudaNdarrayType);\n",
      "3644     PyModule_AddObject(m, \"CudaNdarray\", (PyObject *)&CudaNdarrayType);\n",
      "3645 #if COMPUTE_GPU_MEM_USED\n",
      "3646     for(int i=0;i<TABLE_SIZE;i++){\n",
      "3647         _alloc_size_table[i].ptr=NULL;\n",
      "3648         _alloc_size_table[i].size=0;\n",
      "3649     }\n",
      "3650 #endif\n",
      "3651     //    cublasInit();\n",
      "3652     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())\n",
      "3653     //{\n",
      "3654         //std::cerr << \"WARNING: initcuda_ndarray: error initializing device\\n\";\n",
      "3655     //}\n",
      "3656     if (0) //TODO: is this necessary?\n",
      "3657     {\n",
      "3658         int deviceId = 0; // TODO: what number goes here?\n",
      "3659         cudaSetDevice(deviceId);\n",
      "3660         cudaError_t err = cudaGetLastError();\n",
      "3661         if( cudaSuccess != err)\n",
      "3662         {\n",
      "3663             std::cerr << \"Error in SetDevice:\" << cudaGetErrorString(err) << \"\\n\";\n",
      "3664         }\n",
      "3665     }\n",
      "3666 \n",
      "3667 #if PY_MAJOR_VERSION == 3\n",
      "3668     return m;\n",
      "3669 #endif\n",
      "3670 }\n",
      "3671 \n",
      "3672 \n",
      "3673 //////////////////////////////////////\n",
      "3674 //\n",
      "3675 // C API FOR CudaNdarray\n",
      "3676 //\n",
      "3677 //////////////////////////////////////\n",
      "3678 \n",
      "3679 int\n",
      "3680 CudaNdarray_Check(const PyObject * ob)\n",
      "3681 {\n",
      "3682     //TODO: doesn't work with inheritance\n",
      "3683     return CudaNdarray_CheckExact(ob);\n",
      "3684 }\n",
      "3685 int\n",
      "3686 CudaNdarray_CheckExact(const PyObject * ob)\n",
      "3687 {\n",
      "3688     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);\n",
      "3689 }\n",
      "3690 \n",
      "3691 PyObject *\n",
      "3692 CudaNdarray_New(int nd)\n",
      "3693 {\n",
      "3694     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);\n",
      "3695     if (self == NULL)\n",
      "3696     {\n",
      "3697         PyErr_SetString(PyExc_RuntimeError, \"CudaNdarray_New failed to allocate self\");\n",
      "3698         return NULL;\n",
      "3699     }\n",
      "3700     CudaNdarray_null_init(self);\n",
      "3701 \n",
      "3702     if (nd == 0)\n",
      "3703     {\n",
      "3704         self->nd = 0;\n",
      "3705     }\n",
      "3706     else if (nd > 0)\n",
      "3707     {\n",
      "3708         if (CudaNdarray_set_nd(self, nd))\n",
      "3709         {\n",
      "3710             Py_DECREF(self);\n",
      "3711             return NULL;\n",
      "3712         }\n",
      "3713     }\n",
      "3714     ++_outstanding_mallocs[1];\n",
      "3715     return (PyObject *)self;\n",
      "3716 }\n",
      "3717 \n",
      "3718 \n",
      "3719 \n",
      "3720 //////////////////////////////\n",
      "3721 //\n",
      "3722 // Published helper functions\n",
      "3723 //\n",
      "3724 //////////////////////////////\n",
      "3725 \n",
      "3726 static int\n",
      "3727 cublas_init()\n",
      "3728 {\n",
      "3729     cublasStatus_t err;\n",
      "3730     err = cublasCreate(&handle);\n",
      "3731     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3732     {\n",
      "3733         if(CUBLAS_STATUS_NOT_INITIALIZED == err)\n",
      "3734             PyErr_SetString(PyExc_RuntimeError,\n",
      "3735                             \"cublasCreate() returned this error \"\n",
      "3736                             \"'the CUDA Runtime initialization failed'\");\n",
      "3737         else if(CUBLAS_STATUS_ALLOC_FAILED == err)\n",
      "3738             PyErr_SetString(PyExc_RuntimeError,\n",
      "3739                             \"cublasCreate() returned this error \"\n",
      "3740                             \"'the resources could not be allocated'\");\n",
      "3741         else\n",
      "3742             PyErr_SetString(PyExc_RuntimeError,\n",
      "3743                             \"unknow error during returned by cublasCreate()\");\n",
      "3744         return -1;\n",
      "3745     }\n",
      "3746     // Set the default stream as the one to execute on (default)\n",
      "3747     cublasSetStream(handle, NULL);\n",
      "3748     // Pointer to scalars are on the host (also default)\n",
      "3749     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);\n",
      "3750 #if CUDA_VERSION >= 5000\n",
      "3751     // atomics can be used in kernels to speed up operations (not default)\n",
      "3752     // This may lead to a slight variance from run to run in some operations\n",
      "3753     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);\n",
      "3754 #endif\n",
      "3755     return 0;\n",
      "3756 }\n",
      "3757 \n",
      "3758 static void\n",
      "3759 cublas_shutdown()\n",
      "3760 {\n",
      "3761     if (handle != NULL)\n",
      "3762         cublasDestroy(handle);\n",
      "3763     // No point in handling any errors here\n",
      "3764     handle = NULL;\n",
      "3765 }\n",
      "3766 \n",
      "3767 int\n",
      "3768 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)\n",
      "3769 {\n",
      "3770     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),\n",
      "3771                                            PyArray_DIMS(obj));\n",
      "3772     if (err) {\n",
      "3773         return err;\n",
      "3774     }\n",
      "3775 \n",
      "3776     int typenum = PyArray_TYPE(obj);\n",
      "3777     if (typenum != REAL_TYPENUM)\n",
      "3778     {\n",
      "3779         PyErr_SetString(PyExc_TypeError, \"can only copy from float arrays\");\n",
      "3780         return -1;\n",
      "3781     }\n",
      "3782     assert( 4 ==  PyArray_ITEMSIZE(obj));\n",
      "3783     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(\n",
      "3784         (PyObject*)obj, typenum, self->nd, self->nd);\n",
      "3785     if (!py_src) {\n",
      "3786         return -1;\n",
      "3787     }\n",
      "3788     npy_intp py_src_size = PyArray_SIZE(py_src);\n",
      "3789     void *py_src_data = PyArray_DATA(py_src);\n",
      "3790     cudaError_t cerr;\n",
      "3791     CNDA_BEGIN_ALLOW_THREADS;\n",
      "3792     cerr = cudaMemcpy(self->devdata, py_src_data,\n",
      "3793                       py_src_size * sizeof(real),\n",
      "3794                       cudaMemcpyHostToDevice);\n",
      "3795     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway\n",
      "3796     CNDA_END_ALLOW_THREADS;\n",
      "3797     if (cudaSuccess != cerr)\n",
      "3798     {\n",
      "3799         PyErr_Format(PyExc_RuntimeError,\n",
      "3800                      \"Cuda error '%s' while copying %lli data element\"\n",
      "3801                      \" to device memory. str ptr=%p. dst ptr=%p\",\n",
      "3802                      cudaGetErrorString(cerr),\n",
      "3803                      (long long)py_src_size,\n",
      "3804                      py_src_data,\n",
      "3805                      self->devdata);\n",
      "3806         Py_DECREF(py_src);\n",
      "3807         return -1;\n",
      "3808     }\n",
      "3809     Py_DECREF(py_src);\n",
      "3810     return 0;\n",
      "3811 }\n",
      "3812 \n",
      "3813 PyObject *\n",
      "3814 CudaNdarray_new_nd(int nd)\n",
      "3815 {\n",
      "3816     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();\n",
      "3817     if (!rval || CudaNdarray_set_nd(rval, nd))\n",
      "3818     {\n",
      "3819         Py_XDECREF(rval);\n",
      "3820         rval = NULL;\n",
      "3821     }\n",
      "3822     return (PyObject *) rval;\n",
      "3823 }\n",
      "3824 \n",
      "3825 \n",
      "3826 /**\n",
      "3827  * Initialize 'self' as a view of 'base', with memory storage 'data'\n",
      "3828  */\n",
      "3829 \n",
      "3830 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)\n",
      "3831 {\n",
      "3832     if (self->data_allocated)\n",
      "3833     {\n",
      "3834         assert(self->devdata);\n",
      "3835         if (device_free(self->devdata))\n",
      "3836         {\n",
      "3837             self->devdata = NULL;\n",
      "3838             self->data_allocated = 0;\n",
      "3839             return -1;\n",
      "3840         }\n",
      "3841     }\n",
      "3842     // Get the original base object (base.base.base...)\n",
      "3843     PyObject * orig_base = base;\n",
      "3844     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...\n",
      "3845     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)\n",
      "3846     {\n",
      "3847         // base_base is itself a view\n",
      "3848         orig_base = ((CudaNdarray*) orig_base)->base;\n",
      "3849     }\n",
      "3850     //N.B. XDECREF and XINCREF are no-ops for NULL pointers\n",
      "3851     if (self->base != orig_base)\n",
      "3852     {\n",
      "3853         Py_XDECREF(self->base);\n",
      "3854         self->base = orig_base;\n",
      "3855         Py_XINCREF(self->base);\n",
      "3856     }\n",
      "3857     self->data_allocated = 0;\n",
      "3858     self->devdata = data;\n",
      "3859     return 0;\n",
      "3860 }\n",
      "3861 \n",
      "3862 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)\n",
      "3863 {\n",
      "3864     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)\n",
      "3865     {\n",
      "3866         y[i*sy] = x[i*sx];\n",
      "3867     }\n",
      "3868 }\n",
      "3869 \n",
      "3870 // N1 through N4 are the size of y\n",
      "3871 static __global__ void k_copy_4d(const int N1,\n",
      "3872         const int N2, const int N3, const int N4,\n",
      "3873         const float * x, const int sx1, const int sx2, const int sx3,\n",
      "3874         const int sx4,  float * y, const int sy1, const int sy2,\n",
      "3875         const int sy3, const int sy4)\n",
      "3876 {\n",
      "3877     // These must be made int instead of unsigned int due to a bug in nvcc\n",
      "3878     int bx = blockIdx.x;\n",
      "3879     int by = blockIdx.y;\n",
      "3880 \n",
      "3881     for (int i = bx; i < N1; i += gridDim.x)\n",
      "3882     {\n",
      "3883         for (int j = by; j < N2; j += gridDim.y)\n",
      "3884         {\n",
      "3885             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)\n",
      "3886             {\n",
      "3887                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)\n",
      "3888                 {\n",
      "3889                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =\n",
      "3890                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];\n",
      "3891                 }\n",
      "3892             }\n",
      "3893         }\n",
      "3894     }\n",
      "3895 }\n",
      "3896 \n",
      "3897 //copy from other into self\n",
      "3898 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,\n",
      "3899                                     const CudaNdarray * other,\n",
      "3900                                     bool unbroadcast)\n",
      "3901 {\n",
      "3902     int verbose = 0;\n",
      "3903     if (verbose>1) fprintf(stderr, \"CudaNdarray_CopyFromCudaNdarray\\n\");\n",
      "3904 \n",
      "3905     //standard elemwise size checks\n",
      "3906     if (self->nd == -1)\n",
      "3907     {\n",
      "3908         PyErr_SetString(PyExc_TypeError,\n",
      "3909                         \"can't copy into un-initialized CudaNdarray\");\n",
      "3910         return -1;\n",
      "3911     }\n",
      "3912     CudaNdarray * new_other = NULL;\n",
      "3913 \n",
      "3914     if (self->nd < other->nd)\n",
      "3915     {\n",
      "3916         PyErr_Format(PyExc_NotImplementedError,\n",
      "3917             \"CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the \"\n",
      "3918             \"destination needs to be >= the number of dimensions of the \"\n",
      "3919             \"source. Got %d and %d.\", self->nd, other->nd);\n",
      "3920         return -1;\n",
      "3921     }\n",
      "3922     else if (self->nd != other->nd)\n",
      "3923     {\n",
      "3924         new_other = (CudaNdarray *) CudaNdarray_View(other);\n",
      "3925         int added_dims = self->nd - other->nd;\n",
      "3926         int* pattern = (int*) alloca(self->nd * sizeof(int));\n",
      "3927         for(int i = 0; i < added_dims; i++)\n",
      "3928             pattern[i] = -1;\n",
      "3929         for(int i = 0; i < other->nd; i++)\n",
      "3930             pattern[i + added_dims] = i;\n",
      "3931         CudaNdarray_dimshuffle(new_other, self->nd, pattern);\n",
      "3932         other = new_other;\n",
      "3933     }\n",
      "3934     assert(self->nd == other->nd);\n",
      "3935     //standard elemwise dim checks (also compute total size)\n",
      "3936     unsigned int size = 1;\n",
      "3937     unsigned int size_source = 1;\n",
      "3938     for (int i = 0; i< self->nd; ++i)\n",
      "3939     {\n",
      "3940         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])\n",
      "3941             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )\n",
      "3942         {\n",
      "3943           PyErr_Format(PyExc_ValueError,\n",
      "3944                        \"CudaNdarray_CopyFromCudaNdarray:\"\n",
      "3945                        \" need same dimensions for dim %d,\"\n",
      "3946                        \" destination=%d, source=%d\",\n",
      "3947                        i, CudaNdarray_HOST_DIMS(self)[i],\n",
      "3948                        CudaNdarray_HOST_DIMS(other)[i]);\n",
      "3949           Py_XDECREF(new_other);\n",
      "3950           return -1;\n",
      "3951         }\n",
      "3952         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];\n",
      "3953         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];\n",
      "3954     }\n",
      "3955     if (0 == size)\n",
      "3956     {\n",
      "3957         Py_XDECREF(new_other);\n",
      "3958         return 0; //nothing to copy, we're done.\n",
      "3959     }\n",
      "3960     if (CudaNdarray_is_c_contiguous(self) &&\n",
      "3961         CudaNdarray_is_c_contiguous(other) &&\n",
      "3962         size == size_source)\n",
      "3963     {\n",
      "3964         if (verbose)\n",
      "3965             fprintf(stderr, \"Copying contiguous vector with cublasScopy\\n\");\n",
      "3966 \n",
      "3967         cublasStatus_t err;\n",
      "3968         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,\n",
      "3969                           CudaNdarray_DEV_DATA(self), 1);\n",
      "3970         CNDA_THREAD_SYNC;\n",
      "3971         Py_XDECREF(new_other);\n",
      "3972         if (CUBLAS_STATUS_SUCCESS != err)\n",
      "3973         {\n",
      "3974             PyErr_SetString(PyExc_RuntimeError, \"Error copying memory\");\n",
      "3975             return -1;\n",
      "3976         }\n",
      "3977         return 0;\n",
      "3978     }\n",
      "3979     //TODO: rewrite these copy operations to be more efficient\n",
      "3980     //      See, for example the transpose example in the cuda_sdk.\n",
      "3981     switch (self->nd)\n",
      "3982     {\n",
      "3983         case 0: // scalar\n",
      "3984             {\n",
      "3985                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS\n",
      "3986                 assert(0);\n",
      "3987             }; break;\n",
      "3988         case 1: // vector\n",
      "3989             {\n",
      "3990                 if (verbose) fprintf(stderr, \"Copying non-contiguous vector\\n\");\n",
      "3991                 if (verbose) fprint_CudaNdarray(stderr, other);\n",
      "3992                 unsigned int n_blocks = std::min(size,\n",
      "3993                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "3994                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),\n",
      "3995                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "3996                 k_copy_1d<<<n_blocks, n_threads>>>(size,\n",
      "3997                                             CudaNdarray_DEV_DATA(other),\n",
      "3998                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "3999                                             CudaNdarray_DEV_DATA(self),\n",
      "4000                                             CudaNdarray_HOST_STRIDES(self)[0]);\n",
      "4001                 CNDA_THREAD_SYNC;\n",
      "4002                 cudaError_t err = cudaGetLastError();\n",
      "4003                 if( cudaSuccess != err)\n",
      "4004                 {\n",
      "4005                     PyErr_Format(PyExc_RuntimeError,\n",
      "4006                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4007                                  \" n_threads_per_block=%i)\\n\", \"k_copy_1d\",\n",
      "4008                                  cudaGetErrorString(err), n_blocks, n_threads);\n",
      "4009                     Py_XDECREF(new_other);\n",
      "4010                     return -1;\n",
      "4011                 }\n",
      "4012             }; break;\n",
      "4013         case 4: // 4-tensor\n",
      "4014             {\n",
      "4015                 if (verbose)\n",
      "4016                 {\n",
      "4017                     if (0 != fprint_CudaNdarray(stderr, other))\n",
      "4018                     {\n",
      "4019                         Py_XDECREF(new_other);\n",
      "4020                         return -1;\n",
      "4021                     }\n",
      "4022                 }\n",
      "4023 \n",
      "4024                 // The blocks implement the looping over the first two axes so\n",
      "4025                 // this needs to be (N1, N2)\n",
      "4026                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],\n",
      "4027                                         NUM_VECTOR_OP_BLOCKS),\n",
      "4028                                std::min(CudaNdarray_HOST_DIMS(self)[1],\n",
      "4029                                         NUM_VECTOR_OP_BLOCKS));\n",
      "4030                 // For the threads, just make as many as possible\n",
      "4031                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],\n",
      "4032                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),\n",
      "4033                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],\n",
      "4034                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));\n",
      "4035 \n",
      "4036                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);\n",
      "4037                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);\n",
      "4038 \n",
      "4039                 k_copy_4d<<<n_blocks, n_threads>>>(\n",
      "4040                                             // size of y\n",
      "4041                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1\n",
      "4042                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2\n",
      "4043                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3\n",
      "4044                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4\n",
      "4045                                             CudaNdarray_DEV_DATA(other), // x\n",
      "4046                                             // x strides\n",
      "4047                                             CudaNdarray_HOST_STRIDES(other)[0],\n",
      "4048                                             CudaNdarray_HOST_STRIDES(other)[1],\n",
      "4049                                             CudaNdarray_HOST_STRIDES(other)[2],\n",
      "4050                                             CudaNdarray_HOST_STRIDES(other)[3],\n",
      "4051                                             CudaNdarray_DEV_DATA(self), // y\n",
      "4052                                             // y strides\n",
      "4053                                             CudaNdarray_HOST_STRIDES(self)[0],\n",
      "4054                                             CudaNdarray_HOST_STRIDES(self)[1],\n",
      "4055                                             CudaNdarray_HOST_STRIDES(self)[2],\n",
      "4056                                             CudaNdarray_HOST_STRIDES(self)[3]\n",
      "4057                                             );\n",
      "4058                 CNDA_THREAD_SYNC;\n",
      "4059                 cudaError_t err = cudaGetLastError();\n",
      "4060                 if( cudaSuccess != err)\n",
      "4061                 {\n",
      "4062                     PyErr_Format(PyExc_RuntimeError,\n",
      "4063                                  \"Cuda error: %s: %s.\",\n",
      "4064                                  \"k_copy_4d\",\n",
      "4065                                  cudaGetErrorString(err));\n",
      "4066                     Py_XDECREF(new_other);\n",
      "4067                     return -1;\n",
      "4068                 }\n",
      "4069             }; break;\n",
      "4070         default:\n",
      "4071             {\n",
      "4072                 cudaError_t err = cudaGetLastError();\n",
      "4073                 if(cudaSuccess != err){\n",
      "4074                     PyErr_Format(PyExc_RuntimeError,\n",
      "4075                                  \"Unexpected Cuda error: %s: %s\\n\",\n",
      "4076                                  \"CudaNdarray_CopyFromCudaNdarray\",\n",
      "4077                                  cudaGetErrorString(err));\n",
      "4078                     Py_XDECREF(new_other);\n",
      "4079                     return -1;\n",
      "4080                 }\n",
      "4081 \n",
      "4082                 if (verbose)\n",
      "4083                     fprintf(stderr,\n",
      "4084                             \"Copying with default version unbroadcast=%d\\n\",\n",
      "4085                             unbroadcast);\n",
      "4086                 // call worker routine\n",
      "4087                 unsigned int threads_per_block = std::min(size,\n",
      "4088                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4089                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4090                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);\n",
      "4091                 const CudaNdarray * cuda_dims = other;\n",
      "4092                 if(unbroadcast)\n",
      "4093                     cuda_dims = self;\n",
      "4094                 //copy from other into self\n",
      "4095                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(\n",
      "4096                         size,\n",
      "4097                         (unsigned int)other->nd,\n",
      "4098                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),\n",
      "4099                         (const float*)CudaNdarray_DEV_DATA(other),\n",
      "4100                         (const int *)CudaNdarray_DEV_STRIDES(other),\n",
      "4101                         CudaNdarray_DEV_DATA(self),\n",
      "4102                         (const int *)CudaNdarray_DEV_STRIDES(self));\n",
      "4103                 CNDA_THREAD_SYNC;\n",
      "4104                 err = cudaGetLastError();\n",
      "4105                 if(verbose>1)\n",
      "4106                     fprintf(stderr,\n",
      "4107                             \"INFO k_elemwise_unary_rowmaj (n_blocks=%i,\"\n",
      "4108                             \" n_threads_per_block=%i)\\n\",\n",
      "4109                             n_blocks, threads_per_block);\n",
      "4110                 if( cudaSuccess != err)\n",
      "4111                 {\n",
      "4112                     //fprint_CudaNdarray(stderr, self);\n",
      "4113                     //fprint_CudaNdarray(stderr, other);\n",
      "4114                     PyErr_Format(PyExc_RuntimeError,\n",
      "4115                                  \"Cuda error: %s: %s. (n_blocks=%i,\"\n",
      "4116                                  \" n_threads_per_block=%i)\\n\",\n",
      "4117                                  \"k_elemwise_unary_rowmajor_copy\",\n",
      "4118                                  cudaGetErrorString(err), n_blocks,\n",
      "4119                                  threads_per_block);\n",
      "4120                     Py_XDECREF(new_other);\n",
      "4121                     return -1;\n",
      "4122                 }\n",
      "4123             }\n",
      "4124     };\n",
      "4125     Py_XDECREF(new_other);\n",
      "4126     return 0;\n",
      "4127 }\n",
      "4128 \n",
      "4129 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4130 {\n",
      "4131     if (A->nd != 2)\n",
      "4132     {\n",
      "4133         PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to gemm\");\n",
      "4134         return -1;\n",
      "4135     }\n",
      "4136     if (B->nd != 2)\n",
      "4137     {\n",
      "4138         PyErr_SetString(PyExc_ValueError, \"non-matrix arg B to gemm\");\n",
      "4139         return -1;\n",
      "4140     }\n",
      "4141     if (C->nd != 2)\n",
      "4142     {\n",
      "4143         PyErr_SetString(PyExc_ValueError, \"non-matrix arg C to gemm\");\n",
      "4144         return -1;\n",
      "4145     }\n",
      "4146 \n",
      "4147     // We must allow dimensions to be zeros.\n",
      "4148     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4149             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])\n",
      "4150             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))\n",
      "4151     {\n",
      "4152         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)\",\n",
      "4153                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4154                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4155                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4156                 CudaNdarray_HOST_DIMS(B)[1],\n",
      "4157                 CudaNdarray_HOST_DIMS(C)[0],\n",
      "4158                 CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4159         return -1;\n",
      "4160     }\n",
      "4161 \n",
      "4162     // If matrix A or B has non-unit size and non-unit stride in both\n",
      "4163     // dimensions, we can make a copy.\n",
      "4164     CudaNdarray * A_new = NULL;\n",
      "4165     CudaNdarray * B_new = NULL;\n",
      "4166     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4167          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4168          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4169          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4170         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4171         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4172     {\n",
      "4173         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4174         if (!A_new)\n",
      "4175             return -1;\n",
      "4176         A = A_new;\n",
      "4177     }\n",
      "4178 \n",
      "4179     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)\n",
      "4180          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)\n",
      "4181          && (CudaNdarray_HOST_DIMS(B)[1] > 1)\n",
      "4182          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))\n",
      "4183         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4184         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))\n",
      "4185     {\n",
      "4186         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4187         if (!B_new)\n",
      "4188         {\n",
      "4189             // If A_new is NULL, meaning A was not copied nothing happens\n",
      "4190             Py_XDECREF(A_new);\n",
      "4191             return -1;\n",
      "4192         }\n",
      "4193         B = B_new;\n",
      "4194     }\n",
      "4195 \n",
      "4196     // If matrix C has non-unit size and non-unit stride in both\n",
      "4197     // dimensions, or negative strides, we can't operate. We cannot copy\n",
      "4198     // C either, because the calling code will expect the result to be\n",
      "4199     // in the original C container.\n",
      "4200     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)\n",
      "4201          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)\n",
      "4202          && (CudaNdarray_HOST_DIMS(C)[1] > 1)\n",
      "4203          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))\n",
      "4204         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)\n",
      "4205         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))\n",
      "4206     {\n",
      "4207         PyErr_Format(PyExc_AssertionError,\n",
      "4208                      \"non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)\",\n",
      "4209                      CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4210                      CudaNdarray_HOST_STRIDES(C)[1],\n",
      "4211                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4212                      CudaNdarray_HOST_DIMS(C)[1]);\n",
      "4213         Py_XDECREF(A_new);\n",
      "4214         Py_XDECREF(B_new);\n",
      "4215         return -1;\n",
      "4216     }\n",
      "4217 \n",
      "4218     // the unit integer is divided logically into three fields of 4 bits\n",
      "4219     // the lowermost 4 bits encode the stride pattern of the output\n",
      "4220     // the next higher 4 bits encode the B variable (or y)\n",
      "4221     // the next higher 4 bits encode the C variable (or x)\n",
      "4222     //\n",
      "4223     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)\n",
      "4224     //                                                 1 for unit stride from row to row (Col major)\n",
      "4225 \n",
      "4226     // a stride of 0 implies a dimension of 1 - so we can actually define\n",
      "4227     // a stride of 0 as a 'unit' stride because gemm will never use it.\n",
      "4228     // If a dimension is 0, its stride will not be used either, so we can\n",
      "4229     // consider it a 'unit' stride too.\n",
      "4230     int unit = 0;\n",
      "4231     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {\n",
      "4232         unit |= (0x0 << 8);\n",
      "4233     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {\n",
      "4234         unit |= (0x1 << 8);\n",
      "4235     } else {\n",
      "4236         unit |= (0x2 << 8);\n",
      "4237     }\n",
      "4238     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {\n",
      "4239         unit |= (0x0 << 4);\n",
      "4240     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {\n",
      "4241         unit |= (0x1 << 4);\n",
      "4242     } else {\n",
      "4243         unit |= (0x2 << 4);\n",
      "4244     }\n",
      "4245     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {\n",
      "4246         unit |= (0x0 << 0);\n",
      "4247     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {\n",
      "4248         unit |= (0x1 << 0);\n",
      "4249     } else {\n",
      "4250         unit |= (0x2 << 0);\n",
      "4251     }\n",
      "4252 \n",
      "4253     /* create appropriate strides for malformed matrices that are row or column\n",
      "4254      * vectors\n",
      "4255      */\n",
      "4256     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4257     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4258     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];\n",
      "4259     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];\n",
      "4260     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];\n",
      "4261     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];\n",
      "4262 \n",
      "4263     float* a = CudaNdarray_DEV_DATA(A);\n",
      "4264     float* b = CudaNdarray_DEV_DATA(B);\n",
      "4265     float* c = CudaNdarray_DEV_DATA(C);\n",
      "4266     cublasOperation_t N = CUBLAS_OP_N;\n",
      "4267     cublasOperation_t T = CUBLAS_OP_T;\n",
      "4268     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\\\n';\n",
      "4269     // There should be no negative stride at that point\n",
      "4270 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \\\n",
      "4271     if (sx == 0){sx = 1;}\\\n",
      "4272     if (sy == 0){sy = 1;}\\\n",
      "4273     if (sz == 0){sz = 1;}\\\n",
      "4274     if ((sx > 0) && (sy > 0) && (sz > 0)) { \\\n",
      "4275         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \\\n",
      "4276     } else { \\\n",
      "4277         PyErr_SetString(PyExc_AssertionError, \"negative stride to sGemm\");\\\n",
      "4278         Py_XDECREF(A_new);\\\n",
      "4279         Py_XDECREF(B_new);\\\n",
      "4280         return -1; \\\n",
      "4281     }\n",
      "4282 \n",
      "4283     cublasStatus_t err;\n",
      "4284     switch(unit)\n",
      "4285     {\n",
      "4286         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c, sc_0); break;\n",
      "4287         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c, sc_0); break;\n",
      "4288         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c, sc_0); break;\n",
      "4289         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c, sc_0); break;\n",
      "4290         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c, sc_1); break;\n",
      "4291         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c, sc_1); break;\n",
      "4292         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c, sc_1); break;\n",
      "4293         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c, sc_1); break;\n",
      "4294         default: PyErr_Format(PyExc_ValueError, \"some matrix has no unit stride (unit=%x)\", unit);\n",
      "4295                  return -1;\n",
      "4296     };\n",
      "4297     CNDA_THREAD_SYNC;\n",
      "4298     Py_XDECREF(A_new);\n",
      "4299     Py_XDECREF(B_new);\n",
      "4300 \n",
      "4301     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4302     {\n",
      "4303         PyErr_Format(PyExc_RuntimeError,\n",
      "4304                      \"cublasSgemm failed (%i) %s\\n\"\n",
      "4305                      \" unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p\"\n",
      "4306                      \" sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d\",\n",
      "4307                      err,  cublasGetErrorString(err),\n",
      "4308                      unit, N,\n",
      "4309                      CudaNdarray_HOST_DIMS(C)[0],\n",
      "4310                      CudaNdarray_HOST_DIMS(C)[1],\n",
      "4311                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4312                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);\n",
      "4313 \n",
      "4314         return -1;\n",
      "4315     }\n",
      "4316     return 0;\n",
      "4317 }\n",
      "4318 \n",
      "4319 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)\n",
      "4320 {\n",
      "4321     /**\n",
      "4322     * C <- alpha A B + beta C\n",
      "4323     *    A : matrix\n",
      "4324     *    B, C: vector\n",
      "4325     *    alpha, beta: scalars\n",
      "4326     */\n",
      "4327     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg to gemv\"); return -1; }\n",
      "4328     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4329     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg to gemv\"); return -1; }\n",
      "4330 \n",
      "4331     // We must allow dimensions to be zeros.\n",
      "4332     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])\n",
      "4333             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))\n",
      "4334     {\n",
      "4335         PyErr_Format(PyExc_ValueError, \"dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)\",\n",
      "4336                 CudaNdarray_HOST_DIMS(A)[0],\n",
      "4337                 CudaNdarray_HOST_DIMS(A)[1],\n",
      "4338                 CudaNdarray_HOST_DIMS(B)[0],\n",
      "4339                 CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4340         return -1;\n",
      "4341     }\n",
      "4342 \n",
      "4343     // If matrix A has non-unit size and non-unit stride in both\n",
      "4344     // dimensions, or negative strides, we cannot operate, but we can\n",
      "4345     // make a copy.\n",
      "4346     CudaNdarray * A_new = NULL;\n",
      "4347     CudaNdarray * B_new = NULL;\n",
      "4348     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)\n",
      "4349          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)\n",
      "4350          && (CudaNdarray_HOST_DIMS(A)[1] > 1)\n",
      "4351          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))\n",
      "4352         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4353         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4354     {\n",
      "4355         A_new = (CudaNdarray*) CudaNdarray_Copy(A);\n",
      "4356         if (!A_new)\n",
      "4357             return -1;\n",
      "4358         A = A_new;\n",
      "4359     }\n",
      "4360 \n",
      "4361     // If vector B as a negative stride, we also have to make a copy.\n",
      "4362     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)\n",
      "4363     {\n",
      "4364         B_new = (CudaNdarray*) CudaNdarray_Copy(B);\n",
      "4365         if (!B_new)\n",
      "4366         {\n",
      "4367             // If A was not copied, A_new is NULL, and Py_XDECREF does not\n",
      "4368             // do anything\n",
      "4369             Py_XDECREF(A_new);\n",
      "4370             return -1;\n",
      "4371         }\n",
      "4372         B = B_new;\n",
      "4373     }\n",
      "4374 \n",
      "4375     // cudablas does not handle negative strides as expected\n",
      "4376     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)\n",
      "4377         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))\n",
      "4378     {\n",
      "4379         PyErr_Format(PyExc_ValueError, \"illegal strides in args to gemv (%i,%i)\",\n",
      "4380                 CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4381                 CudaNdarray_HOST_STRIDES(A)[1]);\n",
      "4382         Py_XDECREF(A_new);\n",
      "4383         Py_XDECREF(B_new);\n",
      "4384         return -1;\n",
      "4385     }\n",
      "4386 \n",
      "4387     /* create appropriate strides for malformed matrices that are row or column\n",
      "4388      * vectors\n",
      "4389      */\n",
      "4390     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4391     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4392     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;\n",
      "4393     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;\n",
      "4394 \n",
      "4395     if (sa_0 == 0) sa_0 = 1;\n",
      "4396     if (sa_1 == 0) sa_1 = 1;\n",
      "4397 \n",
      "4398     int used_dot = 0;\n",
      "4399 \n",
      "4400     // This is important because we can end up not calling Sgemv at all\n",
      "4401     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4402     if (CudaNdarray_SIZE(C)) {\n",
      "4403         // A is row vector & alpha==1 & beta==0 -> use cublasSdot\n",
      "4404         if (CudaNdarray_HOST_DIMS(A)[0] == 1 && alpha==1.f && beta==0.f) {\n",
      "4405             //replace this with custom inner product kernel with alpha and beta parameter?\n",
      "4406             cublasPointerMode_t pmode;\n",
      "4407             //set pointer mode to make sure cublas not storing on host pointer\n",
      "4408             cublasGetPointerMode(handle, &pmode);\n",
      "4409             cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_DEVICE);\n",
      "4410             err = cublasSdot(\n",
      "4411                     handle, CudaNdarray_HOST_DIMS(A)[1],\n",
      "4412                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4413                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4414                     CudaNdarray_DEV_DATA(C));\n",
      "4415             cublasSetPointerMode(handle, pmode);\n",
      "4416             used_dot = 1;\n",
      "4417         }\n",
      "4418         // A is row-contiguous | row vector\n",
      "4419         else if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4420             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4421                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4422         {\n",
      "4423             err = cublasSgemv(handle, CUBLAS_OP_N,\n",
      "4424                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],\n",
      "4425                     &alpha,\n",
      "4426                     CudaNdarray_DEV_DATA(A), sa_1,\n",
      "4427                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4428                     &beta,\n",
      "4429                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4430         }\n",
      "4431         // A is column-contiguous | column vector\n",
      "4432         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4433                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4434                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4435         {\n",
      "4436             err = cublasSgemv(handle, CUBLAS_OP_T,\n",
      "4437                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],\n",
      "4438                     &alpha,\n",
      "4439                     CudaNdarray_DEV_DATA(A), sa_0,\n",
      "4440                     CudaNdarray_DEV_DATA(B), sb_0,\n",
      "4441                     &beta,\n",
      "4442                     CudaNdarray_DEV_DATA(C), sc_0);\n",
      "4443         }\n",
      "4444         // A is non vector and have malformed strides\n",
      "4445         else\n",
      "4446         {\n",
      "4447             PyErr_Format(PyExc_AssertionError,\n",
      "4448                          \"Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\\n\"\n",
      "4449                          \"Shapes are: (%i, %i) x %i -> %i\\n\",\n",
      "4450                          CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4451                          CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4452                          CudaNdarray_HOST_STRIDES(B)[0],\n",
      "4453                          CudaNdarray_HOST_STRIDES(C)[0],\n",
      "4454                          CudaNdarray_HOST_DIMS(A)[0],\n",
      "4455                          CudaNdarray_HOST_DIMS(A)[1],\n",
      "4456                          CudaNdarray_HOST_DIMS(B)[0],\n",
      "4457                          CudaNdarray_HOST_DIMS(C)[0]);\n",
      "4458             Py_XDECREF(A_new);\n",
      "4459             Py_XDECREF(B_new);\n",
      "4460             return -1;\n",
      "4461         }\n",
      "4462     }\n",
      "4463 \n",
      "4464     CNDA_THREAD_SYNC;\n",
      "4465     Py_XDECREF(A_new);\n",
      "4466     Py_XDECREF(B_new);\n",
      "4467 \n",
      "4468     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4469     {\n",
      "4470         if (!used_dot)\n",
      "4471         {\n",
      "4472             PyErr_Format(PyExc_RuntimeError,\n",
      "4473                          \"cublasSgemv failed (%i)\",\n",
      "4474                          err);\n",
      "4475         } else {\n",
      "4476         PyErr_Format(PyExc_RuntimeError,\n",
      "4477                      \"cublasSdot failed (%i)\",\n",
      "4478                      err);\n",
      "4479         }\n",
      "4480         return -1;\n",
      "4481     }\n",
      "4482     return 0;\n",
      "4483 }\n",
      "4484 \n",
      "4485 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {\n",
      "4486     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg x to sger\"); return -1; }\n",
      "4487     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, \"non-vector arg y to sger\"); return -1; }\n",
      "4488     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, \"non-matrix arg A to sger\"); return -1; }\n",
      "4489 \n",
      "4490     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])\n",
      "4491         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {\n",
      "4492         PyErr_Format(PyExc_ValueError,\n",
      "4493                      \"dimension mismatch in args to sger (%i)x(%i)->(%i,%i)\",\n",
      "4494                      CudaNdarray_HOST_DIMS(x)[0],\n",
      "4495                      CudaNdarray_HOST_DIMS(y)[0],\n",
      "4496                      CudaNdarray_HOST_DIMS(A)[0],\n",
      "4497                      CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4498         return -1;\n",
      "4499     }\n",
      "4500 \n",
      "4501     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4502     CudaNdarray * x_new = NULL;\n",
      "4503     if(x_strides == 0){\n",
      "4504         if(CudaNdarray_HOST_DIMS(x)[0] != 1){\n",
      "4505             PyErr_Format(PyExc_RuntimeError,\n",
      "4506                          \"CudaNdarray_sger: Invalid input x (should not happen).\"\n",
      "4507                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4508                          \" that has more than 1 element!\");\n",
      "4509             return -1;\n",
      "4510         }\n",
      "4511         x_strides = 1;\n",
      "4512     } else if(x_strides < 0){\n",
      "4513         x_new = (CudaNdarray*) CudaNdarray_Copy(x);\n",
      "4514         x = x_new;\n",
      "4515         x_strides = CudaNdarray_HOST_STRIDES(x)[0];\n",
      "4516     }\n",
      "4517 \n",
      "4518     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4519     CudaNdarray * y_new = NULL;\n",
      "4520     if(y_strides == 0){\n",
      "4521         if(CudaNdarray_HOST_DIMS(y)[0] != 1){\n",
      "4522             PyErr_Format(PyExc_RuntimeError,\n",
      "4523                          \"CudaNdarray_sger: Invalid input y (should not happen).\"\n",
      "4524                          \" We received a CudaNdarray vector with a stride of 0\"\n",
      "4525                          \" that has more than 1 elements!\");\n",
      "4526             Py_XDECREF(x_new);\n",
      "4527             return -1;\n",
      "4528         }\n",
      "4529         y_strides = 1;\n",
      "4530     } else if(y_strides < 0){\n",
      "4531         y_new = (CudaNdarray*) CudaNdarray_Copy(y);\n",
      "4532         y = y_new;\n",
      "4533         y_strides = CudaNdarray_HOST_STRIDES(y)[0];\n",
      "4534     }\n",
      "4535 \n",
      "4536     // Create appropriate strides if A is a row or column vector\n",
      "4537     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]\n",
      "4538                                                  : CudaNdarray_HOST_DIMS(A)[1];\n",
      "4539     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]\n",
      "4540                                                  : CudaNdarray_HOST_DIMS(A)[0];\n",
      "4541 \n",
      "4542     // This is important because we can end up not calling Sger at all\n",
      "4543     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;\n",
      "4544     if(CudaNdarray_SIZE(A)){\n",
      "4545         // If A is in col-major\n",
      "4546         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)\n",
      "4547             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)\n",
      "4548                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))\n",
      "4549         {\n",
      "4550             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,\n",
      "4551                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4552                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4553                        CudaNdarray_DEV_DATA(A), sa_1);\n",
      "4554         }\n",
      "4555         // Since Sger expects A in col-major, we invert x and y to fake this.\n",
      "4556         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)\n",
      "4557                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)\n",
      "4558                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))\n",
      "4559         {\n",
      "4560             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,\n",
      "4561                        CudaNdarray_DEV_DATA(y), y_strides,\n",
      "4562                        CudaNdarray_DEV_DATA(x), x_strides,\n",
      "4563                        CudaNdarray_DEV_DATA(A), sa_0);\n",
      "4564         }\n",
      "4565         // A has to be either c- or f-contiguous, with no negative strides\n",
      "4566         else\n",
      "4567         {\n",
      "4568             PyErr_SetString(PyExc_NotImplementedError,\n",
      "4569                             \"non-contiguous A, or negative strides, in sger\");\n",
      "4570             Py_XDECREF(x_new);\n",
      "4571             Py_XDECREF(y_new);\n",
      "4572             return -1;\n",
      "4573         }\n",
      "4574     }\n",
      "4575     CNDA_THREAD_SYNC;\n",
      "4576     Py_XDECREF(x_new);\n",
      "4577     Py_XDECREF(y_new);\n",
      "4578 \n",
      "4579     if (CUBLAS_STATUS_SUCCESS != err)\n",
      "4580     {\n",
      "4581         PyErr_Format(PyExc_RuntimeError,\n",
      "4582                      \"cublasSger failed (%i)\",\n",
      "4583                      err);\n",
      "4584         return -1;\n",
      "4585     }\n",
      "4586 \n",
      "4587     return 0;\n",
      "4588 }\n",
      "4589 \n",
      "4590 /**\n",
      "4591  *\n",
      "4592  * Precondition:\n",
      "4593  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]\n",
      "4594  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];\n",
      "4595  *\n",
      "4596  *  TODO: templatize this function to support other reductions.\n",
      "4597  *  All that needs to change is the initial value for sum, and the reduction operator.\n",
      "4598  */\n",
      "4599 \n",
      "4600 static __global__ void kernel_reduce_sum(const unsigned int size_z,\n",
      "4601         const unsigned int nd,\n",
      "4602         const int * dims_a,\n",
      "4603         const int * log2_dims_a,\n",
      "4604         const int * a_str,\n",
      "4605         const float * a_data,\n",
      "4606         const int * z_str,\n",
      "4607         float * z_data)\n",
      "4608 {\n",
      "4609     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "4610     const unsigned int numThreads = blockDim.x * gridDim.x;\n",
      "4611 \n",
      "4612     //structure data contains the strides and dimensions of both a and z\n",
      "4613     // a_dim[0], a_dim[1], ... a_dim[nd-1],\n",
      "4614     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],\n",
      "4615     // a_str[0], ... a_str[nd-1],\n",
      "4616     // z_str[0], ... z_str[nd-1]\n",
      "4617     extern __shared__ int structure_data[];\n",
      "4618     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)\n",
      "4619     {\n",
      "4620         structure_data[i+0*nd] = dims_a[i];\n",
      "4621         structure_data[i+1*nd] = log2_dims_a[i];\n",
      "4622         structure_data[i+2*nd] = a_str[i];\n",
      "4623         structure_data[i+3*nd] = z_str[i];\n",
      "4624     }\n",
      "4625     dims_a = structure_data;\n",
      "4626     log2_dims_a = structure_data + nd;\n",
      "4627     a_str = structure_data + 2*nd;\n",
      "4628     z_str = structure_data + 3*nd;\n",
      "4629 \n",
      "4630     __syncthreads(); //wait for all the shared structure to be loaded\n",
      "4631 \n",
      "4632     for (unsigned int i = idx; i < size_z; i += numThreads)\n",
      "4633     {\n",
      "4634         unsigned int ii = i;\n",
      "4635         const float * a_data_i = a_data;\n",
      "4636         float * z_data_i = z_data;\n",
      "4637         unsigned int n_reduce_elements = 1;\n",
      "4638         unsigned int n_reduce_dims = 0;\n",
      "4639         unsigned int reduce_dim0 = nd-1;\n",
      "4640 \n",
      "4641 \n",
      "4642         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread\n",
      "4643         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)\n",
      "4644         for (unsigned int d = 0; d < nd; ++d)\n",
      "4645         {\n",
      "4646             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over\n",
      "4647             {\n",
      "4648                 n_reduce_elements *= dims_a[d];\n",
      "4649                 n_reduce_dims += 1;\n",
      "4650                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;\n",
      "4651             }\n",
      "4652             else //'d' is not a dimension that we are reducing over\n",
      "4653             {\n",
      "4654                 unsigned int pos_d;\n",
      "4655                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch\n",
      "4656                 {\n",
      "4657                     // this branch is not preferred,\n",
      "4658                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4659                     pos_d = (ii % dims_a[d]);\n",
      "4660                     ii = (ii / dims_a[d]);\n",
      "4661                 }\n",
      "4662                 else\n",
      "4663                 {\n",
      "4664                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits\n",
      "4665                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii\n",
      "4666                 }\n",
      "4667                 a_data_i += pos_d * a_str[d];\n",
      "4668                 z_data_i += pos_d * z_str[d];\n",
      "4669             }\n",
      "4670         }\n",
      "4671         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing\n",
      "4672         // do a similar loop\n",
      "4673 \n",
      "4674         float sum = 0.0f;\n",
      "4675         switch(n_reduce_dims)\n",
      "4676         {\n",
      "4677             case 0:\n",
      "4678                 {\n",
      "4679                     sum = a_data_i[0];\n",
      "4680                 }\n",
      "4681                 break;\n",
      "4682             case 1:\n",
      "4683                 {\n",
      "4684                     const int stride = a_str[reduce_dim0];\n",
      "4685                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;\n",
      "4686                     while (a_data_i != a_data_i_max)\n",
      "4687                     {\n",
      "4688                         sum += a_data_i[0];\n",
      "4689                         a_data_i += stride;\n",
      "4690                     }\n",
      "4691                 }\n",
      "4692                 break;\n",
      "4693             case 2:\n",
      "4694                 {\n",
      "4695                     int rd = reduce_dim0+1;\n",
      "4696                     for (; rd < nd; ++rd)\n",
      "4697                     {\n",
      "4698                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over\n",
      "4699                             break;\n",
      "4700                     }\n",
      "4701                     const int stride0 = a_str[reduce_dim0];\n",
      "4702                     const int stride1 = a_str[rd];\n",
      "4703                     for (int ii = 0; ii < dims_a[rd]; ++ii)\n",
      "4704                     {\n",
      "4705                         const float * a_data_ri = a_data_i + ii * stride1;\n",
      "4706                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;\n",
      "4707                         while (a_data_ri != a_data_ri_max)\n",
      "4708                         {\n",
      "4709                             sum += a_data_ri[0];\n",
      "4710                             a_data_ri += stride0;\n",
      "4711                         }\n",
      "4712                     }\n",
      "4713                 };\n",
      "4714                 break;\n",
      "4715             default:\n",
      "4716                 {\n",
      "4717                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)\n",
      "4718                     {\n",
      "4719                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.\n",
      "4720                         unsigned int reduce_ii = reduce_i;\n",
      "4721                         const float * a_data_ri = a_data_i;\n",
      "4722 \n",
      "4723                         //This loop finds the element in the a slice to add.\n",
      "4724                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)\n",
      "4725                         {\n",
      "4726                             unsigned int pos_d;\n",
      "4727                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over\n",
      "4728                             {\n",
      "4729                                 if (log2_dims_a[rd]==-1)\n",
      "4730                                 {\n",
      "4731                                     // this branch is not preferred,\n",
      "4732                                     // because the manual said that integer mod and div operations are slow on gpu\n",
      "4733                                     pos_d = (reduce_ii % dims_a[rd]);\n",
      "4734                                     reduce_ii = (reduce_ii / dims_a[rd]);\n",
      "4735                                 }\n",
      "4736                                 else\n",
      "4737                                 {\n",
      "4738                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits\n",
      "4739                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii\n",
      "4740                                 }\n",
      "4741                                 a_data_ri += pos_d * a_str[rd];\n",
      "4742                             }\n",
      "4743                         }\n",
      "4744                         sum += a_data_ri[0];\n",
      "4745                     }\n",
      "4746                 }\n",
      "4747         }\n",
      "4748         z_data_i[0] = sum;\n",
      "4749     }\n",
      "4750 }\n",
      "4751 \n",
      "4752 static __global__ void kernel_reduce_sum_1011(\n",
      "4753         const unsigned int d0,\n",
      "4754         const unsigned int d1,\n",
      "4755         const unsigned int d2,\n",
      "4756         const unsigned int d3,\n",
      "4757         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,\n",
      "4758         float * Z, const int sZ0)\n",
      "4759 {\n",
      "4760     const int threadCount = blockDim.x * blockDim.y * blockDim.z;\n",
      "4761     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;\n",
      "4762     extern __shared__ float buf[];\n",
      "4763     float mysum = 0.0f;\n",
      "4764 \n",
      "4765     if (warpSize != 32)\n",
      "4766     {\n",
      "4767         return;  //TODO: set error code\n",
      "4768     }\n",
      "4769 \n",
      "4770     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)\n",
      "4771     {\n",
      "4772         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];\n",
      "4773         mysum += Ai;\n",
      "4774     }\n",
      "4775     buf[threadNum] = mysum;\n",
      "4776     __syncthreads();\n",
      "4777 \n",
      "4778     // rest of function is handled by one warp\n",
      "4779     if (threadNum < warpSize)\n",
      "4780     {\n",
      "4781         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)\n",
      "4782         {\n",
      "4783             mysum += buf[i];\n",
      "4784         }\n",
      "4785         buf[threadNum] = mysum;\n",
      "4786         if (threadNum < 16)\n",
      "4787         {\n",
      "4788             //reduce so that threadNum 0 has the sum of everything\n",
      "4789             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];\n",
      "4790             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];\n",
      "4791             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];\n",
      "4792             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];\n",
      "4793             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];\n",
      "4794             if (threadNum == 0)\n",
      "4795             {\n",
      "4796                 Z[blockIdx.x*sZ0] = buf[0];\n",
      "4797             }\n",
      "4798         }\n",
      "4799     }\n",
      "4800 }\n",
      "4801 /**\n",
      "4802  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions\n",
      "4803  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.\n",
      "4804  */\n",
      "4805 int\n",
      "4806 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)\n",
      "4807 {\n",
      "4808     int verbose = 0;\n",
      "4809     //check input rank\n",
      "4810     if (self->nd != A->nd)\n",
      "4811     {\n",
      "4812         PyErr_Format(PyExc_TypeError, \"Rank mismatch in CudaNdarray_sum: %i vs %i\", self->nd, A->nd);\n",
      "4813         return -1;\n",
      "4814     }\n",
      "4815     for (int i = 0; i < self->nd; ++i)\n",
      "4816     {\n",
      "4817         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))\n",
      "4818         {\n",
      "4819             PyErr_Format(PyExc_TypeError, \"Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i\",\n",
      "4820                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);\n",
      "4821             return -1;\n",
      "4822         }\n",
      "4823     }\n",
      "4824 \n",
      "4825     int n_summations = (unsigned int)CudaNdarray_SIZE(self);\n",
      "4826     if (verbose)\n",
      "4827     {\n",
      "4828         std::cerr << \"reduce_sum n_summations \" << n_summations  << '\\n';\n",
      "4829         std::cerr << \"reduce_sum nd \" << self->nd  << '\\n';\n",
      "4830         fprint_CudaNdarray(stderr, A);\n",
      "4831         fprint_CudaNdarray(stderr, self);\n",
      "4832     }\n",
      "4833     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies\n",
      "4834             && (CudaNdarray_HOST_DIMS(self)[0] == 1)\n",
      "4835             && (CudaNdarray_HOST_DIMS(self)[2] == 1)\n",
      "4836             && (CudaNdarray_HOST_DIMS(self)[3] == 1)\n",
      "4837        )\n",
      "4838     {\n",
      "4839         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);\n",
      "4840         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);\n",
      "4841         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;\n",
      "4842         n_threads.z -= 1;\n",
      "4843         if (n_threads.z > 64) n_threads.z = 64;\n",
      "4844         if (n_threads.z)\n",
      "4845         {\n",
      "4846             if (verbose) printf(\"trying kernel_reduce_sum_1011\\n\");\n",
      "4847             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;\n",
      "4848             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(\n",
      "4849                     CudaNdarray_HOST_DIMS(A)[0],\n",
      "4850                     CudaNdarray_HOST_DIMS(A)[1],\n",
      "4851                     CudaNdarray_HOST_DIMS(A)[2],\n",
      "4852                     CudaNdarray_HOST_DIMS(A)[3],\n",
      "4853                     CudaNdarray_DEV_DATA(A),\n",
      "4854                     CudaNdarray_HOST_STRIDES(A)[0],\n",
      "4855                     CudaNdarray_HOST_STRIDES(A)[1],\n",
      "4856                     CudaNdarray_HOST_STRIDES(A)[2],\n",
      "4857                     CudaNdarray_HOST_STRIDES(A)[3],\n",
      "4858                     CudaNdarray_DEV_DATA(self),\n",
      "4859                     CudaNdarray_HOST_STRIDES(self)[1]);\n",
      "4860             CNDA_THREAD_SYNC;\n",
      "4861             if (cudaSuccess == cudaGetLastError()) return 0;\n",
      "4862             if (verbose) printf(\"failed, falling back to kernel_reduce_sum\\n\");\n",
      "4863         }\n",
      "4864     }\n",
      "4865 \n",
      "4866     int n_threads_per_block = std::min(n_summations,\n",
      "4867             NUM_VECTOR_OP_THREADS_PER_BLOCK);\n",
      "4868     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),\n",
      "4869             NUM_VECTOR_OP_BLOCKS);\n",
      "4870     int n_structure_cache = self->nd * 4 * sizeof(int);\n",
      "4871 \n",
      "4872     if (verbose)\n",
      "4873     {\n",
      "4874         std::cerr << \"n_blocks, n_threads_per_block \" << n_blocks << ' ' << n_threads_per_block  << '\\n';\n",
      "4875     }\n",
      "4876     assert (self->nd > 0);\n",
      "4877     assert (self->nd == A->nd);\n",
      "4878     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(\n",
      "4879             n_summations,\n",
      "4880             self->nd,\n",
      "4881             CudaNdarray_DEV_DIMS(A),\n",
      "4882             CudaNdarray_DEV_LOG2DIMS(A),\n",
      "4883             CudaNdarray_DEV_STRIDES(A),\n",
      "4884             CudaNdarray_DEV_DATA(A),\n",
      "4885             CudaNdarray_DEV_STRIDES(self),\n",
      "4886             CudaNdarray_DEV_DATA(self));\n",
      "4887     CNDA_THREAD_SYNC;\n",
      "4888     cudaError_t err = cudaGetLastError();\n",
      "4889     if (cudaSuccess != err)\n",
      "4890     {\n",
      "4891         PyErr_Format(PyExc_RuntimeError, \"Cuda error: %s: %s.\\n\", \"kernel_reduce_sum\", cudaGetErrorString(err));\n",
      "4892         return -1;\n",
      "4893     }\n",
      "4894     return 0;\n",
      "4895 }\n",
      "4896 int\n",
      "4897 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)\n",
      "4898 {\n",
      "4899     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4900     return -1;\n",
      "4901 }\n",
      "4902 int\n",
      "4903 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)\n",
      "4904 {\n",
      "4905     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4906     return -1;\n",
      "4907 }\n",
      "4908 int\n",
      "4909 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)\n",
      "4910 {\n",
      "4911     PyErr_SetString(PyExc_NotImplementedError, \"\");\n",
      "4912     return -1;\n",
      "4913 }\n",
      "4914 \n",
      "4915 \n",
      "4916 /**\n",
      "4917  *\n",
      "4918  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:\n",
      "4919  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1\n",
      "4920  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.\n",
      "4921  *\n",
      "4922  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:\n",
      "4923  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.\n",
      "4924  */\n",
      "4925 int\n",
      "4926 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)\n",
      "4927 {\n",
      "4928     //TODO: pass a workspace pointer to avoid the internal malloc\n",
      "4929     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.\n",
      "4930     int * newstrides = newdims + len;\n",
      "4931     int * dims_taken = newstrides + len;\n",
      "4932     if (!newdims)\n",
      "4933     {\n",
      "4934         PyErr_SetString(PyExc_MemoryError, \"CudaNdarray_dimshuffle: Failed to allocate temporary space\");\n",
      "4935         return -1;\n",
      "4936     }\n",
      "4937     for (int i = 0; i < self->nd; ++i)\n",
      "4938     {\n",
      "4939         dims_taken[i] = 0;\n",
      "4940     }\n",
      "4941     for (int i = 0; i < len; ++i)\n",
      "4942     {\n",
      "4943         if (pattern[i] < 0)\n",
      "4944         {\n",
      "4945             newdims[i] = 1;\n",
      "4946             newstrides[i] = 0;\n",
      "4947         }\n",
      "4948         else if(dims_taken[pattern[i]])\n",
      "4949         {\n",
      "4950             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time\",\n",
      "4951                          pattern[i]);\n",
      "4952             free(newdims);\n",
      "4953             return -1;\n",
      "4954         }\n",
      "4955         else if (pattern[i]>= self->nd)\n",
      "4956         {\n",
      "4957             PyErr_Format(PyExc_ValueError, \"Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for a %d dims CudaNdarray\",\n",
      "4958                          pattern[i], self->nd);\n",
      "4959             free(newdims);\n",
      "4960             return -1;\n",
      "4961         }\n",
      "4962         else\n",
      "4963         {\n",
      "4964             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];\n",
      "4965             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];\n",
      "4966             dims_taken[pattern[i]] = 1;\n",
      "4967         }\n",
      "4968     }\n",
      "4969     //Check if we dropped not broadcastable dims\n",
      "4970     for (int i = 0; i < self->nd; ++i)\n",
      "4971     {\n",
      "4972         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)\n",
      "4973         {\n",
      "4974             PyErr_SetString(PyExc_ValueError, \"Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.\");\n",
      "4975             free(newdims);\n",
      "4976             return -1;\n",
      "4977         }\n",
      "4978     }\n",
      "4979     //swap this structure in for the one in self, and sync to the card\n",
      "4980     if (CudaNdarray_set_nd(self, len))\n",
      "4981     {\n",
      "4982         free(newdims);\n",
      "4983         return -1;\n",
      "4984     }\n",
      "4985     for (int i = 0; i < len; ++i)\n",
      "4986     {\n",
      "4987         CudaNdarray_set_dim(self, i, newdims[i]);\n",
      "4988         CudaNdarray_set_stride(self, i, newstrides[i]);\n",
      "4989     }\n",
      "4990     if (cnda_copy_structure_to_device(self))\n",
      "4991     {\n",
      "4992         free(newdims);\n",
      "4993         return -1;\n",
      "4994     }\n",
      "4995     free(newdims);\n",
      "4996     return 0;\n",
      "4997 }\n",
      "4998 \n",
      "4999 \n",
      "5000 \n",
      "5001 /**\n",
      "5002  *\n",
      "5003  *  This is the function that bind to python.\n",
      "5004  *  See CudaNdarray_dimshuffle to call from C.\n",
      "5005  *  We use -1 to mean 'x' as in Tensor Dimshuffle.\n",
      "5006  */\n",
      "5007 PyObject *\n",
      "5008 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)\n",
      "5009 {\n",
      "5010     PyObject * self = NULL;\n",
      "5011     PyObject * pattern_object = NULL;\n",
      "5012     int * pattern = NULL;\n",
      "5013     PyObject * rval = NULL;\n",
      "5014     int success = -1;\n",
      "5015     //const int * dims = NULL;\n",
      "5016 \n",
      "5017     //args should consist of two python objects (\"OO\")\n",
      "5018     if (! PyArg_ParseTuple(args, \"OO\", &self, &pattern_object))\n",
      "5019         return NULL;\n",
      "5020 \n",
      "5021     if (!CudaNdarray_Check(self) )\n",
      "5022     {\n",
      "5023         PyErr_SetString(PyExc_TypeError, \"First argument to cuda_ndarray.dimshuffle must be a CudaNdarray\");\n",
      "5024         return NULL;\n",
      "5025     }\n",
      "5026 \n",
      "5027     //parse pattern_object into int * pattern\n",
      "5028 \n",
      "5029     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);\n",
      "5030 \n",
      "5031     if (pattern_dim < 0)\n",
      "5032     {\n",
      "5033         PyErr_SetString(PyExc_TypeError, \"Couldn't get length of third argument to cuda_ndarray.dimshuffle\");\n",
      "5034         return NULL;\n",
      "5035     }\n",
      "5036 \n",
      "5037     pattern = (int *) malloc( pattern_dim * sizeof(int));\n",
      "5038 \n",
      "5039     for (Py_ssize_t i = 0; i < pattern_dim; i++)\n",
      "5040     {\n",
      "5041         PyObject * idx = PyLong_FromSsize_t(i);\n",
      "5042 \n",
      "5043         if (idx == NULL)\n",
      "5044         {\n",
      "5045             PyErr_SetString(PyExc_Exception, \"Couldn't make long object to loop over list/tuple\");\n",
      "5046             goto CudaNdarray_dimshuffle_fail;\n",
      "5047         }\n",
      "5048 \n",
      "5049         long elem_value = 0;\n",
      "5050 \n",
      "5051         PyObject * elem = PyObject_GetItem(pattern_object, idx);\n",
      "5052 \n",
      "5053         if (elem == NULL)\n",
      "5054         {\n",
      "5055             Py_XDECREF( elem);\n",
      "5056             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5057             goto CudaNdarray_dimshuffle_fail;\n",
      "5058         }\n",
      "5059 \n",
      "5060         elem_value = PyInt_AsLong(elem);\n",
      "5061 \n",
      "5062         if (elem_value == -1 && PyErr_Occurred() )\n",
      "5063         {\n",
      "5064             Py_XDECREF(elem);\n",
      "5065             PyErr_SetString(PyExc_ValueError, \"Third argument to dimshuffle must be list or tuple of integers\");\n",
      "5066             goto CudaNdarray_dimshuffle_fail;\n",
      "5067         }\n",
      "5068 \n",
      "5069         pattern[i] = elem_value;\n",
      "5070 \n",
      "5071         Py_XDECREF( elem );\n",
      "5072         Py_XDECREF( idx );\n",
      "5073     }\n",
      "5074 \n",
      "5075     //allocate rval\n",
      "5076     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);\n",
      "5077 \n",
      "5078     if (rval == NULL)\n",
      "5079     {\n",
      "5080         //CudaNdarray_New should have set the exception string\n",
      "5081         goto CudaNdarray_dimshuffle_fail;\n",
      "5082     }\n",
      "5083 \n",
      "5084 \n",
      "5085     //printf(\"pattern_dim: %d\\n\",pattern_dim);\n",
      "5086     //printf(\"pattern: %d %d\\n\",pattern[0],pattern[1]);\n",
      "5087     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);\n",
      "5088     //printf(\"dims before: %d %d\\n\",dims[0],dims[1]);\n",
      "5089 \n",
      "5090     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);\n",
      "5091 \n",
      "5092     if (success != 0)\n",
      "5093     {\n",
      "5094         //Exception string should already be set by CudaNdarray_dimshuffle\n",
      "5095         goto CudaNdarray_dimshuffle_fail;\n",
      "5096     }\n",
      "5097 \n",
      "5098     free(pattern);\n",
      "5099 \n",
      "5100     return rval;\n",
      "5101 \n",
      "5102     CudaNdarray_dimshuffle_fail:\n",
      "5103 \n",
      "5104     if (pattern != NULL)\n",
      "5105         free(pattern);\n",
      "5106 \n",
      "5107     Py_XDECREF(rval);\n",
      "5108     return NULL;\n",
      "5109 }\n",
      "5110 \n",
      "5111 \n",
      "5112 int\n",
      "5113 cnda_structure_size(int nd)\n",
      "5114 {\n",
      "5115     // dim0, dim1, ...\n",
      "5116     // str0, str1, ...\n",
      "5117     // log2(dim0), log2(dim1), ...\n",
      "5118     return nd + nd + nd;\n",
      "5119 }\n",
      "5120 \n",
      "5121 const int *\n",
      "5122 CudaNdarray_HOST_DIMS(const CudaNdarray * self)\n",
      "5123 {\n",
      "5124     return self->host_structure;\n",
      "5125 }\n",
      "5126 \n",
      "5127 const int *\n",
      "5128 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)\n",
      "5129 {\n",
      "5130     return self->host_structure + self->nd;\n",
      "5131 }\n",
      "5132 const int *\n",
      "5133 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)\n",
      "5134 {\n",
      "5135     return self->host_structure + 2*self->nd;\n",
      "5136 }\n",
      "5137 \n",
      "5138 int\n",
      "5139 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)\n",
      "5140 {\n",
      "5141     int verbose = 0;\n",
      "5142 \n",
      "5143     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)\n",
      "5144     {\n",
      "5145         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 1\\n\");\n",
      "5146         return 0;\n",
      "5147     }\n",
      "5148 \n",
      "5149     if (cnda1->nd != cnda2->nd)\n",
      "5150     {\n",
      "5151         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 2\\n\");\n",
      "5152         return 0;\n",
      "5153     }\n",
      "5154 \n",
      "5155     for (int i=0; i < 2*cnda1->nd; i++)\n",
      "5156     {\n",
      "5157         if (cnda1->host_structure[i] != cnda2->host_structure[i])\n",
      "5158         {\n",
      "5159             if(verbose)\n",
      "5160                 fprintf(stdout, \"CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\\n\", i, cnda1->host_structure[i], cnda2->host_structure[i]);\n",
      "5161             return 0;\n",
      "5162         }\n",
      "5163     }\n",
      "5164 \n",
      "5165     if (!ignoreBase && cnda1->base != cnda2->base)\n",
      "5166     {\n",
      "5167         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 4\");\n",
      "5168         return 0;\n",
      "5169     }\n",
      "5170     else if (cnda1->data_allocated != cnda2->data_allocated)\n",
      "5171     {\n",
      "5172         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 5\");\n",
      "5173         return 0;\n",
      "5174     }\n",
      "5175     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)\n",
      "5176     {\n",
      "5177         if(verbose) fprintf(stdout, \"CUDANDARRAY_EQUAL FAILED : 6\");\n",
      "5178         // no need to check devdata if data is not allocated\n",
      "5179         return 0;\n",
      "5180     }\n",
      "5181 \n",
      "5182     return 1;\n",
      "5183 }\n",
      "5184 \n",
      "5185 \n",
      "5186 int\n",
      "5187 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)\n",
      "5188 {\n",
      "5189     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);\n",
      "5190 }\n",
      "5191 \n",
      "5192 int\n",
      "5193 cnda_copy_structure_to_device(const CudaNdarray * self)\n",
      "5194 {\n",
      "5195     //If the device structure do not exists, create it.\n",
      "5196     //We allocate it here as we do not need it often.\n",
      "5197     //In fact, we need it so infrequently that we expect\n",
      "5198     //that most object won't need it. Not allocating it\n",
      "5199     //save a significant when creating object.\n",
      "5200     //This speed up a benchmark by 8% with the gc.\n",
      "5201     if (!self->dev_structure)\n",
      "5202     {\n",
      "5203         int struct_size = cnda_structure_size(self->nd);\n",
      "5204         if (struct_size)\n",
      "5205         {\n",
      "5206             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));\n",
      "5207             if (NULL == self->dev_structure)\n",
      "5208             {\n",
      "5209                 return -1;\n",
      "5210             }\n",
      "5211         }\n",
      "5212     }\n",
      "5213     if (cublasSetVector(cnda_structure_size(self->nd),\n",
      "5214                         sizeof(int),\n",
      "5215                         self->host_structure,\n",
      "5216                         1,\n",
      "5217                         self->dev_structure,\n",
      "5218                         1) != CUBLAS_STATUS_SUCCESS)\n",
      "5219     {\n",
      "5220         PyErr_SetString(PyExc_RuntimeError, \"error copying structure to device memory\");\n",
      "5221         return -1;\n",
      "5222     }\n",
      "5223     self->dev_structure_fresh = 1;\n",
      "5224     return 0;\n",
      "5225 }\n",
      "5226 \n",
      "5227 const int *\n",
      "5228 CudaNdarray_DEV_DIMS(const CudaNdarray * self)\n",
      "5229 {\n",
      "5230     if (!self->dev_structure_fresh)\n",
      "5231     {\n",
      "5232         if (cnda_copy_structure_to_device(self))\n",
      "5233             return NULL;\n",
      "5234     }\n",
      "5235     return self->dev_structure;\n",
      "5236 }\n",
      "5237 const int *\n",
      "5238 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)\n",
      "5239 {\n",
      "5240     if (!self->dev_structure_fresh)\n",
      "5241     {\n",
      "5242         if (cnda_copy_structure_to_device(self))\n",
      "5243             return NULL;\n",
      "5244     }\n",
      "5245     return self->dev_structure + self->nd;\n",
      "5246 }\n",
      "5247 const int *\n",
      "5248 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)\n",
      "5249 {\n",
      "5250     if (!self->dev_structure_fresh)\n",
      "5251     {\n",
      "5252         if (cnda_copy_structure_to_device(self))\n",
      "5253             return NULL;\n",
      "5254     }\n",
      "5255     return self->dev_structure + 2*self->nd;\n",
      "5256 }\n",
      "5257 float *\n",
      "5258 CudaNdarray_DEV_DATA(const CudaNdarray * self)\n",
      "5259 {\n",
      "5260     return self->devdata;\n",
      "5261 }\n",
      "5262 \n",
      "5263 /**\n",
      "5264  * Return the number of elements in the ndarray (product of the dimensions)\n",
      "5265  */\n",
      "5266 size_t\n",
      "5267 CudaNdarray_SIZE(const CudaNdarray *self)\n",
      "5268 {\n",
      "5269     if (self->nd == -1) return 0;\n",
      "5270     size_t size = 1;\n",
      "5271     for (int i = 0; i < self->nd; ++i)\n",
      "5272     {\n",
      "5273         size *= CudaNdarray_HOST_DIMS(self)[i];\n",
      "5274     }\n",
      "5275     return size;\n",
      "5276 }\n",
      "5277 \n",
      "5278 PyObject *\n",
      "5279 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)\n",
      "5280 {\n",
      "5281     return PyInt_FromLong(CudaNdarray_SIZE(self));\n",
      "5282 }\n",
      "5283 \n",
      "5284 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)\n",
      "5285 {\n",
      "5286     return CudaNdarray_set_device_data(self, data, (PyObject *) base);\n",
      "5287 }\n",
      "5288 \n",
      "5289 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)\n",
      "5290 {\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5291     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));\n",
      "5292 }\n",
      "5293 \n",
      "5294 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)\n",
      "5295 {\n",
      "5296     cudaError_t err = cudaGetLastError();\n",
      "5297     if( cudaSuccess != err)\n",
      "5298     {\n",
      "5299         PyErr_Format(PyExc_RuntimeError,\n",
      "5300                      \"Cuda error: %s: %s.\",\n",
      "5301                      \"fprint_CudaNdarray was called with an uncleared error\",\n",
      "5302                      cudaGetErrorString(err));\n",
      "5303         return -1;\n",
      "5304     }\n",
      "5305     fprintf(fd, \"CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\\n\",\n",
      "5306             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);\n",
      "5307     fprintf(fd, \"\\tHOST_DIMS:      \");\n",
      "5308     for (int i = 0; i < self->nd; ++i)\n",
      "5309     {\n",
      "5310         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_DIMS(self)[i]);\n",
      "5311     }\n",
      "5312     fprintf(fd, \"\\n\\tHOST_STRIDES: \");\n",
      "5313     for (int i = 0; i < self->nd; ++i)\n",
      "5314     {\n",
      "5315         fprintf(fd, \"%i\\t\", CudaNdarray_HOST_STRIDES(self)[i]);\n",
      "5316     }\n",
      "5317 \n",
      "5318     if (self->dev_structure)\n",
      "5319     {\n",
      "5320         int data=0;\n",
      "5321         fprintf(fd, \"\\n\\tDEV_DIMS:      \");\n",
      "5322         for (int i = 0; i < self->nd; ++i)\n",
      "5323         {\n",
      "5324             cublasGetVector(1, sizeof(int),\n",
      "5325                             self->dev_structure+i, 1,\n",
      "5326                             &data, 1);\n",
      "5327             fprintf(fd, \"%i\\t\", data);\n",
      "5328         }\n",
      "5329         fprintf(fd, \"\\n\\tDEV_STRIDES: \");\n",
      "5330         for (int i = 0; i < self->nd; ++i)\n",
      "5331         {\n",
      "5332             cublasGetVector(1, sizeof(int),\n",
      "5333                             self->dev_structure + self->nd+i, 1,\n",
      "5334                             &data, 1);\n",
      "5335             fprintf(fd, \"%i \\t\", data);\n",
      "5336         }\n",
      "5337         fprintf(fd, \"\\n\");\n",
      "5338     }\n",
      "5339     else\n",
      "5340     {\n",
      "5341         fprintf(fd, \"\\n\\tdev_structure not allocated\\n\");\n",
      "5342     }\n",
      "5343 \n",
      "5344     err = cudaGetLastError();\n",
      "5345     if( cudaSuccess != err)\n",
      "5346     {\n",
      "5347         PyErr_Format(PyExc_RuntimeError,\n",
      "5348                      \"Cuda error: %s: %s.\",\n",
      "5349                      \"fprint_CudaNdarray\",\n",
      "5350                      cudaGetErrorString(err));\n",
      "5351         return -1;\n",
      "5352     }\n",
      "5353     return 0;\n",
      "5354 }\n",
      "5355 \n",
      "5356 \n",
      "5357 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,\n",
      "5358                             const int * dims, int fortran)\n",
      "5359 {\n",
      "5360     bool allocated = false;\n",
      "5361     if (*arr == NULL)\n",
      "5362     {\n",
      "5363         // This allocates the metadata but not the data\n",
      "5364         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);\n",
      "5365         if (*arr == NULL)\n",
      "5366             return -1;\n",
      "5367         allocated = true;\n",
      "5368     }\n",
      "5369 \n",
      "5370     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))\n",
      "5371     {\n",
      "5372         if (allocated)\n",
      "5373         {\n",
      "5374             Py_DECREF(*arr);\n",
      "5375             *arr = NULL;\n",
      "5376         }\n",
      "5377         return -1;\n",
      "5378     }\n",
      "5379     return 0;\n",
      "5380 }\n",
      "5381 \n",
      "5382 \n",
      "5383 /*\n",
      "5384   Local Variables:\n",
      "5385   mode:c++\n",
      "5386   c-basic-offset:4\n",
      "5387   c-file-style:\"stroustrup\"\n",
      "5388   indent-tabs-mode:nil\n",
      "5389   fill-column:79\n",
      "5390   End:\n",
      "5391 */\n",
      "5392 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :\n",
      "5393 \n",
      "===============================\n",
      "ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -Xlinker /DEBUG -D HAVE_ROUND -m64 -Xcompiler -DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD -I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\theano\\\\sandbox\\\\cuda\" -I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include\" -I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\include\" -I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\theano\\\\gof\" -L\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\libs\" -L\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\" -o C:\\\\Users\\\\soais\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-10-10.0.15063-SP0-Intel64_Family_6_Model_78_Stepping_3_GenuineIntel-3.5.4-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd mod.cu -lcublas -lpython35 -lcudart')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc fatal   : Cannot find compiler 'cl.exe' in PATH\n",
      "\n",
      "['nvcc', '-shared', '-O3', '-Xlinker', '/DEBUG', '-D HAVE_ROUND', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD', '-I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\theano\\\\sandbox\\\\cuda\"', '-I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include\"', '-I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\include\"', '-I\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\lib\\\\site-packages\\\\theano\\\\gof\"', '-L\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\\\\libs\"', '-L\"C:\\\\Users\\\\soais\\\\Anaconda3\\\\envs\\\\hse-aml\"', '-o', 'C:\\\\Users\\\\soais\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-10-10.0.15063-SP0-Intel64_Family_6_Model_78_Stepping_3_GenuineIntel-3.5.4-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd', 'mod.cu', '-lcublas', '-lpython35', '-lcudart']\n",
      "(50000, 28, 28) (50000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ce2fbf358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[0], cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_classes = 10\n",
    "h_c1 = 200\n",
    "h_c2 = 50\n",
    "lr_rate = 1e-3\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input placeholders\n",
    "# image is flattened to 1-D vector\n",
    "inputs = tf.placeholder(tf.float32, [None, X_train.shape[1]*X_train.shape[2]])\n",
    "labels = tf.placeholder(tf.float32, [None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "#layer 1 hidden unit count h_c \n",
    "\n",
    "\n",
    "# layer 1 weights\n",
    "W1 = tf.Variable(tf.truncated_normal([X_train.shape[1]*X_train.shape[2], h_c1], stddev=0.01))\n",
    "b1 = tf.Variable(tf.zeros(h_c1))\n",
    "#layer 1 activation\n",
    "h1 = tf.matmul(inputs, W1) + b1\n",
    "# layer 1 output\n",
    "o1 = tf.nn.relu(h1)\n",
    "\n",
    "# layer 2 weights\n",
    "W2 = tf.Variable(tf.truncated_normal([h_c1, h_c2], stddev=0.01))\n",
    "b2 = tf.Variable(tf.zeros(h_c2))\n",
    "#layer 2 activation\n",
    "h2 = tf.matmul(o1, W2) + b2\n",
    "# layer 2 output\n",
    "o2 = tf.nn.relu(h2)\n",
    "\n",
    "\n",
    "# final layer weights and biases\n",
    "W3 = tf.Variable(tf.truncated_normal([h_c2, num_classes], stddev=0.01))\n",
    "b3 = tf.Variable(tf.zeros(num_classes))\n",
    "# final layer activation\n",
    "logits = tf.matmul(o2, W3) + b3\n",
    "predictions = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_rate).minimize(loss)\n",
    "accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration 500, Training Loss=32.633 and Val loss=30.168. Val Accuracy=91.53\n",
      "After iteration 600, Training Loss=29.746 and Val loss=27.989. Val Accuracy=91.75\n",
      "After iteration 700, Training Loss=28.097 and Val loss=26.356. Val Accuracy=92.47\n",
      "After iteration 800, Training Loss=25.295 and Val loss=23.686. Val Accuracy=93.10\n",
      "After iteration 900, Training Loss=25.008 and Val loss=24.156. Val Accuracy=93.04\n",
      "After iteration 1000, Training Loss=21.298 and Val loss=20.487. Val Accuracy=94.26\n",
      "After iteration 1100, Training Loss=19.743 and Val loss=19.649. Val Accuracy=94.36\n",
      "After iteration 1200, Training Loss=18.380 and Val loss=18.377. Val Accuracy=94.77\n",
      "After iteration 1300, Training Loss=17.249 and Val loss=17.052. Val Accuracy=95.12\n",
      "After iteration 1400, Training Loss=18.016 and Val loss=18.564. Val Accuracy=94.75\n",
      "After iteration 1500, Training Loss=15.426 and Val loss=15.959. Val Accuracy=95.49\n",
      "After iteration 1600, Training Loss=14.354 and Val loss=15.548. Val Accuracy=95.51\n",
      "After iteration 1700, Training Loss=13.293 and Val loss=14.601. Val Accuracy=95.99\n",
      "After iteration 1800, Training Loss=12.627 and Val loss=13.763. Val Accuracy=96.24\n",
      "After iteration 1900, Training Loss=14.016 and Val loss=15.767. Val Accuracy=95.48\n",
      "After iteration 2000, Training Loss=11.583 and Val loss=13.336. Val Accuracy=96.29\n",
      "After iteration 2100, Training Loss=11.005 and Val loss=13.281. Val Accuracy=96.19\n",
      "After iteration 2200, Training Loss=10.076 and Val loss=12.544. Val Accuracy=96.63\n",
      "After iteration 2300, Training Loss=9.620 and Val loss=11.865. Val Accuracy=96.67\n",
      "After iteration 2400, Training Loss=11.185 and Val loss=14.108. Val Accuracy=96.18\n",
      "After iteration 2500, Training Loss=9.097 and Val loss=11.847. Val Accuracy=96.65\n",
      "After iteration 2600, Training Loss=8.628 and Val loss=11.820. Val Accuracy=96.76\n",
      "After iteration 2700, Training Loss=7.905 and Val loss=11.321. Val Accuracy=96.95\n",
      "After iteration 2800, Training Loss=7.659 and Val loss=10.776. Val Accuracy=96.89\n",
      "After iteration 2900, Training Loss=9.128 and Val loss=12.922. Val Accuracy=96.56\n",
      "After iteration 3000, Training Loss=7.306 and Val loss=10.972. Val Accuracy=96.97\n",
      "After iteration 3100, Training Loss=6.932 and Val loss=10.867. Val Accuracy=97.07\n",
      "After iteration 3200, Training Loss=6.340 and Val loss=10.482. Val Accuracy=97.20\n",
      "After iteration 3300, Training Loss=6.289 and Val loss=10.042. Val Accuracy=97.10\n",
      "After iteration 3400, Training Loss=7.529 and Val loss=12.186. Val Accuracy=96.67\n",
      "\n",
      "\n",
      "Test Set metrics after traianing\n",
      "Test loss=9.580. Test Accuracy=97.27\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epochs = 7\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for idx in range(0, X_train.shape[0], batch_size):\n",
    "            X = X_train[idx:idx+batch_size]\n",
    "            X = np.reshape(X, [X.shape[0], -1])\n",
    "            y = y_train[idx:idx+batch_size]\n",
    "            y = np.eye(num_classes)[y] #one hot encode\n",
    "            #y = tf.one_hot(y, depth=num_classes)\n",
    "            \n",
    "            feed = {inputs: X, labels: y}\n",
    "            t_loss, _ = sess.run([loss, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration > 400 and iteration % 100 == 0:\n",
    "                #training loss on full batch\n",
    "                train_loss = []\n",
    "                for idx2 in range(0, X_train.shape[0], batch_size):\n",
    "                    X = X_train[idx2: idx2+batch_size]\n",
    "                    X = np.reshape(X, [X.shape[0], -1])\n",
    "                    y = y_train[idx2:idx2+batch_size]\n",
    "                    y = np.eye(num_classes)[y] #one hot encode\n",
    "                    #y = y_train[idx2: idx2+batch_size]\n",
    "                    train_loss.append(sess.run([loss], feed_dict = {inputs:X, labels:y}))\n",
    "                \n",
    "                train_loss_mean = np.mean(train_loss)\n",
    "                training_loss.append(train_loss_mean)\n",
    "                \n",
    "                #validation_loss\n",
    "                val_loss = []\n",
    "                val_accuracy = []\n",
    "                for idx2 in range(0, X_val.shape[0], batch_size):\n",
    "                    X = X_val[idx2: idx2+batch_size]\n",
    "                    X = np.reshape(X, [X.shape[0], -1])\n",
    "                    y = y_val[idx2:idx2+batch_size]\n",
    "                    y = np.eye(num_classes)[y] #one hot encode\n",
    "                    #y = y_test[idx2: idx2+batch_size]\n",
    "                    v_loss, v_acc = sess.run([loss, accuracy], feed_dict = {inputs:X, labels:y})\n",
    "                    val_loss.append(v_loss)\n",
    "                    val_accuracy.append(v_acc)\n",
    "                \n",
    "                val_loss_mean = np.mean(val_loss)\n",
    "                val_accuracy_mean = np.mean(val_accuracy)\n",
    "                validation_loss.append(val_loss_mean)\n",
    "                \n",
    "                print(\"After iteration {}, Training Loss={:.3f} and Val loss={:.3f}. Val Accuracy={:5.2f}\".format(\n",
    "                    iteration, train_loss_mean, val_loss_mean, val_accuracy_mean))\n",
    "            \n",
    "            iteration += 1\n",
    "\n",
    "    #final test_loss and test accuracy\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for idx2 in range(0, X_test.shape[0], batch_size):\n",
    "        X = X_test[idx2: idx2+batch_size]\n",
    "        X = np.reshape(X, [X.shape[0], -1])\n",
    "        y = y_test[idx2:idx2+batch_size]\n",
    "        y = np.eye(num_classes)[y] #one hot encode\n",
    "        #y = y_test[idx2: idx2+batch_size]\n",
    "        t_loss, t_acc = sess.run([loss, accuracy], feed_dict = {inputs:X, labels:y})\n",
    "        test_loss.append(t_loss)\n",
    "        test_accuracy.append(t_acc)\n",
    "\n",
    "    test_loss_mean = np.mean(test_loss)\n",
    "    test_accuracy_mean = np.mean(test_accuracy)\n",
    "\n",
    "    print('\\n')\n",
    "    print('Test Set metrics after traianing')\n",
    "    print(\"Test loss={:.3f}. Test Accuracy={:5.2f}\".format(test_loss_mean, test_accuracy_mean))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFKqmNNEoJRSxZwmAqWUKub+H+4RvlNO+3k8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200020d13c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the training and testing loss\n",
    "x = [i for i in range(len(training_loss))]\n",
    "plt.plot(x, training_loss, label='Train')\n",
    "plt.plot(x, testing_loss, label='Validation')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.title(\"LOSS Graph\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
